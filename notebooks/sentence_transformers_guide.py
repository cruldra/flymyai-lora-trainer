"""
Sentence Transformers å®Œå…¨æŒ‡å—

Sentence Transformersæ˜¯ä¸€ä¸ªPythonåº“ï¼Œç”¨äºç”Ÿæˆå¥å­ã€æ®µè½å’Œå›¾åƒçš„è¯­ä¹‰å‘é‡è¡¨ç¤ºã€‚
å®ƒåŸºäºTransformeræ¨¡å‹ï¼ˆå¦‚BERTã€RoBERTaç­‰ï¼‰ï¼Œå¯ä»¥è½»æ¾åœ°å°†æ–‡æœ¬è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡ã€‚

ç‰¹ç‚¹ï¼š
1. ğŸš€ ç®€å•æ˜“ç”¨ - å‡ è¡Œä»£ç å³å¯ç”Ÿæˆé«˜è´¨é‡å‘é‡
2. ğŸ¯ é¢„è®­ç»ƒæ¨¡å‹ - æä¾›å¤šç§é¢„è®­ç»ƒæ¨¡å‹
3. ğŸŒ å¤šè¯­è¨€æ”¯æŒ - æ”¯æŒ100+ç§è¯­è¨€
4. ğŸ“Š é«˜æ€§èƒ½ - ä¼˜åŒ–çš„æ¨ç†é€Ÿåº¦
5. ğŸ”§ å¯å¾®è°ƒ - æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒ

ä½œè€…: Marimo Notebook
æ—¥æœŸ: 2025-01-XX
"""

import marimo

__generated_with = "0.16.5"
app = marimo.App(width="medium", app_title="Sentence Transformers å®Œå…¨æŒ‡å—")


@app.cell(hide_code=True)
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    # ğŸ¤– Sentence Transformers å®Œå…¨æŒ‡å—

    ## ä»€ä¹ˆæ˜¯Sentence Transformersï¼Ÿ

    **Sentence Transformers** æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¥å­å’Œæ®µè½åµŒå…¥ï¼ˆembeddingsï¼‰çš„Pythonæ¡†æ¶ã€‚å®ƒå¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºå¯†é›†å‘é‡è¡¨ç¤ºï¼Œè¿™äº›å‘é‡å¯ä»¥ç”¨äºï¼š

    - ğŸ” **è¯­ä¹‰æœç´¢** - æ‰¾åˆ°è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬
    - ğŸ“Š **æ–‡æœ¬èšç±»** - å°†ç›¸ä¼¼æ–‡æœ¬åˆ†ç»„
    - ğŸ¯ **æ–‡æœ¬åˆ†ç±»** - åŸºäºå‘é‡çš„åˆ†ç±»ä»»åŠ¡
    - ğŸ”— **é—®ç­”ç³»ç»Ÿ** - åŒ¹é…é—®é¢˜å’Œç­”æ¡ˆ
    - ğŸŒ **è·¨è¯­è¨€æ£€ç´¢** - å¤šè¯­è¨€æ–‡æœ¬åŒ¹é…

    ### æ ¸å¿ƒä¼˜åŠ¿

    1. **ç®€å•æ˜“ç”¨** - 3è¡Œä»£ç å³å¯ç”Ÿæˆå‘é‡
    2. **é«˜è´¨é‡** - åŸºäºSOTAçš„Transformeræ¨¡å‹
    3. **å¿«é€Ÿ** - ä¼˜åŒ–çš„æ¨ç†æ€§èƒ½
    4. **çµæ´»** - æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å’Œå¾®è°ƒ
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“¦ å®‰è£…

    ```bash
    # åŸºç¡€å®‰è£…
    pip install sentence-transformers

    # æˆ–ä½¿ç”¨uv
    uv pip install sentence-transformers
    ```

    **ä¾èµ–é¡¹**:
    - PyTorch >= 1.11.0
    - transformers >= 4.34.0
    - tqdm
    - numpy
    - scikit-learn
    """
    )
    return


@app.cell
def _():
    # ğŸ“¦ å¯¼å…¥å¿…è¦çš„åº“
    from sentence_transformers import SentenceTransformer, util
    import numpy as np
    
    print("=" * 60)
    print("ğŸ¤– Sentence Transformers å¯¼å…¥æˆåŠŸ")
    print("=" * 60)
    
    return SentenceTransformer, np, util


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼šåŠ è½½æ¨¡å‹

    Sentence Transformersæä¾›äº†å¤šç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€‚ç”¨äºä¸åŒçš„åœºæ™¯ã€‚
    """
    )
    return


@app.cell
def _(SentenceTransformer):
    # ğŸ¯ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    # ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"ğŸ“Š æ¨¡å‹åç§°: {model._model_card_vars.get('model_name', 'N/A')}")
    print(f"ğŸ“ å‘é‡ç»´åº¦: {model.get_sentence_embedding_dimension()}")
    print(f"ğŸ”¢ æœ€å¤§åºåˆ—é•¿åº¦: {model.max_seq_length}")
    
    return (model,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“ åŸºç¡€æ“ä½œï¼šæ–‡æœ¬ç¼–ç 

    å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡æ˜¯æœ€åŸºç¡€çš„æ“ä½œã€‚
    """
    )
    return


@app.cell
def _(model, np):
    # ğŸ“ ç¤ºä¾‹1: å•ä¸ªå¥å­ç¼–ç 
    sentence = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
    embedding = model.encode(sentence)
    
    print("=" * 60)
    print("ğŸ“ å•å¥ç¼–ç ")
    print("=" * 60)
    print(f"åŸæ–‡: {sentence}")
    print(f"å‘é‡ç»´åº¦: {embedding.shape}")
    print(f"å‘é‡å‰5ä¸ªå€¼: {embedding[:5]}")
    print()
    
    # ğŸ“ ç¤ºä¾‹2: æ‰¹é‡ç¼–ç 
    sentences = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
        "ä»Šå¤©å¤©æ°”çœŸä¸é”™",
        "æˆ‘å–œæ¬¢åƒæŠ«è¨"
    ]
    
    embeddings = model.encode(sentences)
    
    print("=" * 60)
    print("ğŸ“ æ‰¹é‡ç¼–ç ")
    print("=" * 60)
    print(f"å¥å­æ•°é‡: {len(sentences)}")
    print(f"å‘é‡çŸ©é˜µå½¢çŠ¶: {embeddings.shape}")
    print()

    # æ˜¾ç¤ºæ¯ä¸ªå¥å­
    for num, text in enumerate(sentences):
        print(f"{num + 1}. {text}")

    return embedding, embeddings, sentence, sentences


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ” è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

    è®¡ç®—æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ˜¯æœ€å¸¸è§çš„åº”ç”¨ã€‚
    """
    )
    return


@app.cell
def _(embeddings, model, sentences, util):
    # ğŸ” è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    print("=" * 60)
    print("ğŸ” è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ")
    print("=" * 60)
    
    # è®¡ç®—æ‰€æœ‰å¥å­å¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦
    cosine_scores = util.cos_sim(embeddings, embeddings)
    
    print("\nç›¸ä¼¼åº¦çŸ©é˜µ (0-1ä¹‹é—´ï¼Œè¶Šæ¥è¿‘1è¶Šç›¸ä¼¼):\n")

    # æ‰“å°è¡¨å¤´
    print("     ", end="")
    for col_idx in range(len(sentences)):
        print(f"  å¥{col_idx+1}  ", end="")
    print()

    # æ‰“å°ç›¸ä¼¼åº¦çŸ©é˜µ
    for row_idx, sent in enumerate(sentences):
        print(f"å¥{row_idx+1} ", end="")
        for col_idx in range(len(sentences)):
            score = cosine_scores[row_idx][col_idx].item()
            print(f" {score:.3f} ", end="")
        print(f" | {sent[:15]}...")

    print()

    # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å¥å­å¯¹
    print("ğŸ¯ æœ€ç›¸ä¼¼çš„å¥å­å¯¹:")
    pairs_list = []
    for pair_i in range(len(sentences)):
        for pair_j in range(pair_i + 1, len(sentences)):
            score = cosine_scores[pair_i][pair_j].item()
            pairs_list.append((pair_i, pair_j, score))

    # æ’åºå¹¶æ˜¾ç¤ºå‰3å¯¹
    pairs_list.sort(key=lambda x: x[2], reverse=True)
    for rank_num, (i, j, score) in enumerate(pairs_list[:3], 1):
        print(f"{rank_num}. ç›¸ä¼¼åº¦: {score:.4f}")
        print(f"   å¥å­A: {sentences[i]}")
        print(f"   å¥å­B: {sentences[j]}")
        print()

    return cosine_scores, pairs_list


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ” å®æˆ˜æ¡ˆä¾‹1ï¼šè¯­ä¹‰æœç´¢

    ç»™å®šä¸€ä¸ªæŸ¥è¯¢ï¼Œä»æ–‡æ¡£åº“ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚
    """
    )
    return


@app.cell
def _(model, util):
    # ğŸ” è¯­ä¹‰æœç´¢ç¤ºä¾‹
    print("=" * 60)
    print("ğŸ” è¯­ä¹‰æœç´¢ç¤ºä¾‹")
    print("=" * 60)
    
    # æ–‡æ¡£åº“
    documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
        "æœºå™¨å­¦ä¹ éœ€è¦å¤§é‡çš„æ•°æ®",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†",
        "ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†äººç±»è¯­è¨€",
        "è®¡ç®—æœºè§†è§‰è®©æœºå™¨ç†è§£å›¾åƒ",
        "å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æ¥å­¦ä¹ ",
        "æ•°æ®ç§‘å­¦ç»“åˆç»Ÿè®¡å­¦å’Œç¼–ç¨‹"
    ]
    
    # ç¼–ç æ–‡æ¡£
    doc_embeddings = model.encode(documents, convert_to_tensor=True)
    
    # æŸ¥è¯¢
    query = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
    query_embedding = model.encode(query, convert_to_tensor=True)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    hits = util.semantic_search(query_embedding, doc_embeddings, top_k=3)[0]
    
    print(f"\næŸ¥è¯¢: {query}\n")
    print("ğŸ¯ æœ€ç›¸å…³çš„æ–‡æ¡£:\n")
    
    for rank, hit in enumerate(hits, 1):
        doc_id = hit['corpus_id']
        score_val = hit['score']
        print(f"{rank}. ç›¸ä¼¼åº¦: {score_val:.4f}")
        print(f"   æ–‡æ¡£: {documents[doc_id]}")
        print()
    
    return doc_embeddings, doc_id, documents, hits, query, query_embedding, rank, score_val


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“Š å®æˆ˜æ¡ˆä¾‹2ï¼šæ–‡æœ¬èšç±»

    å°†ç›¸ä¼¼çš„æ–‡æœ¬è‡ªåŠ¨åˆ†ç»„ã€‚
    """
    )
    return


@app.cell
def _(model, np):
    # ğŸ“Š æ–‡æœ¬èšç±»ç¤ºä¾‹
    from sklearn.cluster import KMeans
    
    print("=" * 60)
    print("ğŸ“Š æ–‡æœ¬èšç±»ç¤ºä¾‹")
    print("=" * 60)
    
    # å‡†å¤‡æ–‡æœ¬æ•°æ®
    texts = [
        # ç§‘æŠ€ç±»
        "äººå·¥æ™ºèƒ½çš„å‘å±•",
        "æœºå™¨å­¦ä¹ ç®—æ³•",
        "æ·±åº¦ç¥ç»ç½‘ç»œ",
        # ä½“è‚²ç±»
        "è¶³çƒæ¯”èµ›ç»“æœ",
        "ç¯®çƒè¿åŠ¨å‘˜",
        "å¥¥è¿ä¼šé‡‘ç‰Œ",
        # ç¾é£Ÿç±»
        "ä¸­å›½ä¼ ç»Ÿç¾é£Ÿ",
        "æ„å¤§åˆ©æŠ«è¨",
        "æ—¥æœ¬å¯¿å¸"
    ]
    
    # ç¼–ç 
    text_embeddings = model.encode(texts)
    
    # K-meansèšç±»
    num_clusters = 3
    clustering_model = KMeans(n_clusters=num_clusters, random_state=42)
    clustering_model.fit(text_embeddings)
    cluster_assignment = clustering_model.labels_
    
    # æŒ‰ç±»åˆ«ç»„ç»‡ç»“æœ
    clustered_texts = {}
    for text_idx, cluster_id in enumerate(cluster_assignment):
        if cluster_id not in clustered_texts:
            clustered_texts[cluster_id] = []
        clustered_texts[cluster_id].append(texts[text_idx])
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nå°† {len(texts)} ä¸ªæ–‡æœ¬åˆ†ä¸º {num_clusters} ä¸ªç±»åˆ«:\n")
    
    for cluster_num, cluster_texts in sorted(clustered_texts.items()):
        print(f"ğŸ“ ç±»åˆ« {cluster_num + 1}:")
        for text_item in cluster_texts:
            print(f"   - {text_item}")
        print()
    
    return (
        KMeans,
        cluster_assignment,
        cluster_id,
        cluster_num,
        cluster_texts,
        clustered_texts,
        clustering_model,
        num_clusters,
        text_embeddings,
        text_idx,
        text_item,
        texts,
    )


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ¨ å®æˆ˜æ¡ˆä¾‹3ï¼šé—®ç­”åŒ¹é…

    æ‰¾åˆ°ä¸é—®é¢˜æœ€åŒ¹é…çš„ç­”æ¡ˆã€‚
    """
    )
    return


@app.cell
def _(model, util):
    # ğŸ¨ é—®ç­”åŒ¹é…ç¤ºä¾‹
    print("=" * 60)
    print("ğŸ¨ é—®ç­”åŒ¹é…ç³»ç»Ÿ")
    print("=" * 60)
    
    # é—®ç­”å¯¹
    qa_pairs = {
        "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ": "Pythonæ˜¯ä¸€ç§é«˜çº§ã€è§£é‡Šå‹ã€é€šç”¨çš„ç¼–ç¨‹è¯­è¨€ã€‚",
        "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ": "å­¦ä¹ æœºå™¨å­¦ä¹ éœ€è¦æŒæ¡æ•°å­¦åŸºç¡€ã€ç¼–ç¨‹æŠ€èƒ½å’Œå®è·µç»éªŒã€‚",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚",
        "å¦‚ä½•æé«˜ä»£ç è´¨é‡ï¼Ÿ": "é€šè¿‡ä»£ç å®¡æŸ¥ã€å•å…ƒæµ‹è¯•å’Œéµå¾ªæœ€ä½³å®è·µæ¥æé«˜ä»£ç è´¨é‡ã€‚",
        "ä»€ä¹ˆæ˜¯APIï¼Ÿ": "APIæ˜¯åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼Œå…è®¸ä¸åŒè½¯ä»¶ä¹‹é—´é€šä¿¡ã€‚"
    }
    
    questions_list = list(qa_pairs.keys())
    answers_list = list(qa_pairs.values())
    
    # ç¼–ç é—®é¢˜
    question_embeddings = model.encode(questions_list, convert_to_tensor=True)
    
    # ç”¨æˆ·é—®é¢˜
    user_questions = [
        "Pythonç¼–ç¨‹è¯­è¨€æ˜¯ä»€ä¹ˆ",
        "æ·±åº¦å­¦ä¹ çš„å®šä¹‰",
        "æ€æ ·å†™å‡ºå¥½çš„ä»£ç "
    ]
    
    print("\nğŸ” ç”¨æˆ·é—®é¢˜åŒ¹é…:\n")
    
    for user_q in user_questions:
        user_q_embedding = model.encode(user_q, convert_to_tensor=True)
        search_hits = util.semantic_search(user_q_embedding, question_embeddings, top_k=1)[0]
        
        best_match = search_hits[0]
        matched_q_idx = best_match['corpus_id']
        similarity = best_match['score']
        
        print(f"â“ ç”¨æˆ·é—®é¢˜: {user_q}")
        print(f"âœ… åŒ¹é…é—®é¢˜: {questions_list[matched_q_idx]}")
        print(f"ğŸ’¡ ç­”æ¡ˆ: {answers_list[matched_q_idx]}")
        print(f"ğŸ“Š ç›¸ä¼¼åº¦: {similarity:.4f}")
        print()
    
    return (
        answers_list,
        best_match,
        matched_q_idx,
        qa_pairs,
        question_embeddings,
        questions_list,
        search_hits,
        similarity,
        user_q,
        user_q_embedding,
        user_questions,
    )


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸŒ å®æˆ˜æ¡ˆä¾‹4ï¼šè·¨è¯­è¨€è¯­ä¹‰æœç´¢

    ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€æ£€ç´¢ã€‚
    """
    )
    return


@app.cell
def _(model, util):
    # ğŸŒ è·¨è¯­è¨€æœç´¢ç¤ºä¾‹
    print("=" * 60)
    print("ğŸŒ è·¨è¯­è¨€è¯­ä¹‰æœç´¢")
    print("=" * 60)

    # å¤šè¯­è¨€æ–‡æ¡£åº“
    multilingual_docs = [
        "Machine learning is a subset of artificial intelligence",  # è‹±æ–‡
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",  # ä¸­æ–‡
        "El aprendizaje profundo utiliza redes neuronales",  # è¥¿ç­ç‰™è¯­
        "æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®",  # ä¸­æ–‡
        "Natural language processing helps computers understand human language",  # è‹±æ–‡
    ]

    # ç¼–ç æ–‡æ¡£
    multi_doc_embeddings = model.encode(multilingual_docs, convert_to_tensor=True)

    # ä¸­æ–‡æŸ¥è¯¢
    chinese_query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
    query_emb = model.encode(chinese_query, convert_to_tensor=True)

    # æœç´¢
    results = util.semantic_search(query_emb, multi_doc_embeddings, top_k=3)[0]

    print(f"\nğŸ” æŸ¥è¯¢ (ä¸­æ–‡): {chinese_query}\n")
    print("ğŸ¯ è·¨è¯­è¨€æœç´¢ç»“æœ:\n")

    for position, result in enumerate(results, 1):
        doc_idx = result['corpus_id']
        sim_score = result['score']
        print(f"{position}. ç›¸ä¼¼åº¦: {sim_score:.4f}")
        print(f"   æ–‡æ¡£: {multilingual_docs[doc_idx]}")
        print()

    return (
        chinese_query,
        doc_idx,
        multi_doc_embeddings,
        multilingual_docs,
        position,
        query_emb,
        result,
        results,
        sim_score,
    )


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## âš™ï¸ é«˜çº§åŠŸèƒ½ï¼šç¼–ç å‚æ•°

    `encode()` æ–¹æ³•æ”¯æŒå¤šç§å‚æ•°æ¥ä¼˜åŒ–æ€§èƒ½å’Œç»“æœã€‚
    """
    )
    return


@app.cell
def _(model):
    # âš™ï¸ ç¼–ç å‚æ•°ç¤ºä¾‹
    print("=" * 60)
    print("âš™ï¸ ç¼–ç å‚æ•°æ¼”ç¤º")
    print("=" * 60)

    sample_texts = [
        "è¿™æ˜¯ç¬¬ä¸€ä¸ªå¥å­",
        "è¿™æ˜¯ç¬¬äºŒä¸ªå¥å­"
    ]

    # 1. åŸºç¡€ç¼–ç ï¼ˆè¿”å›numpyæ•°ç»„ï¼‰
    basic_emb = model.encode(sample_texts)
    print(f"\n1ï¸âƒ£ åŸºç¡€ç¼–ç :")
    print(f"   ç±»å‹: {type(basic_emb)}")
    print(f"   å½¢çŠ¶: {basic_emb.shape}")

    # 2. è½¬æ¢ä¸ºTensorï¼ˆç”¨äºPyTorchï¼‰
    tensor_emb = model.encode(sample_texts, convert_to_tensor=True)
    print(f"\n2ï¸âƒ£ Tensorç¼–ç :")
    print(f"   ç±»å‹: {type(tensor_emb)}")
    print(f"   å½¢çŠ¶: {tensor_emb.shape}")

    # 3. å½’ä¸€åŒ–å‘é‡
    normalized_emb = model.encode(sample_texts, normalize_embeddings=True)
    print(f"\n3ï¸âƒ£ å½’ä¸€åŒ–ç¼–ç :")
    print(f"   å‘é‡é•¿åº¦: {np.linalg.norm(normalized_emb[0]):.4f} (åº”è¯¥æ¥è¿‘1.0)")

    # 4. æ‰¹å¤„ç†å¤§å°
    large_batch = ["å¥å­" + str(num) for num in range(100)]
    batch_emb = model.encode(large_batch, batch_size=32, show_progress_bar=True)
    print(f"\n4ï¸âƒ£ æ‰¹å¤„ç†ç¼–ç :")
    print(f"   å¤„ç†äº† {len(large_batch)} ä¸ªå¥å­")
    print(f"   æ‰¹å¤§å°: 32")

    return (
        basic_emb,
        batch_emb,
        large_batch,
        normalized_emb,
        num,
        sample_texts,
        tensor_emb,
    )


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ¯ å¸¸ç”¨é¢„è®­ç»ƒæ¨¡å‹

    ä¸åŒçš„æ¨¡å‹é€‚ç”¨äºä¸åŒçš„åœºæ™¯ã€‚
    """
    )
    return


@app.cell
def _():
    # ğŸ¯ å¸¸ç”¨æ¨¡å‹åˆ—è¡¨
    print("=" * 60)
    print("ğŸ¯ æ¨èçš„é¢„è®­ç»ƒæ¨¡å‹")
    print("=" * 60)

    models_info = [
        {
            "name": "all-MiniLM-L6-v2",
            "lang": "è‹±æ–‡",
            "dim": 384,
            "speed": "âš¡âš¡âš¡",
            "quality": "â­â­â­",
            "use_case": "é€šç”¨è‹±æ–‡ä»»åŠ¡ï¼Œé€Ÿåº¦ä¼˜å…ˆ"
        },
        {
            "name": "all-mpnet-base-v2",
            "lang": "è‹±æ–‡",
            "dim": 768,
            "speed": "âš¡âš¡",
            "quality": "â­â­â­â­â­",
            "use_case": "é«˜è´¨é‡è‹±æ–‡ä»»åŠ¡"
        },
        {
            "name": "paraphrase-multilingual-MiniLM-L12-v2",
            "lang": "å¤šè¯­è¨€",
            "dim": 384,
            "speed": "âš¡âš¡âš¡",
            "quality": "â­â­â­â­",
            "use_case": "å¤šè¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä¸­æ–‡"
        },
        {
            "name": "paraphrase-multilingual-mpnet-base-v2",
            "lang": "å¤šè¯­è¨€",
            "dim": 768,
            "speed": "âš¡âš¡",
            "quality": "â­â­â­â­â­",
            "use_case": "é«˜è´¨é‡å¤šè¯­è¨€ä»»åŠ¡"
        },
        {
            "name": "distiluse-base-multilingual-cased-v2",
            "lang": "å¤šè¯­è¨€",
            "dim": 512,
            "speed": "âš¡âš¡âš¡",
            "quality": "â­â­â­â­",
            "use_case": "å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡"
        }
    ]

    print("\næ¨¡å‹å¯¹æ¯”:\n")
    for model_info in models_info:
        print(f"ğŸ“¦ {model_info['name']}")
        print(f"   è¯­è¨€: {model_info['lang']}")
        print(f"   ç»´åº¦: {model_info['dim']}")
        print(f"   é€Ÿåº¦: {model_info['speed']}")
        print(f"   è´¨é‡: {model_info['quality']}")
        print(f"   ç”¨é€”: {model_info['use_case']}")
        print()

    return model_info, models_info


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

    æé«˜ç¼–ç é€Ÿåº¦å’Œæ•ˆç‡çš„æ–¹æ³•ã€‚
    """
    )
    return


@app.cell
def _(model):
    import time

    # ğŸš€ æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹
    print("=" * 60)
    print("ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§")
    print("=" * 60)

    test_sentences = ["æµ‹è¯•å¥å­ " + str(n) for n in range(1000)]

    # 1. å°æ‰¹é‡ vs å¤§æ‰¹é‡
    print("\n1ï¸âƒ£ æ‰¹é‡å¤§å°å¯¹æ¯”:\n")

    start = time.time()
    _ = model.encode(test_sentences, batch_size=8, show_progress_bar=False)
    time_small = time.time() - start
    print(f"   æ‰¹å¤§å°=8:  {time_small:.2f}ç§’")

    start = time.time()
    _ = model.encode(test_sentences, batch_size=64, show_progress_bar=False)
    time_large = time.time() - start
    print(f"   æ‰¹å¤§å°=64: {time_large:.2f}ç§’")
    print(f"   æé€Ÿ: {time_small/time_large:.2f}x")

    # 2. å½’ä¸€åŒ–çš„å½±å“
    print("\n2ï¸âƒ£ å½’ä¸€åŒ–å¯¹æ¯”:\n")

    start = time.time()
    _ = model.encode(test_sentences[:100], normalize_embeddings=False, show_progress_bar=False)
    time_no_norm = time.time() - start
    print(f"   ä¸å½’ä¸€åŒ–: {time_no_norm:.3f}ç§’")

    start = time.time()
    _ = model.encode(test_sentences[:100], normalize_embeddings=True, show_progress_bar=False)
    time_norm = time.time() - start
    print(f"   å½’ä¸€åŒ–:   {time_norm:.3f}ç§’")

    # 3. æœ€ä½³å®è·µå»ºè®®
    print("\n3ï¸âƒ£ æœ€ä½³å®è·µ:\n")
    print("   âœ… ä½¿ç”¨è¾ƒå¤§çš„batch_sizeï¼ˆå¦‚32-64ï¼‰")
    print("   âœ… å¯¹äºç›¸ä¼¼åº¦è®¡ç®—ï¼Œä½¿ç”¨normalize_embeddings=True")
    print("   âœ… ä½¿ç”¨convert_to_tensor=Trueé¿å…ç±»å‹è½¬æ¢")
    print("   âœ… é¢„å…ˆç¼–ç é™æ€æ–‡æ¡£åº“ï¼Œé¿å…é‡å¤è®¡ç®—")
    print("   âœ… ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰")

    return n, start, test_sentences, time, time_large, time_no_norm, time_norm, time_small


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“š ä¸»è¦APIå‚è€ƒ

    ä»¥ä¸‹æ˜¯Sentence Transformersçš„æ ¸å¿ƒAPIæ€»ç»“ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ### ğŸ”§ æ ¸å¿ƒç±»å’Œæ–¹æ³•

    | API | è¯´æ˜ | å‚æ•° | è¿”å›å€¼ |
    |-----|------|------|--------|
    | `SentenceTransformer(model_name)` | åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ | `model_name`: æ¨¡å‹åç§°æˆ–è·¯å¾„<br>`device`: è®¾å¤‡('cuda'/'cpu')<br>`cache_folder`: ç¼“å­˜ç›®å½• | SentenceTransformerå¯¹è±¡ |
    | `model.encode(sentences)` | å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡ | `sentences`: å­—ç¬¦ä¸²æˆ–åˆ—è¡¨<br>`batch_size`: æ‰¹å¤§å°(é»˜è®¤32)<br>`show_progress_bar`: æ˜¾ç¤ºè¿›åº¦<br>`convert_to_tensor`: è¿”å›Tensor<br>`normalize_embeddings`: å½’ä¸€åŒ– | numpyæ•°ç»„æˆ–Tensor |
    | `model.get_sentence_embedding_dimension()` | è·å–å‘é‡ç»´åº¦ | æ—  | int |
    | `model.max_seq_length` | æœ€å¤§åºåˆ—é•¿åº¦ | æ—  | int |
    | `model.tokenize(texts)` | æ–‡æœ¬åˆ†è¯ | `texts`: å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ | å­—å…¸(input_ids, attention_maskç­‰) |

    ### ğŸ” ç›¸ä¼¼åº¦è®¡ç®—å·¥å…·

    | API | è¯´æ˜ | å‚æ•° | è¿”å›å€¼ |
    |-----|------|------|--------|
    | `util.cos_sim(a, b)` | è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ | `a`: å‘é‡æˆ–çŸ©é˜µ<br>`b`: å‘é‡æˆ–çŸ©é˜µ | ç›¸ä¼¼åº¦çŸ©é˜µ(Tensor) |
    | `util.dot_score(a, b)` | è®¡ç®—ç‚¹ç§¯åˆ†æ•° | `a`: å‘é‡æˆ–çŸ©é˜µ<br>`b`: å‘é‡æˆ–çŸ©é˜µ | åˆ†æ•°çŸ©é˜µ(Tensor) |
    | `util.semantic_search(query, corpus)` | è¯­ä¹‰æœç´¢ | `query`: æŸ¥è¯¢å‘é‡<br>`corpus`: æ–‡æ¡£å‘é‡<br>`top_k`: è¿”å›å‰kä¸ªç»“æœ<br>`score_function`: è¯„åˆ†å‡½æ•° | åˆ—è¡¨[{'corpus_id': int, 'score': float}] |
    | `util.paraphrase_mining(model, sentences)` | æŒ–æ˜ç›¸ä¼¼å¥å¯¹ | `model`: æ¨¡å‹å¯¹è±¡<br>`sentences`: å¥å­åˆ—è¡¨<br>`top_k`: è¿”å›å‰kå¯¹ | åˆ—è¡¨[(score, idx1, idx2)] |

    ### ğŸ“Š è¯„ä¼°å’Œè®­ç»ƒ

    | API | è¯´æ˜ | å‚æ•° | è¿”å›å€¼ |
    |-----|------|------|--------|
    | `model.similarity(sentences1, sentences2)` | è®¡ç®—ä¸¤ç»„å¥å­çš„ç›¸ä¼¼åº¦ | `sentences1`: å¥å­åˆ—è¡¨<br>`sentences2`: å¥å­åˆ—è¡¨ | ç›¸ä¼¼åº¦çŸ©é˜µ |
    | `model.save(path)` | ä¿å­˜æ¨¡å‹ | `path`: ä¿å­˜è·¯å¾„ | æ—  |
    | `SentenceTransformer.load(path)` | åŠ è½½æ¨¡å‹ | `path`: æ¨¡å‹è·¯å¾„ | SentenceTransformerå¯¹è±¡ |

    ### ğŸ¯ å¸¸ç”¨å‚æ•°è¯´æ˜

    | å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
    |------|------|--------|------|
    | `batch_size` | int | 32 | æ‰¹å¤„ç†å¤§å°ï¼Œè¶Šå¤§é€Ÿåº¦è¶Šå¿«ä½†å†…å­˜å ç”¨è¶Šé«˜ |
    | `show_progress_bar` | bool | True | æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ |
    | `convert_to_tensor` | bool | False | æ˜¯å¦è¿”å›PyTorch Tensor |
    | `normalize_embeddings` | bool | False | æ˜¯å¦å½’ä¸€åŒ–å‘é‡ï¼ˆæ¨èç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰ |
    | `device` | str | None | è®¾å¤‡é€‰æ‹©ï¼š'cuda'ã€'cpu'æˆ–None(è‡ªåŠ¨) |
    | `num_workers` | int | 0 | æ•°æ®åŠ è½½çš„å·¥ä½œè¿›ç¨‹æ•° |

    ### ğŸŒŸ æ¨èæ¨¡å‹åˆ—è¡¨

    | æ¨¡å‹åç§° | è¯­è¨€ | ç»´åº¦ | é€Ÿåº¦ | è´¨é‡ | é€‚ç”¨åœºæ™¯ |
    |---------|------|------|------|------|---------|
    | `all-MiniLM-L6-v2` | è‹±æ–‡ | 384 | âš¡âš¡âš¡ | â­â­â­ | é€šç”¨è‹±æ–‡ï¼Œé€Ÿåº¦ä¼˜å…ˆ |
    | `all-mpnet-base-v2` | è‹±æ–‡ | 768 | âš¡âš¡ | â­â­â­â­â­ | é«˜è´¨é‡è‹±æ–‡ä»»åŠ¡ |
    | `paraphrase-multilingual-MiniLM-L12-v2` | å¤šè¯­è¨€ | 384 | âš¡âš¡âš¡ | â­â­â­â­ | å¤šè¯­è¨€ï¼ˆå«ä¸­æ–‡ï¼‰ |
    | `paraphrase-multilingual-mpnet-base-v2` | å¤šè¯­è¨€ | 768 | âš¡âš¡ | â­â­â­â­â­ | é«˜è´¨é‡å¤šè¯­è¨€ |
    | `distiluse-base-multilingual-cased-v2` | å¤šè¯­è¨€ | 512 | âš¡âš¡âš¡ | â­â­â­â­ | å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡ |
    | `msmarco-distilbert-base-v4` | è‹±æ–‡ | 768 | âš¡âš¡ | â­â­â­â­ | ä¿¡æ¯æ£€ç´¢ |
    | `multi-qa-MiniLM-L6-cos-v1` | è‹±æ–‡ | 384 | âš¡âš¡âš¡ | â­â­â­â­ | é—®ç­”ç³»ç»Ÿ |

    ### ğŸ’¡ ä½¿ç”¨æŠ€å·§

    1. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹**
       - è‹±æ–‡ä»»åŠ¡ï¼š`all-mpnet-base-v2`ï¼ˆè´¨é‡ï¼‰æˆ–`all-MiniLM-L6-v2`ï¼ˆé€Ÿåº¦ï¼‰
       - ä¸­æ–‡/å¤šè¯­è¨€ï¼š`paraphrase-multilingual-mpnet-base-v2`
       - é—®ç­”ç³»ç»Ÿï¼š`multi-qa-*`ç³»åˆ—

    2. **æ€§èƒ½ä¼˜åŒ–**
       - å¢å¤§`batch_size`ï¼ˆ32-128ï¼‰
       - ä½¿ç”¨GPUï¼š`device='cuda'`
       - é¢„å…ˆç¼–ç é™æ€æ–‡æ¡£
       - ä½¿ç”¨`normalize_embeddings=True`ç®€åŒ–ç›¸ä¼¼åº¦è®¡ç®—

    3. **ç›¸ä¼¼åº¦è®¡ç®—**
       - å½’ä¸€åŒ–å‘é‡åï¼Œä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯
       - ä½¿ç”¨`util.semantic_search()`è¿›è¡Œé«˜æ•ˆæœç´¢
       - é˜ˆå€¼å»ºè®®ï¼š>0.7ä¸ºé«˜ç›¸ä¼¼ï¼Œ0.5-0.7ä¸ºä¸­ç­‰ï¼Œ<0.5ä¸ºä½ç›¸ä¼¼

    4. **å†…å­˜ç®¡ç†**
       - å¤§è§„æ¨¡æ•°æ®åˆ†æ‰¹å¤„ç†
       - ä½¿ç”¨`convert_to_tensor=False`èŠ‚çœå†…å­˜
       - åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å‘é‡
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ”— èµ„æºé“¾æ¥

    ### å®˜æ–¹èµ„æº
    - ğŸ“– [å®˜æ–¹æ–‡æ¡£](https://www.sbert.net/)
    - ğŸ’» [GitHubä»“åº“](https://github.com/UKPLab/sentence-transformers)
    - ğŸ¤— [HuggingFaceæ¨¡å‹åº“](https://huggingface.co/sentence-transformers)
    - ğŸ“Š [é¢„è®­ç»ƒæ¨¡å‹åˆ—è¡¨](https://www.sbert.net/docs/pretrained_models.html)

    ### å­¦ä¹ èµ„æº
    - ğŸ“š [å…¥é—¨æ•™ç¨‹](https://www.sbert.net/docs/quickstart.html)
    - ğŸ“ [ç¤ºä¾‹ä»£ç ](https://github.com/UKPLab/sentence-transformers/tree/master/examples)
    - ğŸ“ [è®ºæ–‡](https://arxiv.org/abs/1908.10084)

    ### ç¤¾åŒº
    - ğŸ’¬ [GitHub Discussions](https://github.com/UKPLab/sentence-transformers/discussions)
    - ğŸ› [Issue Tracker](https://github.com/UKPLab/sentence-transformers/issues)
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“ æ€»ç»“

    **Sentence Transformers** æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œæ˜“ç”¨çš„æ–‡æœ¬å‘é‡åŒ–åº“ï¼š

    âœ… **ä¼˜ç‚¹**:
    - ç®€å•æ˜“ç”¨ï¼Œ3è¡Œä»£ç å³å¯å¼€å§‹
    - ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹
    - ä¼˜ç§€çš„å¤šè¯­è¨€æ”¯æŒ
    - é«˜æ€§èƒ½æ¨ç†
    - æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒ

    ğŸ¯ **é€‚ç”¨åœºæ™¯**:
    - è¯­ä¹‰æœç´¢å’Œä¿¡æ¯æ£€ç´¢
    - æ–‡æœ¬èšç±»å’Œåˆ†ç±»
    - é—®ç­”ç³»ç»Ÿ
    - æ¨èç³»ç»Ÿ
    - é‡å¤æ£€æµ‹

    ğŸ’¡ **å¿«é€Ÿå¼€å§‹**:
    ```python
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    embeddings = model.encode(['ä½ å¥½ä¸–ç•Œ', 'Hello World'])
    ```

    ğŸš€ **ä¸‹ä¸€æ­¥**:
    - å°è¯•ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹
    - åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹
    - é›†æˆåˆ°å®é™…åº”ç”¨ä¸­
    """
    )
    return


if __name__ == "__main__":
    app.run()


