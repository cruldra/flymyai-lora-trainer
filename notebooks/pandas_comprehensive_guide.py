import marimo

__generated_with = "0.16.5"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # ğŸ¼ Pandaså®Œå…¨æŒ‡å—

    ## ğŸ“š ä»€ä¹ˆæ˜¯Pandasï¼Ÿ

    **Pandas**æ˜¯Pythonä¸­æœ€æµè¡Œçš„æ•°æ®åˆ†æå’Œå¤„ç†åº“ï¼Œæä¾›äº†é«˜æ€§èƒ½ã€æ˜“ç”¨çš„æ•°æ®ç»“æ„å’Œæ•°æ®åˆ†æå·¥å…·ã€‚

    ### æ ¸å¿ƒç‰¹ç‚¹

    - **å¼ºå¤§çš„æ•°æ®ç»“æ„**ï¼šSeriesï¼ˆä¸€ç»´ï¼‰å’ŒDataFrameï¼ˆäºŒç»´ï¼‰
    - **çµæ´»çš„æ•°æ®æ“ä½œ**ï¼šç´¢å¼•ã€åˆ‡ç‰‡ã€è¿‡æ»¤ã€åˆ†ç»„ã€èšåˆ
    - **æ•°æ®æ¸…æ´—**ï¼šå¤„ç†ç¼ºå¤±å€¼ã€é‡å¤å€¼ã€å¼‚å¸¸å€¼
    - **æ•°æ®è½¬æ¢**ï¼šåˆå¹¶ã€è¿æ¥ã€é‡å¡‘ã€é€è§†
    - **æ—¶é—´åºåˆ—**ï¼šå¼ºå¤§çš„æ—¥æœŸæ—¶é—´å¤„ç†èƒ½åŠ›
    - **IOå·¥å…·**ï¼šè¯»å†™CSVã€Excelã€SQLã€JSONç­‰å¤šç§æ ¼å¼
    - **é«˜æ€§èƒ½**ï¼šåŸºäºNumPyæ„å»ºï¼Œè¿ç®—é€Ÿåº¦å¿«

    ### ä¸»è¦åº”ç”¨åœºæ™¯

    - ğŸ“Š æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
    - ğŸ“ˆ æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰
    - ğŸ”„ æ•°æ®è½¬æ¢å’Œç‰¹å¾å·¥ç¨‹
    - ğŸ“‰ æ—¶é—´åºåˆ—åˆ†æ
    - ğŸ“‹ æŠ¥è¡¨ç”Ÿæˆå’Œæ•°æ®å¯è§†åŒ–
    - ğŸ¤– æœºå™¨å­¦ä¹ æ•°æ®å‡†å¤‡

    ### æœ¬æŒ‡å—å†…å®¹

    æœ¬ç¬”è®°æœ¬å°†å…¨é¢ä»‹ç»Pandasçš„æ ¸å¿ƒæ¦‚å¿µå’Œå¸¸ç”¨APIï¼ŒåŒ…æ‹¬ï¼š

    1. **æ•°æ®ç»“æ„**ï¼šSerieså’ŒDataFrame
    2. **æ•°æ®åˆ›å»º**ï¼šä»å„ç§æ•°æ®æºåˆ›å»º
    3. **æ•°æ®æŸ¥çœ‹**ï¼šæŸ¥çœ‹å’Œæ£€æŸ¥æ•°æ®
    4. **æ•°æ®é€‰æ‹©**ï¼šç´¢å¼•ã€åˆ‡ç‰‡ã€è¿‡æ»¤
    5. **æ•°æ®æ¸…æ´—**ï¼šå¤„ç†ç¼ºå¤±å€¼å’Œé‡å¤å€¼
    6. **æ•°æ®è½¬æ¢**ï¼šæ’åºã€æ˜ å°„ã€åº”ç”¨å‡½æ•°
    7. **æ•°æ®èšåˆ**ï¼šåˆ†ç»„å’Œèšåˆæ“ä½œ
    8. **æ•°æ®åˆå¹¶**ï¼šåˆå¹¶ã€è¿æ¥ã€æ‹¼æ¥
    9. **æ•°æ®é‡å¡‘**ï¼šé€è§†ã€å †å ã€èåˆ
    10. **æ—¶é—´åºåˆ—**ï¼šæ—¥æœŸæ—¶é—´å¤„ç†
    11. **æ•°æ®IO**ï¼šè¯»å†™å„ç§æ ¼å¼
    12. **å®æˆ˜æ¡ˆä¾‹**ï¼šå®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹
    """
    )
    return


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _():
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    import warnings
    warnings.filterwarnings('ignore')

    print(f"âœ… Pandasç‰ˆæœ¬: {pd.__version__}")
    print(f"âœ… NumPyç‰ˆæœ¬: {np.__version__}")
    return datetime, np, pd, timedelta, warnings


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“Š 1. æ ¸å¿ƒæ•°æ®ç»“æ„

    Pandasæœ‰ä¸¤ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„ï¼š

    ### Seriesï¼ˆä¸€ç»´æ•°æ®ï¼‰

    **Series**æ˜¯å¸¦æ ‡ç­¾çš„ä¸€ç»´æ•°ç»„ï¼Œå¯ä»¥å­˜å‚¨ä»»ä½•æ•°æ®ç±»å‹ã€‚

    **ç‰¹ç‚¹**ï¼š
    - ç±»ä¼¼äºPythonçš„åˆ—è¡¨æˆ–NumPyæ•°ç»„
    - æ¯ä¸ªå…ƒç´ éƒ½æœ‰ä¸€ä¸ªæ ‡ç­¾ï¼ˆç´¢å¼•ï¼‰
    - å¯ä»¥é€šè¿‡æ ‡ç­¾æˆ–ä½ç½®è®¿é—®å…ƒç´ 
    """
    )
    return


@app.cell
def _(np, pd):
    print("=" * 60)
    print("ğŸ“Š Seriesç¤ºä¾‹")
    print("=" * 60)

    # 1. ä»åˆ—è¡¨åˆ›å»ºSeries
    series_from_list = pd.Series([10, 20, 30, 40, 50])
    print("\n1ï¸âƒ£ ä»åˆ—è¡¨åˆ›å»ºSeries:")
    print(series_from_list)

    # 2. å¸¦è‡ªå®šä¹‰ç´¢å¼•çš„Series
    series_with_index = pd.Series([10, 20, 30, 40, 50], 
                                   index=['a', 'b', 'c', 'd', 'e'])
    print("\n2ï¸âƒ£ å¸¦è‡ªå®šä¹‰ç´¢å¼•çš„Series:")
    print(series_with_index)

    # 3. ä»å­—å…¸åˆ›å»ºSeries
    series_from_dict = pd.Series({
        'åŒ—äº¬': 2154,
        'ä¸Šæµ·': 2428,
        'å¹¿å·': 1868,
        'æ·±åœ³': 1756
    })
    print("\n3ï¸âƒ£ ä»å­—å…¸åˆ›å»ºSeriesï¼ˆåŸå¸‚äººå£ï¼Œä¸‡äººï¼‰:")
    print(series_from_dict)

    # 4. Seriesçš„åŸºæœ¬å±æ€§
    print("\n4ï¸âƒ£ Seriesçš„åŸºæœ¬å±æ€§:")
    print(f"   æ•°æ®ç±»å‹: {series_from_dict.dtype}")
    print(f"   å½¢çŠ¶: {series_from_dict.shape}")
    print(f"   å¤§å°: {series_from_dict.size}")
    print(f"   ç´¢å¼•: {series_from_dict.index.tolist()}")
    print(f"   å€¼: {series_from_dict.values}")

    # 5. Seriesçš„åŸºæœ¬æ“ä½œ
    print("\n5ï¸âƒ£ Seriesçš„åŸºæœ¬æ“ä½œ:")
    print(f"   æœ€å¤§å€¼: {series_from_dict.max()}")
    print(f"   æœ€å°å€¼: {series_from_dict.min()}")
    print(f"   å¹³å‡å€¼: {series_from_dict.mean():.2f}")
    print(f"   æ€»å’Œ: {series_from_dict.sum()}")

    return series_from_dict, series_from_list, series_with_index


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### DataFrameï¼ˆäºŒç»´æ•°æ®ï¼‰

    **DataFrame**æ˜¯å¸¦æ ‡ç­¾çš„äºŒç»´è¡¨æ ¼æ•°æ®ç»“æ„ï¼Œç±»ä¼¼äºExcelè¡¨æ ¼æˆ–SQLè¡¨ã€‚

    **ç‰¹ç‚¹**ï¼š
    - æ¯åˆ—å¯ä»¥æ˜¯ä¸åŒçš„æ•°æ®ç±»å‹
    - æœ‰è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼•
    - å¯ä»¥çœ‹ä½œæ˜¯Seriesçš„å­—å…¸
    - æ˜¯Pandasä¸­æœ€å¸¸ç”¨çš„æ•°æ®ç»“æ„
    """
    )
    return


@app.cell
def _(np, pd):
    print("=" * 60)
    print("ğŸ“‹ DataFrameç¤ºä¾‹")
    print("=" * 60)

    # 1. ä»å­—å…¸åˆ›å»ºDataFrame
    data_dict = {
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],
        'å¹´é¾„': [25, 30, 35, 28, 32],
        'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·'],
        'è–ªèµ„': [15000, 18000, 16000, 17000, 19000]
    }
    df_from_dict = pd.DataFrame(data_dict)
    print("\n1ï¸âƒ£ ä»å­—å…¸åˆ›å»ºDataFrame:")
    print(df_from_dict)

    # 2. ä»åˆ—è¡¨çš„åˆ—è¡¨åˆ›å»ºDataFrame
    data_list = [
        ['å¼ ä¸‰', 25, 'åŒ—äº¬', 15000],
        ['æå››', 30, 'ä¸Šæµ·', 18000],
        ['ç‹äº”', 35, 'å¹¿å·', 16000]
    ]
    df_from_list = pd.DataFrame(data_list, 
                                 columns=['å§“å', 'å¹´é¾„', 'åŸå¸‚', 'è–ªèµ„'])
    print("\n2ï¸âƒ£ ä»åˆ—è¡¨åˆ›å»ºDataFrame:")
    print(df_from_list)

    # 3. DataFrameçš„åŸºæœ¬å±æ€§
    print("\n3ï¸âƒ£ DataFrameçš„åŸºæœ¬å±æ€§:")
    print(f"   å½¢çŠ¶: {df_from_dict.shape}")
    print(f"   è¡Œæ•°: {len(df_from_dict)}")
    print(f"   åˆ—æ•°: {len(df_from_dict.columns)}")
    print(f"   åˆ—å: {df_from_dict.columns.tolist()}")
    print(f"   ç´¢å¼•: {df_from_dict.index.tolist()}")
    print(f"   æ•°æ®ç±»å‹:\n{df_from_dict.dtypes}")

    # 4. DataFrameçš„åŸºæœ¬ä¿¡æ¯
    print("\n4ï¸âƒ£ DataFrameçš„åŸºæœ¬ä¿¡æ¯:")
    print(df_from_dict.info())

    return data_dict, data_list, df_from_dict, df_from_list


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ‘€ 2. æ•°æ®æŸ¥çœ‹å’Œæ£€æŸ¥

    Pandasæä¾›äº†å¤šç§æ–¹æ³•æ¥å¿«é€ŸæŸ¥çœ‹å’Œæ£€æŸ¥æ•°æ®ã€‚
    """
    )
    return


@app.cell
def _(np, pd):
    print("=" * 60)
    print("ğŸ‘€ æ•°æ®æŸ¥çœ‹ç¤ºä¾‹")
    print("=" * 60)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    sample_df = pd.DataFrame({
        'æ—¥æœŸ': pd.date_range('2024-01-01', periods=100),
        'é”€å”®é¢': np.random.randint(1000, 10000, 100),
        'æˆæœ¬': np.random.randint(500, 5000, 100),
        'åœ°åŒº': np.random.choice(['åŒ—åŒº', 'å—åŒº', 'ä¸œåŒº', 'è¥¿åŒº'], 100),
        'äº§å“': np.random.choice(['äº§å“A', 'äº§å“B', 'äº§å“C'], 100)
    })

    # 1. æŸ¥çœ‹å‰å‡ è¡Œ
    print("\n1ï¸âƒ£ æŸ¥çœ‹å‰5è¡Œ (head):")
    print(sample_df.head())

    # 2. æŸ¥çœ‹åå‡ è¡Œ
    print("\n2ï¸âƒ£ æŸ¥çœ‹å3è¡Œ (tail):")
    print(sample_df.tail(3))

    # 3. éšæœºæŠ½æ ·
    print("\n3ï¸âƒ£ éšæœºæŠ½æ ·3è¡Œ (sample):")
    print(sample_df.sample(3))

    # 4. æè¿°æ€§ç»Ÿè®¡
    print("\n4ï¸âƒ£ æè¿°æ€§ç»Ÿè®¡ (describe):")
    print(sample_df.describe())

    # 5. æŸ¥çœ‹æ•°æ®ç±»å‹
    print("\n5ï¸âƒ£ æ•°æ®ç±»å‹ (dtypes):")
    print(sample_df.dtypes)

    # 6. æŸ¥çœ‹å”¯ä¸€å€¼
    print("\n6ï¸âƒ£ åœ°åŒºçš„å”¯ä¸€å€¼:")
    print(f"   å”¯ä¸€å€¼: {sample_df['åœ°åŒº'].unique()}")
    print(f"   å”¯ä¸€å€¼æ•°é‡: {sample_df['åœ°åŒº'].nunique()}")
    print(f"   å€¼è®¡æ•°:\n{sample_df['åœ°åŒº'].value_counts()}")

    return (sample_df,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ¯ 3. æ•°æ®é€‰æ‹©å’Œç´¢å¼•

    Pandasæä¾›äº†å¤šç§æ–¹å¼æ¥é€‰æ‹©å’Œè®¿é—®æ•°æ®ã€‚
    """
    )
    return


@app.cell
def _(sample_df):
    print("=" * 60)
    print("ğŸ¯ æ•°æ®é€‰æ‹©ç¤ºä¾‹")
    print("=" * 60)

    # 1. é€‰æ‹©å•åˆ—
    print("\n1ï¸âƒ£ é€‰æ‹©å•åˆ—:")
    print(sample_df['é”€å”®é¢'].head())

    # 2. é€‰æ‹©å¤šåˆ—
    print("\n2ï¸âƒ£ é€‰æ‹©å¤šåˆ—:")
    print(sample_df[['æ—¥æœŸ', 'é”€å”®é¢', 'åœ°åŒº']].head())

    # 3. ä½¿ç”¨locæŒ‰æ ‡ç­¾é€‰æ‹©
    print("\n3ï¸âƒ£ ä½¿ç”¨locé€‰æ‹©å‰3è¡Œ:")
    print(sample_df.loc[0:2, ['æ—¥æœŸ', 'é”€å”®é¢']])

    # 4. ä½¿ç”¨ilocæŒ‰ä½ç½®é€‰æ‹©
    print("\n4ï¸âƒ£ ä½¿ç”¨ilocé€‰æ‹©å‰3è¡Œçš„å‰2åˆ—:")
    print(sample_df.iloc[0:3, 0:2])

    # 5. æ¡ä»¶è¿‡æ»¤
    print("\n5ï¸âƒ£ æ¡ä»¶è¿‡æ»¤ï¼ˆé”€å”®é¢>5000ï¼‰:")
    high_sales = sample_df[sample_df['é”€å”®é¢'] > 5000]
    print(f"   ç¬¦åˆæ¡ä»¶çš„è®°å½•æ•°: {len(high_sales)}")
    print(high_sales.head())

    # 6. å¤šæ¡ä»¶è¿‡æ»¤
    print("\n6ï¸âƒ£ å¤šæ¡ä»¶è¿‡æ»¤ï¼ˆé”€å”®é¢>5000 ä¸” åœ°åŒº='åŒ—åŒº'ï¼‰:")
    complex_filter = sample_df[(sample_df['é”€å”®é¢'] > 5000) & 
                                (sample_df['åœ°åŒº'] == 'åŒ—åŒº')]
    print(f"   ç¬¦åˆæ¡ä»¶çš„è®°å½•æ•°: {len(complex_filter)}")
    print(complex_filter.head())

    # 7. ä½¿ç”¨isinè¿‡æ»¤
    print("\n7ï¸âƒ£ ä½¿ç”¨isinè¿‡æ»¤ï¼ˆäº§å“ä¸ºAæˆ–Bï¼‰:")
    product_filter = sample_df[sample_df['äº§å“'].isin(['äº§å“A', 'äº§å“B'])]
    print(f"   ç¬¦åˆæ¡ä»¶çš„è®°å½•æ•°: {len(product_filter)}")

    return complex_filter, high_sales, product_filter


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ§¹ 4. æ•°æ®æ¸…æ´—

    æ•°æ®æ¸…æ´—æ˜¯æ•°æ®åˆ†æçš„é‡è¦æ­¥éª¤ï¼ŒåŒ…æ‹¬å¤„ç†ç¼ºå¤±å€¼ã€é‡å¤å€¼å’Œå¼‚å¸¸å€¼ã€‚
    """
    )
    return


@app.cell
def _(np, pd):
    print("=" * 60)
    print("ğŸ§¹ æ•°æ®æ¸…æ´—ç¤ºä¾‹")
    print("=" * 60)

    # åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼å’Œé‡å¤å€¼çš„æ•°æ®
    dirty_data = pd.DataFrame({
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'å¼ ä¸‰', 'èµµå…­', None, 'é’±ä¸ƒ'],
        'å¹´é¾„': [25, 30, None, 25, 28, 32, 35],
        'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'åŒ—äº¬', None, 'æ­å·', 'æ·±åœ³'],
        'è–ªèµ„': [15000, 18000, 16000, 15000, 17000, 19000, None]
    })

    print("\nåŸå§‹æ•°æ®ï¼ˆåŒ…å«ç¼ºå¤±å€¼å’Œé‡å¤å€¼ï¼‰:")
    print(dirty_data)

    # 1. æ£€æŸ¥ç¼ºå¤±å€¼
    print("\n1ï¸âƒ£ æ£€æŸ¥ç¼ºå¤±å€¼:")
    print(f"   æ¯åˆ—ç¼ºå¤±å€¼æ•°é‡:\n{dirty_data.isnull().sum()}")
    print(f"   æ€»ç¼ºå¤±å€¼æ•°é‡: {dirty_data.isnull().sum().sum()}")

    # 2. åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
    print("\n2ï¸âƒ£ åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ:")
    cleaned_dropna = dirty_data.dropna()
    print(cleaned_dropna)

    # 3. å¡«å……ç¼ºå¤±å€¼
    print("\n3ï¸âƒ£ å¡«å……ç¼ºå¤±å€¼:")
    filled_data = dirty_data.copy()
    filled_data['å¹´é¾„'].fillna(filled_data['å¹´é¾„'].mean(), inplace=True)
    filled_data['åŸå¸‚'].fillna('æœªçŸ¥', inplace=True)
    filled_data['è–ªèµ„'].fillna(filled_data['è–ªèµ„'].median(), inplace=True)
    filled_data['å§“å'].fillna('åŒ¿å', inplace=True)
    print(filled_data)

    # 4. æ£€æŸ¥é‡å¤å€¼
    print("\n4ï¸âƒ£ æ£€æŸ¥é‡å¤å€¼:")
    print(f"   é‡å¤è¡Œæ•°: {dirty_data.duplicated().sum()}")
    print(f"   é‡å¤çš„è¡Œ:")
    print(dirty_data[dirty_data.duplicated(keep=False)])

    # 5. åˆ é™¤é‡å¤å€¼
    print("\n5ï¸âƒ£ åˆ é™¤é‡å¤å€¼:")
    deduped_data = dirty_data.drop_duplicates()
    print(deduped_data)

    # 6. æ•°æ®ç±»å‹è½¬æ¢
    print("\n6ï¸âƒ£ æ•°æ®ç±»å‹è½¬æ¢:")
    type_converted = filled_data.copy()
    type_converted['å¹´é¾„'] = type_converted['å¹´é¾„'].astype(int)
    print(f"   è½¬æ¢åçš„æ•°æ®ç±»å‹:\n{type_converted.dtypes}")

    return cleaned_dropna, deduped_data, dirty_data, filled_data, type_converted


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ”„ 5. æ•°æ®è½¬æ¢

    æ•°æ®è½¬æ¢åŒ…æ‹¬æ’åºã€æ˜ å°„ã€åº”ç”¨å‡½æ•°ç­‰æ“ä½œã€‚
    """
    )
    return


@app.cell
def _(np, pd, sample_df):
    print("=" * 60)
    print("ğŸ”„ æ•°æ®è½¬æ¢ç¤ºä¾‹")
    print("=" * 60)

    # 1. æ’åº
    print("\n1ï¸âƒ£ æŒ‰é”€å”®é¢é™åºæ’åº:")
    sorted_df = sample_df.sort_values('é”€å”®é¢', ascending=False)
    print(sorted_df.head())

    # 2. å¤šåˆ—æ’åº
    print("\n2ï¸âƒ£ æŒ‰åœ°åŒºå‡åºã€é”€å”®é¢é™åºæ’åº:")
    multi_sorted = sample_df.sort_values(['åœ°åŒº', 'é”€å”®é¢'],
                                         ascending=[True, False])
    print(multi_sorted.head(10))

    # 3. æ·»åŠ æ–°åˆ—
    print("\n3ï¸âƒ£ æ·»åŠ åˆ©æ¶¦åˆ—:")
    transform_df = sample_df.copy()
    transform_df['åˆ©æ¶¦'] = transform_df['é”€å”®é¢'] - transform_df['æˆæœ¬']
    transform_df['åˆ©æ¶¦ç‡'] = (transform_df['åˆ©æ¶¦'] / transform_df['é”€å”®é¢'] * 100).round(2)
    print(transform_df[['æ—¥æœŸ', 'é”€å”®é¢', 'æˆæœ¬', 'åˆ©æ¶¦', 'åˆ©æ¶¦ç‡']].head())

    # 4. ä½¿ç”¨applyåº”ç”¨å‡½æ•°
    print("\n4ï¸âƒ£ ä½¿ç”¨applyåº”ç”¨å‡½æ•°:")
    def categorize_sales(sales):
        if sales >= 7000:
            return 'é«˜'
        elif sales >= 4000:
            return 'ä¸­'
        else:
            return 'ä½'

    transform_df['é”€å”®ç­‰çº§'] = transform_df['é”€å”®é¢'].apply(categorize_sales)
    print(transform_df[['é”€å”®é¢', 'é”€å”®ç­‰çº§']].head(10))

    # 5. ä½¿ç”¨mapæ˜ å°„
    print("\n5ï¸âƒ£ ä½¿ç”¨mapæ˜ å°„:")
    region_map = {'åŒ—åŒº': 'North', 'å—åŒº': 'South', 'ä¸œåŒº': 'East', 'è¥¿åŒº': 'West'}
    transform_df['Region_EN'] = transform_df['åœ°åŒº'].map(region_map)
    print(transform_df[['åœ°åŒº', 'Region_EN']].head())

    # 6. ä½¿ç”¨replaceæ›¿æ¢å€¼
    print("\n6ï¸âƒ£ ä½¿ç”¨replaceæ›¿æ¢å€¼:")
    replaced_df = transform_df.copy()
    replaced_df['äº§å“'] = replaced_df['äº§å“'].replace({
        'äº§å“A': 'Product-A',
        'äº§å“B': 'Product-B',
        'äº§å“C': 'Product-C'
    })
    print(replaced_df[['äº§å“']].head())

    return categorize_sales, multi_sorted, region_map, replaced_df, sorted_df, transform_df


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“Š 6. æ•°æ®èšåˆå’Œåˆ†ç»„

    åˆ†ç»„èšåˆæ˜¯æ•°æ®åˆ†æä¸­æœ€å¸¸ç”¨çš„æ“ä½œä¹‹ä¸€ã€‚
    """
    )
    return


@app.cell
def _(sample_df):
    print("=" * 60)
    print("ğŸ“Š æ•°æ®èšåˆå’Œåˆ†ç»„ç¤ºä¾‹")
    print("=" * 60)

    # 1. æŒ‰å•åˆ—åˆ†ç»„èšåˆ
    print("\n1ï¸âƒ£ æŒ‰åœ°åŒºåˆ†ç»„ï¼Œè®¡ç®—å¹³å‡é”€å”®é¢:")
    region_avg = sample_df.groupby('åœ°åŒº')['é”€å”®é¢'].mean().round(2)
    print(region_avg)

    # 2. æŒ‰å¤šåˆ—åˆ†ç»„
    print("\n2ï¸âƒ£ æŒ‰åœ°åŒºå’Œäº§å“åˆ†ç»„ï¼Œè®¡ç®—æ€»é”€å”®é¢:")
    multi_group = sample_df.groupby(['åœ°åŒº', 'äº§å“'])['é”€å”®é¢'].sum()
    print(multi_group)

    # 3. å¤šç§èšåˆå‡½æ•°
    print("\n3ï¸âƒ£ æŒ‰åœ°åŒºåˆ†ç»„ï¼Œåº”ç”¨å¤šç§èšåˆå‡½æ•°:")
    agg_result = sample_df.groupby('åœ°åŒº')['é”€å”®é¢'].agg(['sum', 'mean', 'min', 'max', 'count'])
    agg_result.columns = ['æ€»é”€å”®é¢', 'å¹³å‡é”€å”®é¢', 'æœ€å°é”€å”®é¢', 'æœ€å¤§é”€å”®é¢', 'è®°å½•æ•°']
    print(agg_result)

    # 4. å¯¹ä¸åŒåˆ—åº”ç”¨ä¸åŒèšåˆå‡½æ•°
    print("\n4ï¸âƒ£ å¯¹ä¸åŒåˆ—åº”ç”¨ä¸åŒèšåˆå‡½æ•°:")
    complex_agg = sample_df.groupby('åœ°åŒº').agg({
        'é”€å”®é¢': ['sum', 'mean'],
        'æˆæœ¬': ['sum', 'mean']
    }).round(2)
    print(complex_agg)

    # 5. ä½¿ç”¨transform
    print("\n5ï¸âƒ£ ä½¿ç”¨transformæ·»åŠ ç»„å†…å¹³å‡å€¼:")
    transform_result = sample_df.copy()
    transform_result['åœ°åŒºå¹³å‡é”€å”®é¢'] = sample_df.groupby('åœ°åŒº')['é”€å”®é¢'].transform('mean').round(2)
    print(transform_result[['åœ°åŒº', 'é”€å”®é¢', 'åœ°åŒºå¹³å‡é”€å”®é¢']].head(10))

    # 6. é€è§†è¡¨
    print("\n6ï¸âƒ£ åˆ›å»ºé€è§†è¡¨:")
    pivot_result = sample_df.pivot_table(
        values='é”€å”®é¢',
        index='åœ°åŒº',
        columns='äº§å“',
        aggfunc='mean',
        fill_value=0
    ).round(2)
    print(pivot_result)

    return agg_result, complex_agg, multi_group, pivot_result, region_avg, transform_result


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ”— 7. æ•°æ®åˆå¹¶å’Œè¿æ¥

    Pandasæä¾›äº†å¤šç§æ–¹å¼æ¥åˆå¹¶å’Œè¿æ¥æ•°æ®é›†ã€‚
    """
    )
    return


@app.cell
def _(pd):
    print("=" * 60)
    print("ğŸ”— æ•°æ®åˆå¹¶å’Œè¿æ¥ç¤ºä¾‹")
    print("=" * 60)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    employees = pd.DataFrame({
        'å‘˜å·¥ID': [1, 2, 3, 4],
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
        'éƒ¨é—¨ID': [101, 102, 101, 103]
    })

    departments = pd.DataFrame({
        'éƒ¨é—¨ID': [101, 102, 103, 104],
        'éƒ¨é—¨åç§°': ['æŠ€æœ¯éƒ¨', 'é”€å”®éƒ¨', 'äººäº‹éƒ¨', 'è´¢åŠ¡éƒ¨']
    })

    salaries = pd.DataFrame({
        'å‘˜å·¥ID': [1, 2, 3, 5],
        'è–ªèµ„': [15000, 18000, 16000, 20000]
    })

    print("\nå‘˜å·¥è¡¨:")
    print(employees)
    print("\néƒ¨é—¨è¡¨:")
    print(departments)
    print("\nè–ªèµ„è¡¨:")
    print(salaries)

    # 1. å†…è¿æ¥ï¼ˆinner joinï¼‰
    print("\n1ï¸âƒ£ å†…è¿æ¥ï¼ˆå‘˜å·¥å’Œéƒ¨é—¨ï¼‰:")
    inner_join = pd.merge(employees, departments, on='éƒ¨é—¨ID', how='inner')
    print(inner_join)

    # 2. å·¦è¿æ¥ï¼ˆleft joinï¼‰
    print("\n2ï¸âƒ£ å·¦è¿æ¥ï¼ˆå‘˜å·¥å’Œè–ªèµ„ï¼‰:")
    left_join = pd.merge(employees, salaries, on='å‘˜å·¥ID', how='left')
    print(left_join)

    # 3. å³è¿æ¥ï¼ˆright joinï¼‰
    print("\n3ï¸âƒ£ å³è¿æ¥ï¼ˆå‘˜å·¥å’Œè–ªèµ„ï¼‰:")
    right_join = pd.merge(employees, salaries, on='å‘˜å·¥ID', how='right')
    print(right_join)

    # 4. å¤–è¿æ¥ï¼ˆouter joinï¼‰
    print("\n4ï¸âƒ£ å¤–è¿æ¥ï¼ˆå‘˜å·¥å’Œè–ªèµ„ï¼‰:")
    outer_join = pd.merge(employees, salaries, on='å‘˜å·¥ID', how='outer')
    print(outer_join)

    # 5. å¤šè¡¨è¿æ¥
    print("\n5ï¸âƒ£ å¤šè¡¨è¿æ¥:")
    full_info = pd.merge(employees, departments, on='éƒ¨é—¨ID')
    full_info = pd.merge(full_info, salaries, on='å‘˜å·¥ID', how='left')
    print(full_info)

    # 6. concatæ‹¼æ¥
    print("\n6ï¸âƒ£ å‚ç›´æ‹¼æ¥ï¼ˆconcatï¼‰:")
    df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
    df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})
    concat_result = pd.concat([df1, df2], ignore_index=True)
    print(concat_result)

    return concat_result, departments, df1, df2, employees, full_info, inner_join, left_join, outer_join, right_join, salaries


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ”€ 8. æ•°æ®é‡å¡‘

    æ•°æ®é‡å¡‘åŒ…æ‹¬é€è§†ã€å †å ã€èåˆç­‰æ“ä½œã€‚
    """
    )
    return


@app.cell
def _(pd):
    print("=" * 60)
    print("ğŸ”€ æ•°æ®é‡å¡‘ç¤ºä¾‹")
    print("=" * 60)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    sales_data = pd.DataFrame({
        'æ—¥æœŸ': ['2024-01', '2024-01', '2024-02', '2024-02'],
        'äº§å“': ['A', 'B', 'A', 'B'],
        'é”€å”®é¢': [100, 150, 120, 180]
    })

    print("\nåŸå§‹æ•°æ®:")
    print(sales_data)

    # 1. pivot - é€è§†
    print("\n1ï¸âƒ£ ä½¿ç”¨pivoté€è§†:")
    pivoted = sales_data.pivot(index='æ—¥æœŸ', columns='äº§å“', values='é”€å”®é¢')
    print(pivoted)

    # 2. melt - èåˆï¼ˆpivotçš„é€†æ“ä½œï¼‰
    print("\n2ï¸âƒ£ ä½¿ç”¨meltèåˆ:")
    melted = pivoted.reset_index().melt(id_vars='æ—¥æœŸ',
                                         var_name='äº§å“',
                                         value_name='é”€å”®é¢')
    print(melted)

    # 3. stack - å †å 
    print("\n3ï¸âƒ£ ä½¿ç”¨stackå †å :")
    stacked = pivoted.stack()
    print(stacked)

    # 4. unstack - åå †å 
    print("\n4ï¸âƒ£ ä½¿ç”¨unstackåå †å :")
    unstacked = stacked.unstack()
    print(unstacked)

    # 5. å®½æ ¼å¼è½¬é•¿æ ¼å¼
    print("\n5ï¸âƒ£ å®½æ ¼å¼è½¬é•¿æ ¼å¼:")
    wide_data = pd.DataFrame({
        'å­¦ç”Ÿ': ['å¼ ä¸‰', 'æå››', 'ç‹äº”'],
        'è¯­æ–‡': [85, 90, 88],
        'æ•°å­¦': [92, 88, 95],
        'è‹±è¯­': [78, 85, 90]
    })
    print("å®½æ ¼å¼:")
    print(wide_data)

    long_data = wide_data.melt(id_vars='å­¦ç”Ÿ',
                                var_name='ç§‘ç›®',
                                value_name='æˆç»©')
    print("\né•¿æ ¼å¼:")
    print(long_data)

    return long_data, melted, pivoted, sales_data, stacked, unstacked, wide_data


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## â° 9. æ—¶é—´åºåˆ—å¤„ç†

    Pandasæä¾›äº†å¼ºå¤§çš„æ—¶é—´åºåˆ—å¤„ç†åŠŸèƒ½ã€‚
    """
    )
    return


@app.cell
def _(np, pd):
    print("=" * 60)
    print("â° æ—¶é—´åºåˆ—å¤„ç†ç¤ºä¾‹")
    print("=" * 60)

    # 1. åˆ›å»ºæ—¥æœŸèŒƒå›´
    print("\n1ï¸âƒ£ åˆ›å»ºæ—¥æœŸèŒƒå›´:")
    date_range = pd.date_range('2024-01-01', periods=10, freq='D')
    print(date_range)

    # 2. åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
    print("\n2ï¸âƒ£ åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®:")
    ts_data = pd.DataFrame({
        'æ—¥æœŸ': pd.date_range('2024-01-01', periods=30, freq='D'),
        'é”€å”®é¢': np.random.randint(1000, 5000, 30),
        'è®¿å®¢æ•°': np.random.randint(100, 500, 30)
    })
    ts_data.set_index('æ—¥æœŸ', inplace=True)
    print(ts_data.head(10))

    # 3. æ—¥æœŸæ—¶é—´å±æ€§æå–
    print("\n3ï¸âƒ£ æå–æ—¥æœŸæ—¶é—´å±æ€§:")
    ts_extract = ts_data.copy()
    ts_extract['å¹´'] = ts_extract.index.year
    ts_extract['æœˆ'] = ts_extract.index.month
    ts_extract['æ—¥'] = ts_extract.index.day
    ts_extract['æ˜ŸæœŸ'] = ts_extract.index.dayofweek
    ts_extract['æ˜ŸæœŸå'] = ts_extract.index.day_name()
    print(ts_extract.head())

    # 4. æ—¶é—´åºåˆ—é‡é‡‡æ ·
    print("\n4ï¸âƒ£ æŒ‰å‘¨é‡é‡‡æ ·:")
    weekly_data = ts_data.resample('W').sum()
    print(weekly_data)

    # 5. æ»šåŠ¨çª—å£è®¡ç®—
    print("\n5ï¸âƒ£ è®¡ç®—7å¤©ç§»åŠ¨å¹³å‡:")
    ts_rolling = ts_data.copy()
    ts_rolling['é”€å”®é¢_7æ—¥å‡å€¼'] = ts_rolling['é”€å”®é¢'].rolling(window=7).mean().round(2)
    print(ts_rolling.head(10))

    # 6. æ—¶é—´åç§»
    print("\n6ï¸âƒ£ æ—¶é—´åç§»:")
    ts_shift = ts_data.copy()
    ts_shift['æ˜¨æ—¥é”€å”®é¢'] = ts_shift['é”€å”®é¢'].shift(1)
    ts_shift['é”€å”®é¢å˜åŒ–'] = ts_shift['é”€å”®é¢'] - ts_shift['æ˜¨æ—¥é”€å”®é¢']
    print(ts_shift.head(10))

    return date_range, ts_data, ts_extract, ts_rolling, ts_shift, weekly_data


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ’¾ 10. æ•°æ®IOæ“ä½œ

    Pandasæ”¯æŒè¯»å†™å¤šç§æ•°æ®æ ¼å¼ã€‚
    """
    )
    return


@app.cell
def _(pd, sample_df):
    print("=" * 60)
    print("ğŸ’¾ æ•°æ®IOæ“ä½œç¤ºä¾‹")
    print("=" * 60)

    # 1. CSVæ“ä½œ
    print("\n1ï¸âƒ£ CSVæ“ä½œ:")
    csv_file = 'temp_data.csv'
    sample_df.head(10).to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"   å·²ä¿å­˜åˆ°: {csv_file}")

    read_csv = pd.read_csv(csv_file)
    print(f"   è¯»å–çš„æ•°æ®å½¢çŠ¶: {read_csv.shape}")
    print(read_csv.head(3))

    # 2. Excelæ“ä½œï¼ˆéœ€è¦openpyxlåº“ï¼‰
    print("\n2ï¸âƒ£ Excelæ“ä½œ:")
    try:
        excel_file = 'temp_data.xlsx'
        sample_df.head(10).to_excel(excel_file, index=False, sheet_name='é”€å”®æ•°æ®')
        print(f"   å·²ä¿å­˜åˆ°: {excel_file}")

        read_excel = pd.read_excel(excel_file, sheet_name='é”€å”®æ•°æ®')
        print(f"   è¯»å–çš„æ•°æ®å½¢çŠ¶: {read_excel.shape}")
    except ImportError:
        print("   éœ€è¦å®‰è£…openpyxl: pip install openpyxl")

    # 3. JSONæ“ä½œ
    print("\n3ï¸âƒ£ JSONæ“ä½œ:")
    json_file = 'temp_data.json'
    sample_df.head(5).to_json(json_file, orient='records', force_ascii=False, indent=2)
    print(f"   å·²ä¿å­˜åˆ°: {json_file}")

    read_json = pd.read_json(json_file)
    print(f"   è¯»å–çš„æ•°æ®å½¢çŠ¶: {read_json.shape}")

    # 4. ä»å­—å…¸åˆ—è¡¨åˆ›å»º
    print("\n4ï¸âƒ£ ä»å­—å…¸åˆ—è¡¨åˆ›å»ºDataFrame:")
    dict_list = [
        {'å§“å': 'å¼ ä¸‰', 'å¹´é¾„': 25, 'åŸå¸‚': 'åŒ—äº¬'},
        {'å§“å': 'æå››', 'å¹´é¾„': 30, 'åŸå¸‚': 'ä¸Šæµ·'},
        {'å§“å': 'ç‹äº”', 'å¹´é¾„': 35, 'åŸå¸‚': 'å¹¿å·'}
    ]
    df_from_dict_list = pd.DataFrame(dict_list)
    print(df_from_dict_list)

    # 5. è½¬æ¢ä¸ºå­—å…¸
    print("\n5ï¸âƒ£ DataFrameè½¬æ¢ä¸ºå­—å…¸:")
    to_dict_records = df_from_dict_list.to_dict('records')
    print(f"   recordsæ ¼å¼: {to_dict_records}")

    to_dict_list = df_from_dict_list.to_dict('list')
    print(f"   listæ ¼å¼: {to_dict_list}")

    return csv_file, df_from_dict_list, dict_list, excel_file, json_file, read_csv, read_excel, read_json, to_dict_list, to_dict_records


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ¯ 11. å¸¸ç”¨æŠ€å·§å’Œæœ€ä½³å®è·µ

    ä¸€äº›å®ç”¨çš„PandasæŠ€å·§å’Œæœ€ä½³å®è·µã€‚
    """
    )
    return


@app.cell
def _(np, pd):
    print("=" * 60)
    print("ğŸ¯ å¸¸ç”¨æŠ€å·§ç¤ºä¾‹")
    print("=" * 60)

    # 1. é“¾å¼æ“ä½œ
    print("\n1ï¸âƒ£ é“¾å¼æ“ä½œ:")
    chained_result = (pd.DataFrame({
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
        'å¹´é¾„': [25, 30, 35, 28],
        'è–ªèµ„': [15000, 18000, 16000, 17000]
    })
    .assign(ç¨åè–ªèµ„=lambda x: x['è–ªèµ„'] * 0.8)
    .query('å¹´é¾„ > 26')
    .sort_values('è–ªèµ„', ascending=False)
    .reset_index(drop=True)
    )
    print(chained_result)

    # 2. ä½¿ç”¨queryè¿›è¡Œè¿‡æ»¤
    print("\n2ï¸âƒ£ ä½¿ç”¨queryè¿›è¡Œè¿‡æ»¤:")
    query_df = pd.DataFrame({
        'äº§å“': ['A', 'B', 'C', 'A', 'B'],
        'é”€å”®é¢': [100, 200, 150, 180, 220],
        'åœ°åŒº': ['åŒ—', 'å—', 'ä¸œ', 'è¥¿', 'åŒ—']
    })
    filtered = query_df.query('é”€å”®é¢ > 150 and åœ°åŒº == "åŒ—"')
    print(filtered)

    # 3. ä½¿ç”¨evalè¿›è¡Œè®¡ç®—
    print("\n3ï¸âƒ£ ä½¿ç”¨evalè¿›è¡Œè®¡ç®—:")
    eval_df = pd.DataFrame({
        'A': [1, 2, 3],
        'B': [4, 5, 6],
        'C': [7, 8, 9]
    })
    eval_df.eval('D = A + B * C', inplace=True)
    print(eval_df)

    # 4. å†…å­˜ä¼˜åŒ–
    print("\n4ï¸âƒ£ å†…å­˜ä¼˜åŒ–:")
    memory_df = pd.DataFrame({
        'æ•´æ•°åˆ—': np.random.randint(0, 100, 1000),
        'åˆ†ç±»åˆ—': np.random.choice(['A', 'B', 'C'], 1000)
    })

    print(f"   ä¼˜åŒ–å‰å†…å­˜: {memory_df.memory_usage(deep=True).sum() / 1024:.2f} KB")

    memory_df['æ•´æ•°åˆ—'] = memory_df['æ•´æ•°åˆ—'].astype('int8')
    memory_df['åˆ†ç±»åˆ—'] = memory_df['åˆ†ç±»åˆ—'].astype('category')

    print(f"   ä¼˜åŒ–åå†…å­˜: {memory_df.memory_usage(deep=True).sum() / 1024:.2f} KB")

    # 5. ä½¿ç”¨cutè¿›è¡Œåˆ†ç®±
    print("\n5ï¸âƒ£ ä½¿ç”¨cutè¿›è¡Œåˆ†ç®±:")
    cut_df = pd.DataFrame({
        'å¹´é¾„': [18, 25, 35, 45, 55, 65, 75]
    })
    cut_df['å¹´é¾„æ®µ'] = pd.cut(cut_df['å¹´é¾„'],
                              bins=[0, 30, 50, 100],
                              labels=['é’å¹´', 'ä¸­å¹´', 'è€å¹´'])
    print(cut_df)

    # 6. ä½¿ç”¨qcutè¿›è¡Œç­‰é¢‘åˆ†ç®±
    print("\n6ï¸âƒ£ ä½¿ç”¨qcutè¿›è¡Œç­‰é¢‘åˆ†ç®±:")
    qcut_df = pd.DataFrame({
        'åˆ†æ•°': [60, 70, 75, 80, 85, 90, 95, 100]
    })
    qcut_df['ç­‰çº§'] = pd.qcut(qcut_df['åˆ†æ•°'],
                              q=4,
                              labels=['D', 'C', 'B', 'A'])
    print(qcut_df)

    return chained_result, cut_df, eval_df, filtered, memory_df, qcut_df, query_df


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸš€ 12. å®Œæ•´å®æˆ˜æ¡ˆä¾‹

    è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„æ•°æ®åˆ†ææ¡ˆä¾‹æ¥ç»¼åˆè¿ç”¨Pandasçš„å„ç§åŠŸèƒ½ã€‚

    ### åœºæ™¯ï¼šç”µå•†é”€å”®æ•°æ®åˆ†æ

    æˆ‘ä»¬å°†åˆ†æä¸€ä¸ªç”µå•†å¹³å°çš„é”€å”®æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°æ®åŠ è½½å’Œæ¸…æ´—
    - æ¢ç´¢æ€§æ•°æ®åˆ†æ
    - æ•°æ®è½¬æ¢å’Œç‰¹å¾å·¥ç¨‹
    - æ•°æ®èšåˆå’Œå¯è§†åŒ–
    """
    )
    return


@app.cell
def _(np, pd):
    print("=" * 60)
    print("ğŸš€ å®Œæ•´å®æˆ˜æ¡ˆä¾‹ï¼šç”µå•†é”€å”®æ•°æ®åˆ†æ")
    print("=" * 60)

    # 1. åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    print("\nğŸ“Š æ­¥éª¤1ï¼šåˆ›å»ºæ¨¡æ‹Ÿç”µå•†é”€å”®æ•°æ®")
    np.random.seed(42)
    n_records = 1000

    ecommerce_data = pd.DataFrame({
        'è®¢å•ID': range(1, n_records + 1),
        'æ—¥æœŸ': pd.date_range('2024-01-01', periods=n_records, freq='H'),
        'ç”¨æˆ·ID': np.random.randint(1000, 2000, n_records),
        'äº§å“ç±»åˆ«': np.random.choice(['ç”µå­äº§å“', 'æœè£…', 'é£Ÿå“', 'å›¾ä¹¦', 'å®¶å±…'], n_records),
        'äº§å“åç§°': [f'äº§å“{i}' for i in np.random.randint(1, 100, n_records)],
        'æ•°é‡': np.random.randint(1, 10, n_records),
        'å•ä»·': np.random.uniform(10, 1000, n_records).round(2),
        'åœ°åŒº': np.random.choice(['ååŒ—', 'åä¸œ', 'åå—', 'åä¸­', 'è¥¿å—', 'ä¸œåŒ—'], n_records),
        'æ”¯ä»˜æ–¹å¼': np.random.choice(['æ”¯ä»˜å®', 'å¾®ä¿¡', 'ä¿¡ç”¨å¡', 'è´§åˆ°ä»˜æ¬¾'], n_records)
    })

    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
    ecommerce_data.loc[np.random.choice(ecommerce_data.index, 20), 'åœ°åŒº'] = None
    ecommerce_data.loc[np.random.choice(ecommerce_data.index, 15), 'æ”¯ä»˜æ–¹å¼'] = None

    print(f"æ•°æ®å½¢çŠ¶: {ecommerce_data.shape}")
    print("\nå‰5æ¡è®°å½•:")
    print(ecommerce_data.head())

    # 2. æ•°æ®æ¸…æ´—
    print("\nğŸ§¹ æ­¥éª¤2ï¼šæ•°æ®æ¸…æ´—")

    # æ£€æŸ¥ç¼ºå¤±å€¼
    print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{ecommerce_data.isnull().sum()}")

    # å¡«å……ç¼ºå¤±å€¼
    ecommerce_data['åœ°åŒº'].fillna('æœªçŸ¥', inplace=True)
    ecommerce_data['æ”¯ä»˜æ–¹å¼'].fillna('å…¶ä»–', inplace=True)

    # æ£€æŸ¥é‡å¤å€¼
    duplicates = ecommerce_data.duplicated().sum()
    print(f"é‡å¤è®°å½•æ•°: {duplicates}")

    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")

    # 3. ç‰¹å¾å·¥ç¨‹
    print("\nğŸ”§ æ­¥éª¤3ï¼šç‰¹å¾å·¥ç¨‹")

    # è®¡ç®—æ€»é‡‘é¢
    ecommerce_data['æ€»é‡‘é¢'] = (ecommerce_data['æ•°é‡'] * ecommerce_data['å•ä»·']).round(2)

    # æå–æ—¶é—´ç‰¹å¾
    ecommerce_data['å¹´'] = ecommerce_data['æ—¥æœŸ'].dt.year
    ecommerce_data['æœˆ'] = ecommerce_data['æ—¥æœŸ'].dt.month
    ecommerce_data['æ—¥'] = ecommerce_data['æ—¥æœŸ'].dt.day
    ecommerce_data['å°æ—¶'] = ecommerce_data['æ—¥æœŸ'].dt.hour
    ecommerce_data['æ˜ŸæœŸ'] = ecommerce_data['æ—¥æœŸ'].dt.dayofweek
    ecommerce_data['æ˜¯å¦å‘¨æœ«'] = ecommerce_data['æ˜ŸæœŸ'].isin([5, 6])

    print("æ–°å¢ç‰¹å¾:")
    print(ecommerce_data[['æ—¥æœŸ', 'æ€»é‡‘é¢', 'å¹´', 'æœˆ', 'æ—¥', 'å°æ—¶', 'æ˜ŸæœŸ', 'æ˜¯å¦å‘¨æœ«']].head())

    return duplicates, ecommerce_data, n_records


@app.cell
def _(ecommerce_data):
    print("=" * 60)
    print("ğŸ“ˆ æ­¥éª¤4ï¼šæ¢ç´¢æ€§æ•°æ®åˆ†æ")
    print("=" * 60)

    # 1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    print("\n1ï¸âƒ£ æ•°å€¼åˆ—çš„æè¿°æ€§ç»Ÿè®¡:")
    print(ecommerce_data[['æ•°é‡', 'å•ä»·', 'æ€»é‡‘é¢']].describe())

    # 2. æŒ‰äº§å“ç±»åˆ«åˆ†æ
    print("\n2ï¸âƒ£ æŒ‰äº§å“ç±»åˆ«åˆ†æ:")
    category_analysis = ecommerce_data.groupby('äº§å“ç±»åˆ«').agg({
        'è®¢å•ID': 'count',
        'æ€»é‡‘é¢': ['sum', 'mean', 'max'],
        'æ•°é‡': 'sum'
    }).round(2)
    category_analysis.columns = ['è®¢å•æ•°', 'æ€»é”€å”®é¢', 'å¹³å‡è®¢å•é‡‘é¢', 'æœ€å¤§è®¢å•é‡‘é¢', 'æ€»é”€é‡']
    category_analysis = category_analysis.sort_values('æ€»é”€å”®é¢', ascending=False)
    print(category_analysis)

    # 3. æŒ‰åœ°åŒºåˆ†æ
    print("\n3ï¸âƒ£ æŒ‰åœ°åŒºåˆ†æ:")
    region_analysis = ecommerce_data.groupby('åœ°åŒº').agg({
        'è®¢å•ID': 'count',
        'æ€»é‡‘é¢': 'sum'
    }).round(2)
    region_analysis.columns = ['è®¢å•æ•°', 'æ€»é”€å”®é¢']
    region_analysis = region_analysis.sort_values('æ€»é”€å”®é¢', ascending=False)
    print(region_analysis)

    # 4. æŒ‰æ”¯ä»˜æ–¹å¼åˆ†æ
    print("\n4ï¸âƒ£ æŒ‰æ”¯ä»˜æ–¹å¼åˆ†æ:")
    payment_analysis = ecommerce_data['æ”¯ä»˜æ–¹å¼'].value_counts()
    payment_pct = (payment_analysis / len(ecommerce_data) * 100).round(2)
    payment_df = pd.DataFrame({
        'è®¢å•æ•°': payment_analysis,
        'å æ¯”(%)': payment_pct
    })
    print(payment_df)

    # 5. æ—¶é—´è¶‹åŠ¿åˆ†æ
    print("\n5ï¸âƒ£ æŒ‰æ—¥æœŸåˆ†æé”€å”®è¶‹åŠ¿:")
    daily_sales = ecommerce_data.groupby(ecommerce_data['æ—¥æœŸ'].dt.date).agg({
        'è®¢å•ID': 'count',
        'æ€»é‡‘é¢': 'sum'
    }).round(2)
    daily_sales.columns = ['è®¢å•æ•°', 'æ€»é”€å”®é¢']
    print(daily_sales.head(10))

    # 6. æŒ‰å°æ—¶åˆ†æ
    print("\n6ï¸âƒ£ æŒ‰å°æ—¶åˆ†æè®¢å•åˆ†å¸ƒ:")
    hourly_orders = ecommerce_data.groupby('å°æ—¶')['è®¢å•ID'].count()
    print(hourly_orders)

    # 7. å‘¨æœ«vså·¥ä½œæ—¥
    print("\n7ï¸âƒ£ å‘¨æœ«vså·¥ä½œæ—¥å¯¹æ¯”:")
    weekend_analysis = ecommerce_data.groupby('æ˜¯å¦å‘¨æœ«').agg({
        'è®¢å•ID': 'count',
        'æ€»é‡‘é¢': ['sum', 'mean']
    }).round(2)
    weekend_analysis.columns = ['è®¢å•æ•°', 'æ€»é”€å”®é¢', 'å¹³å‡è®¢å•é‡‘é¢']
    weekend_analysis.index = ['å·¥ä½œæ—¥', 'å‘¨æœ«']
    print(weekend_analysis)

    return category_analysis, daily_sales, hourly_orders, payment_analysis, payment_df, payment_pct, region_analysis, weekend_analysis


@app.cell
def _(ecommerce_data):
    print("=" * 60)
    print("ğŸ¯ æ­¥éª¤5ï¼šé«˜çº§åˆ†æ")
    print("=" * 60)

    # 1. ç”¨æˆ·è¡Œä¸ºåˆ†æ
    print("\n1ï¸âƒ£ ç”¨æˆ·è´­ä¹°è¡Œä¸ºåˆ†æ:")
    user_behavior = ecommerce_data.groupby('ç”¨æˆ·ID').agg({
        'è®¢å•ID': 'count',
        'æ€»é‡‘é¢': 'sum',
        'äº§å“ç±»åˆ«': lambda x: x.nunique()
    }).round(2)
    user_behavior.columns = ['è´­ä¹°æ¬¡æ•°', 'æ€»æ¶ˆè´¹é‡‘é¢', 'è´­ä¹°ç±»åˆ«æ•°']
    user_behavior['å¹³å‡è®¢å•é‡‘é¢'] = (user_behavior['æ€»æ¶ˆè´¹é‡‘é¢'] / user_behavior['è´­ä¹°æ¬¡æ•°']).round(2)

    print("ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡:")
    print(user_behavior.describe())

    print("\næ¶ˆè´¹é‡‘é¢TOP10ç”¨æˆ·:")
    print(user_behavior.nlargest(10, 'æ€»æ¶ˆè´¹é‡‘é¢'))

    # 2. RFMåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
    print("\n2ï¸âƒ£ RFMåˆ†æ:")
    latest_date = ecommerce_data['æ—¥æœŸ'].max()

    rfm = ecommerce_data.groupby('ç”¨æˆ·ID').agg({
        'æ—¥æœŸ': lambda x: (latest_date - x.max()).days,  # Recency
        'è®¢å•ID': 'count',  # Frequency
        'æ€»é‡‘é¢': 'sum'  # Monetary
    }).round(2)
    rfm.columns = ['æœ€è¿‘è´­ä¹°å¤©æ•°', 'è´­ä¹°é¢‘æ¬¡', 'æ€»æ¶ˆè´¹é‡‘é¢']

    # RFMè¯„åˆ†ï¼ˆç®€åŒ–ï¼‰- ä½¿ç”¨cutä»£æ›¿qcutä»¥é¿å…åˆ†ç®±é”™è¯¯
    try:
        rfm['Rè¯„åˆ†'] = pd.qcut(rfm['æœ€è¿‘è´­ä¹°å¤©æ•°'], 5, labels=[5,4,3,2,1], duplicates='drop')
    except ValueError:
        rfm['Rè¯„åˆ†'] = pd.cut(rfm['æœ€è¿‘è´­ä¹°å¤©æ•°'], 5, labels=[5,4,3,2,1])

    try:
        rfm['Fè¯„åˆ†'] = pd.qcut(rfm['è´­ä¹°é¢‘æ¬¡'], 5, labels=[1,2,3,4,5], duplicates='drop')
    except ValueError:
        rfm['Fè¯„åˆ†'] = pd.cut(rfm['è´­ä¹°é¢‘æ¬¡'], 5, labels=[1,2,3,4,5])

    try:
        rfm['Mè¯„åˆ†'] = pd.qcut(rfm['æ€»æ¶ˆè´¹é‡‘é¢'], 5, labels=[1,2,3,4,5], duplicates='drop')
    except ValueError:
        rfm['Mè¯„åˆ†'] = pd.cut(rfm['æ€»æ¶ˆè´¹é‡‘é¢'], 5, labels=[1,2,3,4,5])

    print(rfm.head(10))

    # 3. äº§å“å…³è”åˆ†æ
    print("\n3ï¸âƒ£ äº§å“ç±»åˆ«ç»„åˆåˆ†æ:")
    user_categories = ecommerce_data.groupby('ç”¨æˆ·ID')['äº§å“ç±»åˆ«'].apply(list)

    # ç»Ÿè®¡ç±»åˆ«ç»„åˆ
    from itertools import combinations
    category_pairs = []
    for categories in user_categories:
        if len(categories) >= 2:
            for pair in combinations(set(categories), 2):
                category_pairs.append(tuple(sorted(pair)))

    if category_pairs:
        pair_counts = pd.Series(category_pairs).value_counts().head(10)
        print("å¸¸è§äº§å“ç±»åˆ«ç»„åˆ:")
        print(pair_counts)

    # 4. å®¢å•ä»·åˆ†æ
    print("\n4ï¸âƒ£ å®¢å•ä»·åˆ†å¸ƒåˆ†æ:")
    avg_order_value = ecommerce_data.groupby('è®¢å•ID')['æ€»é‡‘é¢'].sum()

    print(f"å¹³å‡å®¢å•ä»·: {avg_order_value.mean():.2f}")
    print(f"ä¸­ä½æ•°å®¢å•ä»·: {avg_order_value.median():.2f}")
    print(f"æœ€é«˜å®¢å•ä»·: {avg_order_value.max():.2f}")
    print(f"æœ€ä½å®¢å•ä»·: {avg_order_value.min():.2f}")

    return avg_order_value, category_pairs, combinations, latest_date, pair_counts, rfm, user_behavior, user_categories


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“š 13. Pandasæœ€ä½³å®è·µæ€»ç»“

    ### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

    1. **ä½¿ç”¨å‘é‡åŒ–æ“ä½œ**ï¼šé¿å…ä½¿ç”¨å¾ªç¯ï¼Œä½¿ç”¨Pandaså†…ç½®çš„å‘é‡åŒ–æ–¹æ³•
    2. **é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹**ï¼šä½¿ç”¨`category`ç±»å‹å­˜å‚¨é‡å¤å­—ç¬¦ä¸²ï¼Œä½¿ç”¨`int8`ç­‰èŠ‚çœå†…å­˜
    3. **ä½¿ç”¨`query()`å’Œ`eval()`**ï¼šå¯¹äºå¤§æ•°æ®é›†ï¼Œè¿™äº›æ–¹æ³•æ›´é«˜æ•ˆ
    4. **åˆ†å—è¯»å–å¤§æ–‡ä»¶**ï¼šä½¿ç”¨`chunksize`å‚æ•°åˆ†å—è¯»å–
    5. **ä½¿ç”¨`inplace=True`è°¨æ…**ï¼šè™½ç„¶èŠ‚çœå†…å­˜ï¼Œä½†å¯èƒ½å¯¼è‡´æ„å¤–ç»“æœ

    ### ä»£ç å¯è¯»æ€§

    1. **ä½¿ç”¨é“¾å¼æ“ä½œ**ï¼šè®©ä»£ç æ›´ç®€æ´æ˜“è¯»
    2. **åˆç†å‘½åå˜é‡**ï¼šä½¿ç”¨æè¿°æ€§çš„å˜é‡å
    3. **æ·»åŠ æ³¨é‡Š**ï¼šè§£é‡Šå¤æ‚çš„æ“ä½œé€»è¾‘
    4. **æ‹†åˆ†å¤æ‚æ“ä½œ**ï¼šå°†å¤æ‚çš„æ•°æ®å¤„ç†æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤

    ### æ•°æ®è´¨é‡

    1. **å§‹ç»ˆæ£€æŸ¥æ•°æ®**ï¼šä½¿ç”¨`info()`, `describe()`, `head()`ç­‰æ–¹æ³•
    2. **å¤„ç†ç¼ºå¤±å€¼**ï¼šæ˜ç¡®ç¼ºå¤±å€¼çš„å¤„ç†ç­–ç•¥
    3. **éªŒè¯æ•°æ®ç±»å‹**ï¼šç¡®ä¿æ¯åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®
    4. **æ£€æŸ¥é‡å¤å€¼**ï¼šåŠæ—¶å‘ç°å’Œå¤„ç†é‡å¤æ•°æ®
    5. **æ•°æ®éªŒè¯**ï¼šä½¿ç”¨æ–­è¨€éªŒè¯æ•°æ®çš„åˆç†æ€§

    ### å¸¸ç”¨æ–¹æ³•é€ŸæŸ¥è¡¨

    #### ğŸ“Š æ•°æ®åˆ›å»º

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `pd.DataFrame()` | åˆ›å»ºDataFrame | `pd.DataFrame({'A': [1, 2], 'B': [3, 4]})` |
    | `pd.Series()` | åˆ›å»ºSeries | `pd.Series([1, 2, 3, 4])` |
    | `pd.read_csv()` | è¯»å–CSVæ–‡ä»¶ | `pd.read_csv('data.csv')` |
    | `pd.read_excel()` | è¯»å–Excelæ–‡ä»¶ | `pd.read_excel('data.xlsx')` |
    | `pd.read_json()` | è¯»å–JSONæ–‡ä»¶ | `pd.read_json('data.json')` |
    | `pd.read_sql()` | ä»SQLæ•°æ®åº“è¯»å– | `pd.read_sql(query, connection)` |

    #### ğŸ‘€ æ•°æ®æŸ¥çœ‹

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `df.head(n)` | æŸ¥çœ‹å‰nè¡Œï¼ˆé»˜è®¤5è¡Œï¼‰ | `df.head(10)` |
    | `df.tail(n)` | æŸ¥çœ‹ånè¡Œï¼ˆé»˜è®¤5è¡Œï¼‰ | `df.tail(10)` |
    | `df.sample(n)` | éšæœºæŠ½æ ·nè¡Œ | `df.sample(5)` |
    | `df.info()` | æŸ¥çœ‹æ•°æ®ä¿¡æ¯å’Œç±»å‹ | `df.info()` |
    | `df.describe()` | æè¿°æ€§ç»Ÿè®¡ | `df.describe()` |
    | `df.shape` | æ•°æ®å½¢çŠ¶ï¼ˆè¡Œæ•°ï¼Œåˆ—æ•°ï¼‰ | `df.shape` |
    | `df.columns` | åˆ—ååˆ—è¡¨ | `df.columns` |
    | `df.dtypes` | å„åˆ—æ•°æ®ç±»å‹ | `df.dtypes` |
    | `df.index` | ç´¢å¼•ä¿¡æ¯ | `df.index` |

    #### ğŸ¯ æ•°æ®é€‰æ‹©

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `df['col']` | é€‰æ‹©å•åˆ— | `df['å§“å']` |
    | `df[['col1', 'col2']]` | é€‰æ‹©å¤šåˆ— | `df[['å§“å', 'å¹´é¾„']]` |
    | `df.loc[]` | æŒ‰æ ‡ç­¾é€‰æ‹©è¡Œåˆ— | `df.loc[0:5, ['å§“å', 'å¹´é¾„']]` |
    | `df.iloc[]` | æŒ‰ä½ç½®é€‰æ‹©è¡Œåˆ— | `df.iloc[0:5, 0:2]` |
    | `df[condition]` | æ¡ä»¶è¿‡æ»¤ | `df[df['å¹´é¾„'] > 25]` |
    | `df.query()` | æŸ¥è¯¢è¡¨è¾¾å¼è¿‡æ»¤ | `df.query('å¹´é¾„ > 25 and åŸå¸‚ == "åŒ—äº¬"')` |
    | `df.isin()` | æˆå‘˜æ£€æŸ¥ | `df[df['åŸå¸‚'].isin(['åŒ—äº¬', 'ä¸Šæµ·'])]` |

    #### ğŸ§¹ æ•°æ®æ¸…æ´—

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `df.isnull()` | æ£€æŸ¥ç¼ºå¤±å€¼ | `df.isnull().sum()` |
    | `df.notnull()` | æ£€æŸ¥éç¼ºå¤±å€¼ | `df.notnull()` |
    | `df.dropna()` | åˆ é™¤ç¼ºå¤±å€¼ | `df.dropna(axis=0)` |
    | `df.fillna()` | å¡«å……ç¼ºå¤±å€¼ | `df.fillna(0)` |
    | `df.duplicated()` | æ£€æŸ¥é‡å¤å€¼ | `df.duplicated()` |
    | `df.drop_duplicates()` | åˆ é™¤é‡å¤å€¼ | `df.drop_duplicates()` |
    | `df.replace()` | æ›¿æ¢å€¼ | `df.replace({'A': 'B'})` |
    | `df.astype()` | è½¬æ¢æ•°æ®ç±»å‹ | `df['å¹´é¾„'].astype(int)` |

    #### ğŸ”„ æ•°æ®è½¬æ¢

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `df.sort_values()` | æŒ‰å€¼æ’åº | `df.sort_values('å¹´é¾„', ascending=False)` |
    | `df.sort_index()` | æŒ‰ç´¢å¼•æ’åº | `df.sort_index()` |
    | `df.apply()` | åº”ç”¨å‡½æ•° | `df['å¹´é¾„'].apply(lambda x: x + 1)` |
    | `df.map()` | æ˜ å°„å€¼ | `df['æ€§åˆ«'].map({'M': 'ç”·', 'F': 'å¥³'})` |
    | `df.assign()` | æ·»åŠ æ–°åˆ— | `df.assign(æ–°åˆ—=df['A'] + df['B'])` |
    | `df.rename()` | é‡å‘½ååˆ— | `df.rename(columns={'old': 'new'})` |
    | `df.drop()` | åˆ é™¤è¡Œæˆ–åˆ— | `df.drop(['åˆ—å'], axis=1)` |
    | `df.reset_index()` | é‡ç½®ç´¢å¼• | `df.reset_index(drop=True)` |

    #### ğŸ“Š æ•°æ®èšåˆ

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `df.groupby()` | åˆ†ç»„ | `df.groupby('ç±»åˆ«')['é”€å”®é¢'].sum()` |
    | `df.agg()` | èšåˆå‡½æ•° | `df.agg({'A': 'sum', 'B': 'mean'})` |
    | `df.pivot_table()` | é€è§†è¡¨ | `df.pivot_table(values='é”€å”®é¢', index='åœ°åŒº', columns='äº§å“')` |
    | `df.value_counts()` | å€¼è®¡æ•° | `df['ç±»åˆ«'].value_counts()` |
    | `df.sum()` | æ±‚å’Œ | `df.sum()` |
    | `df.mean()` | å¹³å‡å€¼ | `df.mean()` |
    | `df.median()` | ä¸­ä½æ•° | `df.median()` |
    | `df.std()` | æ ‡å‡†å·® | `df.std()` |
    | `df.min()` / `df.max()` | æœ€å°å€¼/æœ€å¤§å€¼ | `df.min()` |
    | `df.count()` | è®¡æ•° | `df.count()` |

    #### ğŸ”— æ•°æ®åˆå¹¶

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `pd.merge()` | åˆå¹¶ï¼ˆç±»ä¼¼SQL JOINï¼‰ | `pd.merge(df1, df2, on='key', how='inner')` |
    | `pd.concat()` | æ‹¼æ¥ | `pd.concat([df1, df2], axis=0)` |
    | `df.join()` | è¿æ¥ | `df1.join(df2, on='key')` |
    | `df.append()` | è¿½åŠ è¡Œ | `df.append(new_row, ignore_index=True)` |

    #### ğŸ”€ æ•°æ®é‡å¡‘

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `df.pivot()` | é€è§†ï¼ˆå®½æ ¼å¼ï¼‰ | `df.pivot(index='æ—¥æœŸ', columns='äº§å“', values='é”€å”®é¢')` |
    | `df.melt()` | èåˆï¼ˆé•¿æ ¼å¼ï¼‰ | `df.melt(id_vars='æ—¥æœŸ', value_vars=['A', 'B'])` |
    | `df.stack()` | å †å ï¼ˆåˆ—è½¬è¡Œï¼‰ | `df.stack()` |
    | `df.unstack()` | åå †å ï¼ˆè¡Œè½¬åˆ—ï¼‰ | `df.unstack()` |
    | `df.transpose()` / `df.T` | è½¬ç½® | `df.T` |

    #### â° æ—¶é—´åºåˆ—

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `pd.date_range()` | åˆ›å»ºæ—¥æœŸèŒƒå›´ | `pd.date_range('2024-01-01', periods=10, freq='D')` |
    | `pd.to_datetime()` | è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ | `pd.to_datetime(df['æ—¥æœŸ'])` |
    | `df.resample()` | é‡é‡‡æ · | `df.resample('W').sum()` |
    | `df.rolling()` | æ»šåŠ¨çª—å£ | `df.rolling(window=7).mean()` |
    | `df.shift()` | æ—¶é—´åç§» | `df.shift(1)` |
    | `df.diff()` | å·®åˆ† | `df.diff()` |

    #### ğŸ’¾ æ•°æ®IO

    | æ–¹æ³• | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `df.to_csv()` | ä¿å­˜ä¸ºCSV | `df.to_csv('data.csv', index=False)` |
    | `df.to_excel()` | ä¿å­˜ä¸ºExcel | `df.to_excel('data.xlsx', index=False)` |
    | `df.to_json()` | ä¿å­˜ä¸ºJSON | `df.to_json('data.json')` |
    | `df.to_sql()` | ä¿å­˜åˆ°SQLæ•°æ®åº“ | `df.to_sql('table_name', connection)` |
    | `df.to_dict()` | è½¬æ¢ä¸ºå­—å…¸ | `df.to_dict('records')` |

    ### å­¦ä¹ èµ„æº

    - ğŸ“– [Pandaså®˜æ–¹æ–‡æ¡£](https://pandas.pydata.org/docs/)
    - ğŸ“š [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)
    - ğŸ’¡ [Pandas Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html)
    - ğŸ“ [Real Python Pandas Tutorials](https://realpython.com/learning-paths/pandas-data-science/)

    ### æ€»ç»“

    Pandasæ˜¯Pythonæ•°æ®åˆ†æçš„æ ¸å¿ƒå·¥å…·ï¼ŒæŒæ¡å®ƒå¯¹äºæ•°æ®ç§‘å­¦å®¶å’Œåˆ†æå¸ˆè‡³å…³é‡è¦ã€‚

    **å…³é”®è¦ç‚¹**ï¼š
    - ğŸ¯ ç†è§£Serieså’ŒDataFrameçš„æ ¸å¿ƒæ¦‚å¿µ
    - ğŸ”§ ç†Ÿç»ƒä½¿ç”¨æ•°æ®é€‰æ‹©ã€è¿‡æ»¤ã€è½¬æ¢æ–¹æ³•
    - ğŸ§¹ é‡è§†æ•°æ®æ¸…æ´—å’Œè´¨é‡æ£€æŸ¥
    - ğŸ“Š æŒæ¡åˆ†ç»„èšåˆå’Œæ•°æ®é‡å¡‘
    - â° äº†è§£æ—¶é—´åºåˆ—å¤„ç†
    - ğŸ’¾ ç†Ÿæ‚‰å„ç§æ•°æ®IOæ“ä½œ
    - ğŸš€ æ³¨é‡æ€§èƒ½ä¼˜åŒ–å’Œä»£ç å¯è¯»æ€§

    é€šè¿‡æœ¬æŒ‡å—çš„å­¦ä¹ å’Œå®è·µï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
    - âœ… ä½¿ç”¨Pandasè¿›è¡Œæ•°æ®åŠ è½½å’Œä¿å­˜
    - âœ… è¿›è¡Œæ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
    - âœ… æ‰§è¡Œå¤æ‚çš„æ•°æ®è½¬æ¢å’Œèšåˆ
    - âœ… è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ
    - âœ… ä¸ºæœºå™¨å­¦ä¹ å‡†å¤‡æ•°æ®

    ç»§ç»­å®è·µï¼Œä¸æ–­æå‡ä½ çš„PandasæŠ€èƒ½ï¼ğŸ¼
    """
    )
    return


if __name__ == "__main__":
    app.run()

