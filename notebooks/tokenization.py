import marimo

__generated_with = "0.15.2"
app = marimo.App(
    width="medium",
    app_title="Tokenization Converting Text to Numbers for Neural Networks",
)


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.8vn9rjdb7m.webp)""")
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    # å¼•è¨€ï¼šä¸ºä»€ä¹ˆåˆ†è¯å¾ˆé‡è¦

    æƒ³è±¡ä¸€ä¸‹ï¼Œåœ¨æ²¡æœ‰å…ˆæ•™ä¼šè®¡ç®—æœºé˜…è¯»çš„æƒ…å†µä¸‹ï¼Œè¯•å›¾æ•™å®ƒç†è§£èå£«æ¯”äºšçš„ä½œå“ã€‚è¿™å°±æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šè®¡ç®—æœºç†è§£æ•°å­¦ï¼Œè€Œäººç±»ä½¿ç”¨æ–‡å­—ã€‚åˆ†è¯æ˜¯è¿æ¥è¿™ä¸¤ä¸ªä¸–ç•Œçš„é‡è¦æ¡¥æ¢ã€‚

    æ¯å½“æ‚¨å‘ChatGPTæé—®ã€åœ¨çº¿æœç´¢ä¿¡æ¯æˆ–åœ¨ç”µå­é‚®ä»¶ä¸­è·å¾—è‡ªåŠ¨å®Œæˆå»ºè®®æ—¶ï¼Œåˆ†è¯éƒ½åœ¨å¹•åé»˜é»˜å·¥ä½œï¼Œå°†æ‚¨çš„æ–‡æœ¬è½¬æ¢ä¸ºé©±åŠ¨è¿™äº›æ™ºèƒ½ç³»ç»Ÿçš„æ•°å­—åºåˆ—ã€‚

    æœ¬æ–‡æ¢è®¨äº†åˆ†è¯å¦‚ä½•å°†äººç±»è¯­è¨€è½¬æ¢ä¸ºæœºå™¨å¯è¯»çš„æ•°å­—ï¼Œä¸ºä»€ä¹ˆä¸åŒçš„åˆ†è¯æ–¹æ³•ä¼šæå¤§åœ°å½±å“æ¨¡å‹æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•ä¸ºæ‚¨çš„é¡¹ç›®å®ç°ç”Ÿäº§å°±ç»ªçš„åˆ†è¯ã€‚æ— è®ºæ‚¨æ˜¯åœ¨æ„å»ºèŠå¤©æœºå™¨äººã€åˆ†æå®¢æˆ·åé¦ˆï¼Œè¿˜æ˜¯è®­ç»ƒä¸‹ä¸€ä»£è¯­è¨€æ¨¡å‹ï¼ŒæŒæ¡åˆ†è¯å¯¹æ‚¨çš„æˆåŠŸéƒ½è‡³å…³é‡è¦ã€‚

    å¦‚æœæ‚¨å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·åœ¨LinkedInæˆ–Mediumä¸Šå…³æ³¨Rickï¼Œè·å–æ›´å¤šä¼ä¸šAIå’ŒAIæ´å¯Ÿã€‚

    è®©æˆ‘ä»¬è§£ç å…è®¸æœºå™¨ç†è§£æˆ‘ä»¬çš„ç§˜å¯†è¯­è¨€ã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    # ç¬¬5ç¯‡ï¼šåˆ†è¯ â€” å°†æ–‡æœ¬è½¬æ¢ä¸ºç¥ç»ç½‘ç»œçš„æ•°å­—

    ## å­¦ä¹ ç›®æ ‡

    åœ¨æœ¬æ•™ç¨‹ç»“æŸæ—¶ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

    - ç†è§£åˆ†è¯å¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—è¡¨ç¤º
    - æ¯”è¾ƒä¸‰ç§ä¸»è¦çš„åˆ†è¯ç®—æ³•ï¼šBPEã€WordPieceå’ŒUnigram
    - ä½¿ç”¨Hugging Faceçš„transformersåº“å®ç°åˆ†è¯
    - å¤„ç†ç”Ÿäº§ç³»ç»Ÿä¸­çš„å¸¸è§è¾¹ç¼˜æƒ…å†µ
    - æœ‰æ•ˆè°ƒè¯•åˆ†è¯é—®é¢˜
    - ä¸ºä¸“ä¸šé¢†åŸŸæ„å»ºè‡ªå®šä¹‰åˆ†è¯å™¨

    è¿™æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„æ–‡ç« ç³»åˆ—ï¼Œæ‰€ä»¥è¯·åŠ¡å¿…å…‹éš†githubä»“åº“ï¼Œè¿è¡Œç¤ºä¾‹ï¼Œå¹¶åŠ è½½ç¬”è®°æœ¬ã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## å¼•è¨€ï¼šä¸ºä»€ä¹ˆåˆ†è¯å¾ˆé‡è¦

    ç¥ç»ç½‘ç»œå¤„ç†æ•°å­—ï¼Œè€Œä¸æ˜¯æ–‡æœ¬ã€‚åˆ†è¯å°†äººç±»è¯­è¨€è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—åºåˆ—ã€‚è¿™ç§è½¬æ¢å†³å®šäº†æ‚¨çš„æ¨¡å‹æ€§èƒ½å¦‚ä½•ã€‚

    ### ç°å®ä¸–ç•Œçš„å½±å“

    è€ƒè™‘è¿™äº›ä¸šåŠ¡åœºæ™¯ï¼š

    - **å®¢æˆ·æ”¯æŒ**ï¼šèŠå¤©æœºå™¨äººéœ€è¦åŒºåˆ†"can't login"å’Œ"cannot log in"
    - **é‡‘èåˆ†æ**ï¼šç³»ç»Ÿå¿…é¡»å°†"Q4 2023"è¯†åˆ«ä¸ºä¸€ä¸ªå•å…ƒï¼Œè€Œä¸æ˜¯ä¸‰ä¸ª
    - **åŒ»ç–—è®°å½•**ï¼š"å¿ƒè‚Œæ¢—æ­»"å¿…é¡»ä¿æŒåœ¨ä¸€èµ·ä»¥ä¿æŒæ„ä¹‰

    ç³Ÿç³•çš„åˆ†è¯ä¼šå¯¼è‡´ï¼š

    - è¯¯è§£ç”¨æˆ·æ„å›¾
    - é”™è¯¯çš„æ•°æ®æå–
    - æ›´é«˜çš„è®¡ç®—æˆæœ¬
    - é™ä½æ¨¡å‹å‡†ç¡®æ€§
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

    ![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.9rjr701gol.webp)

    æ¶æ„è§£é‡Šï¼šæ–‡æœ¬é€šè¿‡åˆ†è¯å™¨ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„è¯æ±‡è¡¨å°†å…¶è½¬æ¢ä¸ºæ•°å­—IDã€‚è¿™äº›IDè¢«è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼Œç„¶åè¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚è¯æ±‡è¡¨åœ¨æ–‡æœ¬ç‰‡æ®µå’Œæ•°å­—ä¹‹é—´å»ºç«‹æ˜ å°„å…³ç³»ã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## æ ¸å¿ƒæ¦‚å¿µï¼šæ–‡æœ¬åˆ°Token

    ### ä»€ä¹ˆæ˜¯Tokenï¼Ÿ

    Tokenæ˜¯æ¨¡å‹å¤„ç†çš„æ–‡æœ¬åŸºæœ¬å•å…ƒã€‚å®ƒä»¬å¯ä»¥æ˜¯ï¼š

    - **å®Œæ•´å•è¯**ï¼š`"cat"` â†’ `["cat"]`
    - **å­è¯**ï¼š`"unhappy"` â†’ `["un", "happy"]`
    - **å­—ç¬¦**ï¼š`"hi"` â†’ `["h", "i"]`

    ### åˆ†è¯è¿‡ç¨‹

    ![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.102iu7634v.webp)

    è¿‡ç¨‹è§£é‡Šï¼šç”¨æˆ·æä¾›æ–‡æœ¬ã€‚åˆ†è¯å™¨åœ¨å…¶è¯æ±‡è¡¨ä¸­æŸ¥æ‰¾æ¯ä¸ªç‰‡æ®µä»¥æ‰¾åˆ°æ•°å­—IDã€‚ç‰¹æ®Štokenå¦‚`[CLS]`å’Œ`[SEP]`æ ‡è®°å¼€å§‹å’Œç»“æŸã€‚æ¨¡å‹æ¥æ”¶è¿™äº›æ•°å­—è¿›è¡Œå¤„ç†ã€‚
    """
    )
    return


@app.cell
def _():
    #è¿™æ®µä»£ç æ¼”ç¤ºäº†ä½¿ç”¨BERTè¿›è¡ŒåŸºç¡€åˆ†è¯ï¼š
    import logging

    from transformers import AutoTokenizer

    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    def demonstrate_basic_tokenization():
        # å±•ç¤ºåˆ†è¯å¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—
        # è¿™ä¸ªä¾‹å­ä½¿ç”¨BERTçš„åˆ†è¯å™¨å¤„ç†ä¸€ä¸ªç®€å•çš„å¥å­

        # åŠ è½½BERTåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

        # ç¤ºä¾‹æ–‡æœ¬
        text = "Tokenization converts text to numbers."

        # åˆ†è¯
        tokens = tokenizer.tokenize(text)
        token_ids = tokenizer.encode(text)

        # æ˜¾ç¤ºç»“æœ
        logger.info(f"åŸå§‹æ–‡æœ¬: {text}")
        logger.info(f"Token: {tokens}")
        logger.info(f"Token ID: {token_ids}")

        # æ˜¾ç¤ºtokenåˆ°IDçš„æ˜ å°„
        for token, token_id in zip(tokens, token_ids[1:-1]):  # è·³è¿‡ç‰¹æ®Štoken
            logger.info(f"  '{token}' â†’ {token_id}")

        return tokens, token_ids

    # è¿è¡Œæ¼”ç¤º
    tokens, ids = demonstrate_basic_tokenization()
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **ä»£ç è§£é‡Š**ï¼šè¿™ä¸ªå‡½æ•°åŠ è½½BERTçš„åˆ†è¯å™¨å¹¶å¤„ç†ä¸€ä¸ªå¥å­ã€‚å®ƒæ˜¾ç¤ºäº†æ–‡æœ¬tokenå’Œå®ƒä»¬çš„æ•°å­—IDã€‚æ˜ å°„æ­ç¤ºäº†æ¯ä¸ªå•è¯å¦‚ä½•è¢«åˆ†é…ç›¸åº”çš„æ•°å­—ã€‚ç‰¹æ®Štoken `[CLS]`å’Œ`[SEP]`æ¡†å®šåºåˆ—ã€‚

    **å‡½æ•°åˆ†æ**ï¼š`demonstrate_basic_tokenization`

    - **ç›®çš„**ï¼šæ¼”ç¤ºåŸºç¡€çš„æ–‡æœ¬åˆ°æ•°å­—è½¬æ¢è¿‡ç¨‹
    - **å‚æ•°**ï¼šæ— å‚æ•°
    - **è¿”å›å€¼**ï¼šå…ƒç»„(tokens: å­—ç¬¦ä¸²åˆ—è¡¨, token_ids: æ•´æ•°åˆ—è¡¨)
    - **åŠŸèƒ½**ï¼šå°†åˆ†è¯ç»“æœè®°å½•åˆ°æ§åˆ¶å°ï¼Œé¦–æ¬¡è¿è¡Œæ—¶ä¸‹è½½BERTè¯æ±‡è¡¨
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## åˆ†è¯ç®—æ³•

    ä¸‰ç§ä¸»è¦ç®—æ³•é©±åŠ¨ç°ä»£åˆ†è¯ã€‚æ¯ç§ç®—æ³•éƒ½åœ¨è¯æ±‡è¡¨å¤§å°å’Œåºåˆ—é•¿åº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

    ### ç®—æ³•æ¯”è¾ƒ

    | ç®—æ³• | ä½¿ç”¨æ¨¡å‹ | æ–¹æ³• | è¯æ±‡è¡¨å¤§å° | æœ€é€‚ç”¨äº |
    |------|----------|------|------------|----------|
    | **BPE (å­—èŠ‚å¯¹ç¼–ç )** | GPT, RoBERTa | åŸºäºé¢‘ç‡çš„åˆå¹¶ | 30k-50k | é€šç”¨æ–‡æœ¬å¤„ç† |
    | **WordPiece** | BERT | ä¼¼ç„¶æœ€å¤§åŒ– | 30k | å¤šè¯­è¨€åº”ç”¨ |
    | **Unigram** | T5, mBART | æ¦‚ç‡æ¨¡å‹ | 32k-250k | çµæ´»çš„tokené€‰æ‹© |
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## å­—èŠ‚å¯¹ç¼–ç  (BPE)

    BPEé€šè¿‡åˆå¹¶é¢‘ç¹çš„å­—ç¬¦å¯¹æ¥æ„å»ºè¯æ±‡è¡¨ï¼š
    """
    )
    return


@app.cell
def _():
    def demonstrate_bpe_tokenization():
        import logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        from transformers import AutoTokenizer
        # æ¼”ç¤ºä½¿ç”¨RoBERTaçš„BPEåˆ†è¯
        # BPEé€šè¿‡å°†æœªçŸ¥å•è¯åˆ†è§£ä¸ºå·²çŸ¥çš„å­è¯æ¥å¤„ç†å®ƒä»¬

        tokenizer = AutoTokenizer.from_pretrained('roberta-base')

        # æµ‹è¯•æ˜¾ç¤ºBPEè¡Œä¸ºçš„å•è¯
        test_words = [
            "tokenization",      # å¸¸è§å•è¯
            "pretokenization",   # å¤åˆå•è¯
            "cryptocurrency",    # æŠ€æœ¯æœ¯è¯­
            "antidisestablish"   # ç½•è§å•è¯
        ]

        logger.info("=== BPEåˆ†è¯ (RoBERTa) ===")

        for word in test_words:
            tokens = tokenizer.tokenize(word)
            ids = tokenizer.encode(word, add_special_tokens=False)

            logger.info(f"\\n'{word}':")
            logger.info(f"  Token: {tokens}")
            logger.info(f"  æ•°é‡: {len(tokens)}")

            # æ˜¾ç¤ºBPEå¦‚ä½•åˆ†å‰²å•è¯
            if len(tokens) > 1:
                logger.info(f"  åˆ†å‰²æ¨¡å¼: {' + '.join(tokens)}")

        return tokenizer

    # è¿è¡ŒBPEæ¼”ç¤º
    bpe_tokenizer = demonstrate_bpe_tokenization()
    return


@app.cell
def _(mo):
    mo.md(r"""**ä»£ç è§£é‡Š**ï¼šBPEåˆ†è¯æ ¹æ®é¢‘ç‡å°†å•è¯åˆ†å‰²æˆæ›´å°çš„å•å…ƒã€‚å¸¸è§å•è¯ä¿æŒå®Œæ•´ï¼Œè€Œä¸å¸¸è§çš„å•è¯åˆ†è§£ä¸ºç†Ÿæ‚‰çš„éƒ¨åˆ†ã€‚è¿™å…è®¸å¤„ç†ä»»ä½•å•è¯ï¼Œå³ä½¿æ˜¯è®­ç»ƒæ•°æ®ä¸­æœªè§è¿‡çš„å•è¯ã€‚""")
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## WordPieceåˆ†è¯

    WordPieceä½¿ç”¨ç»Ÿè®¡ä¼¼ç„¶æ¥åˆ›å»ºå­è¯ï¼š
    """
    )
    return


@app.cell
def _():
    def demonstrate_wordpiece_tokenization():
        import logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        from transformers import AutoTokenizer
        # å±•ç¤ºBERTä½¿ç”¨çš„WordPieceåˆ†è¯
        # æ³¨æ„æ ‡è®°å•è¯å»¶ç»­çš„##å‰ç¼€

        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

        # ç”¨äºæ¯”è¾ƒçš„ç›¸åŒæµ‹è¯•å•è¯
        test_words = [
            "tokenization",
            "pretokenization",
            "cryptocurrency",
            "antidisestablish"
        ]

        logger.info("\n=== WordPieceåˆ†è¯ (BERT) ===")

        for word in test_words:
            tokens = tokenizer.tokenize(word)

            logger.info(f"\n'{word}':")
            logger.info(f"  Token: {tokens}")

            # è§£é‡Š##ç¬¦å·
            if any(t.startswith('##') for t in tokens):
                logger.info("  æ³¨æ„: ##è¡¨ç¤ºå‰ä¸€ä¸ªtokençš„å»¶ç»­")

                # ä»ç‰‡æ®µé‡æ„å•è¯
                reconstructed = tokens[0]
                for token in tokens[1:]:
                    reconstructed += token.replace('##', '')
                logger.info(f"  é‡æ„: {reconstructed}")

        return tokenizer

    # è¿è¡ŒWordPieceæ¼”ç¤º
    wordpiece_tokenizer = demonstrate_wordpiece_tokenization()
    return


@app.cell
def _(mo):
    mo.md(r"""**ä»£ç è§£é‡Š**ï¼šWordPieceç”¨##æ ‡è®°éåˆå§‹å­è¯ã€‚è¿™ä¿ç•™äº†å•è¯è¾¹ç•Œï¼Œå¸®åŠ©æ¨¡å‹ç†è§£tokenå…³ç³»ã€‚é‡æ„æ˜¾ç¤ºäº†ç‰‡æ®µå¦‚ä½•é‡æ–°ç»„åˆæˆå•è¯ã€‚""")
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## ç®—æ³•é€‰æ‹©æŒ‡å—

    ![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.7lkcl944t3.webp)

    **å†³ç­–æµç¨‹**ï¼šä»æ‚¨çš„åº”ç”¨ç±»å‹å¼€å§‹ã€‚é€šç”¨NLPä»»åŠ¡é€‚åˆä½¿ç”¨BPEã€‚å¤šè¯­è¨€åº”ç”¨éœ€è¦åœ¨å¤šæ ·åŒ–è¯­è¨€ä¸Šè®­ç»ƒçš„åˆ†è¯å™¨ã€‚æŠ€æœ¯é¢†åŸŸå—ç›Šäºä¸“é—¨çš„è¯æ±‡è¡¨ã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## å®ç°æŒ‡å—

    ### è®¾ç½®æ‚¨çš„ç¯å¢ƒ

    é¦–å…ˆï¼Œå®‰è£…æ‰€éœ€çš„ä¾èµ–ï¼š

    ```bash
    # requirements.txt
    transformers==4.36.0
    torch==2.1.0
    tokenizers==0.15.0
    datasets==2.16.0
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### å®Œæ•´çš„åˆ†è¯ç®¡é“

    æœ¬èŠ‚æ¼”ç¤ºäº†ä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„åˆ†è¯ç®¡é“ï¼š
    """
    )
    return


@app.cell
def _():
    def __():
        import logging

        import torch
        from transformers import AutoTokenizer

        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        class TokenizationPipeline:
            """
            ç”Ÿäº§å°±ç»ªçš„åˆ†è¯ç®¡é“ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€‚
            æ”¯æŒæ‰¹å¤„ç†å’Œå¤šç§è¾“å‡ºæ ¼å¼ã€‚
            """

            def __init__(self, model_name='bert-base-uncased', max_length=512):
                """
                ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆå§‹åŒ–åˆ†è¯å™¨ã€‚

                å‚æ•°:
                -----------
                model_name : str
                    Hugging Face æ¨¡å‹æ ‡è¯†ç¬¦
                max_length : int
                    æœ€å¤§åºåˆ—é•¿åº¦
                """
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.max_length = max_length
                logger.info(f"å·²åˆå§‹åŒ–åˆ†è¯å™¨: {model_name}")

            def tokenize_single(self, text, return_offsets=False):
                """
                å¯¹å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²è¿›è¡Œåˆ†è¯ã€‚

                å‚æ•°:
                -----------
                text : str
                    è¦åˆ†è¯çš„è¾“å…¥æ–‡æœ¬
                return_offsets : bool
                    æ˜¯å¦è¿”å›å­—ç¬¦åç§»æ˜ å°„

                è¿”å›:
                --------
                dict : åˆ†è¯ç»“æœï¼ŒåŒ…æ‹¬ input_ids, attention_mask
                """
                if not text:
                    logger.warning("æä¾›äº†ç©ºæ–‡æœ¬")
                    text = ""

                try:
                    encoding = self.tokenizer(
                        text,
                        truncation=True,
                        max_length=self.max_length,
                        padding='max_length',
                        return_offsets_mapping=return_offsets,
                        return_tensors='pt'
                    )

                    logger.info(f"å·²å°† {len(text)} ä¸ªå­—ç¬¦åˆ†è¯ä¸º {encoding['input_ids'].shape[1]} ä¸ªtoken")
                    return encoding

                except Exception as e:
                    logger.error(f"åˆ†è¯å¤±è´¥: {str(e)}")
                    raise

            def tokenize_batch(self, texts, show_progress=True):
                """
                é«˜æ•ˆåœ°å¯¹ä¸€æ‰¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚

                å‚æ•°:
                -----------
                texts : list of str
                    è¦åˆ†è¯çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨
                show_progress : bool
                    æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯

                è¿”å›:
                --------
                dict : æ‰¹å¤„ç†åˆ†è¯ç»“æœ
                """
                if not texts:
                    logger.warning("æä¾›äº†ç©ºæ–‡æœ¬åˆ—è¡¨")
                    return None

                # æ¸…ç†æ–‡æœ¬
                texts = [text if text else "" for text in texts]

                # ä¸ºå†…å­˜æ•ˆç‡åˆ†æ‰¹å¤„ç†
                batch_size = 32
                all_encodings = []

                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i + batch_size]

                    if show_progress:
                        logger.info(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")

                    encoding = self.tokenizer(
                        batch,
                        truncation=True,
                        max_length=self.max_length,
                        padding=True,
                        return_tensors='pt'
                    )
                    all_encodings.append(encoding)

                # åˆå¹¶æ‰¹æ¬¡
                combined = {
                    key: torch.cat([e[key] for e in all_encodings], dim=0)
                    for key in all_encodings[0].keys()
                }

                logger.info(f"å·²åˆ†è¯ {len(texts)} ä¸ªæ–‡æœ¬")
                return combined

        # åˆ›å»ºç®¡é“å®ä¾‹
        pipeline = TokenizationPipeline()

        # æ¼”ç¤ºå•æ–‡æœ¬åˆ†è¯
        logger.info("=== å•æ–‡æœ¬åˆ†è¯æ¼”ç¤º ===")
        sample_text = "This is a sample text for tokenization pipeline demonstration."
        result = pipeline.tokenize_single(sample_text)
        logger.info(f"è¾“å…¥æ–‡æœ¬: {sample_text}")
        logger.info(f"è¾“å‡ºå½¢çŠ¶: {result['input_ids'].shape}")
        logger.info(f"Tokenæ•°é‡: {result['input_ids'].shape[1]}")

        # æ¼”ç¤ºæ‰¹å¤„ç†åˆ†è¯
        logger.info("\n=== æ‰¹å¤„ç†åˆ†è¯æ¼”ç¤º ===")
        batch_texts = [
            "First text for batch processing.",
            "Second text with different length.",
            "Third text that is much longer and contains more words for testing purposes."
        ]
        batch_result = pipeline.tokenize_batch(batch_texts)
        logger.info(f"æ‰¹å¤„ç†è¾“å…¥: {len(batch_texts)} ä¸ªæ–‡æœ¬")
        logger.info(f"æ‰¹å¤„ç†è¾“å‡ºå½¢çŠ¶: {batch_result['input_ids'].shape}")

        # æ¼”ç¤ºåç§»æ˜ å°„
        logger.info("\n=== åç§»æ˜ å°„æ¼”ç¤º ===")
        offset_text = "Apple Inc. was founded in 1976."
        offset_result = pipeline.tokenize_single(offset_text, return_offsets=True)
        tokens = pipeline.tokenizer.convert_ids_to_tokens(offset_result['input_ids'][0])
        offsets = offset_result['offset_mapping'][0]

        logger.info(f"åŸæ–‡: {offset_text}")
        logger.info("Token â†’ åŸæ–‡ä½ç½®:")
        for token, (start, end) in zip(tokens[:10], offsets[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            if start == end:
                logger.info(f"  {token:12} â†’ [SPECIAL]")
            else:
                original = offset_text[start:end]
                logger.info(f"  {token:12} â†’ '{original}' [{start}:{end}]")

    __()
    return


@app.cell
def _(mo):
    mo.md(r"""**å®ç°ç»†èŠ‚**ï¼šè¿™ä¸ªç±»å°è£…äº†åˆ†è¯é€»è¾‘å¹¶æä¾›äº†é€‚å½“çš„é”™è¯¯å¤„ç†ã€‚å®ƒæ”¯æŒå•ä¸ªæ–‡æœ¬å’Œæ‰¹å¤„ç†ã€‚åç§»æ˜ å°„åŠŸèƒ½ä½¿tokenåˆ°å­—ç¬¦çš„å¯¹é½æˆä¸ºå¯èƒ½ï¼Œè¿™å¯¹äºNERç­‰ä»»åŠ¡å¾ˆæœ‰ç”¨ã€‚""")
    return





@app.cell
def _(mo):
    mo.md(
        r"""
    ### å¤„ç†ç‰¹æ®ŠToken

    ç‰¹æ®Štokenä¸ºåºåˆ—æä¾›ç»“æ„ï¼š
    """
    )
    return


@app.cell
def _():
    def __():
        import logging

        from transformers import AutoTokenizer

        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        def demonstrate_special_tokens():
            """
            æ¼”ç¤ºç‰¹æ®Štokenåœ¨ä¸åŒåºåˆ—ç±»å‹ä¸­çš„ä½¿ç”¨ã€‚
            å±•ç¤ºå•åºåˆ—å’Œåºåˆ—å¯¹ä¸­ç‰¹æ®Štokençš„ä½ç½®å’Œä½œç”¨ã€‚
            """
            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

            # å•ä¸ªåºåˆ—
            text1 = "What is tokenization?"
            encoding1 = tokenizer(text1)
            tokens1 = tokenizer.convert_ids_to_tokens(encoding1['input_ids'])

            logger.info("=== å•åºåˆ—ä¸­çš„ç‰¹æ®ŠToken ===")
            logger.info(f"æ–‡æœ¬: {text1}")
            logger.info(f"Token: {tokens1}")
            logger.info(f"[CLS] åœ¨ä½ç½® 0: æ ‡è®°åºåˆ—å¼€å§‹")
            logger.info(f"[SEP] åœ¨ä½ç½® {len(tokens1)-1}: æ ‡è®°åºåˆ—ç»“æŸ")

            # åºåˆ—å¯¹ï¼ˆç”¨äºQAä»»åŠ¡ï¼‰
            question = "What is tokenization?"
            context = "Tokenization converts text into tokens."

            encoding2 = tokenizer(question, context)
            tokens2 = tokenizer.convert_ids_to_tokens(encoding2['input_ids'])

            logger.info("\n=== åºåˆ—å¯¹ä¸­çš„ç‰¹æ®ŠToken ===")
            logger.info(f"é—®é¢˜: {question}")
            logger.info(f"ä¸Šä¸‹æ–‡: {context}")

            # æ‰¾åˆ°åˆ†éš”ç¬¦ä½ç½®
            sep_positions = [i for i, token in enumerate(tokens2) if token == '[SEP]']
            logger.info(f"[SEP] ä½ç½®: {sep_positions}")
            logger.info(f"é—®é¢˜token: ä½ç½® 1 åˆ° {sep_positions[0]-1}")
            logger.info(f"ä¸Šä¸‹æ–‡token: ä½ç½® {sep_positions[0]+1} åˆ° {sep_positions[1]-1}")

            return tokens1, tokens2

        # è¿è¡Œç‰¹æ®Štokenæ¼”ç¤º
        tokens1, tokens2 = demonstrate_special_tokens()

    __()
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **ç‰¹æ®ŠTokenåŠŸèƒ½**ï¼š

    - **[CLS]**: åˆ†ç±»token - èšåˆåºåˆ—å«ä¹‰
    - **[SEP]**: åˆ†éš”ç¬¦token - æ ‡è®°åºåˆ—ä¹‹é—´çš„è¾¹ç•Œ
    - **[PAD]**: å¡«å……token - å¡«å……è¾ƒçŸ­åºåˆ—ä»¥åŒ¹é…æ‰¹æ¬¡é•¿åº¦
    - **[UNK]**: æœªçŸ¥token - æ›¿æ¢è¯æ±‡è¡¨å¤–çš„å•è¯
    - **[MASK]**: æ©ç token - ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## é«˜çº§åŠŸèƒ½

    ### NERçš„åç§»æ˜ å°„

    è·Ÿè¸ªtokenåœ¨åŸå§‹æ–‡æœ¬ä¸­çš„ä½ç½®ï¼š
    """
    )
    return


@app.cell
def _():
    def __():
        import logging

        from transformers import AutoTokenizer

        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        def demonstrate_offset_mapping():
            """
            æ¼”ç¤ºåç§»æ˜ å°„åŠŸèƒ½ï¼Œç”¨äºNERç­‰éœ€è¦ç²¾ç¡®å­—ç¬¦ä½ç½®çš„ä»»åŠ¡ã€‚
            """
            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

            text = "Apple Inc. was founded by Steve Jobs in Cupertino."
            encoding = tokenizer(
                text,
                return_offsets_mapping=True,
                add_special_tokens=True
            )

            tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])
            offsets = encoding['offset_mapping']

            logger.info("=== Tokenåˆ°å­—ç¬¦æ˜ å°„ ===")
            logger.info(f"åŸæ–‡: {text}\n")

            # åˆ›å»ºå¯è§†åŒ–å¯¹é½
            logger.info("Token â†’ åŸæ–‡ [å¼€å§‹:ç»“æŸ]")
            logger.info("-" * 40)

            for token, (start, end) in zip(tokens, offsets):
                if start == end:  # ç‰¹æ®Štoken
                    logger.info(f"{token:12} â†’ [SPECIAL]")
                else:
                    original = text[start:end]
                    logger.info(f"{token:12} â†’ '{original}' [{start}:{end}]")

            # æ¼”ç¤ºå®ä½“æå–
            entity_tokens = [2, 3]  # "apple inc"
            logger.info(f"\nä»token {entity_tokens}æå–å®ä½“:")

            start_char = offsets[entity_tokens[0]][0]
            end_char = offsets[entity_tokens[-1]][1]
            entity = text[start_char:end_char]
            logger.info(f"æå–ç»“æœ: '{entity}'")

            return encoding

        # è¿è¡Œåç§»æ˜ å°„æ¼”ç¤º
        encoding = demonstrate_offset_mapping()

    __()
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **åç§»æ˜ å°„ä¼˜åŠ¿**ï¼š

    - ä¿ç•™ç²¾ç¡®çš„å­—ç¬¦ä½ç½®
    - æ”¯æŒåœ¨æºæ–‡æœ¬ä¸­é«˜äº®æ˜¾ç¤º
    - æ”¯æŒå®ä½“æå–
    - åœ¨åˆ†è¯è¿‡ç¨‹ä¸­ä¿æŒå¯¹é½
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## ç”Ÿäº§è€ƒè™‘

    ### æ€§èƒ½ä¼˜åŒ–

    åˆ†è¯ç»å¸¸æˆä¸ºç“¶é¢ˆã€‚ä»¥ä¸‹æ˜¯ä¼˜åŒ–æ–¹æ³•ï¼š
    """
    )
    return


@app.cell
def _():
    def __():
        import logging
        import time

        from transformers import AutoTokenizer

        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        def benchmark_tokenization_methods():
            """
            å¯¹æ¯”ä¸åŒåˆ†è¯æ–¹æ³•çš„æ€§èƒ½ã€‚
            æµ‹è¯•å•ç‹¬å¤„ç†ã€æ‰¹é‡å¤„ç†å’Œå¿«é€Ÿåˆ†è¯å™¨çš„é€Ÿåº¦å·®å¼‚ã€‚
            """
            # åˆ›å»ºæµ‹è¯•è¯­æ–™åº“
            texts = ["This is a sample sentence for benchmarking."] * 1000

            # æ–¹æ³•1ï¼šå•ç‹¬åˆ†è¯
            tokenizer_slow = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=False)

            start = time.time()
            for text in texts:
                _ = tokenizer_slow(text)
            individual_time = time.time() - start

            # æ–¹æ³•2ï¼šæ‰¹é‡åˆ†è¯
            start = time.time()
            _ = tokenizer_slow(texts, padding=True, truncation=True)
            batch_time = time.time() - start

            # æ–¹æ³•3ï¼šå¿«é€Ÿåˆ†è¯å™¨
            tokenizer_fast = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)

            start = time.time()
            _ = tokenizer_fast(texts, padding=True, truncation=True)
            fast_time = time.time() - start

            logger.info("=== æ€§èƒ½æ¯”è¾ƒ ===")
            logger.info(f"å•ç‹¬å¤„ç†: {individual_time:.2f}s")
            logger.info(f"æ‰¹é‡å¤„ç†: {batch_time:.2f}s ({individual_time/batch_time:.1f}x æ›´å¿«)")
            logger.info(f"å¿«é€Ÿåˆ†è¯å™¨: {fast_time:.2f}s ({batch_time/fast_time:.1f}x æ¯”æ‰¹é‡æ›´å¿«)")

            return {
                'individual': individual_time,
                'batch': batch_time,
                'fast': fast_time
            }

        # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        results = benchmark_tokenization_methods()

    __()
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **ä¼˜åŒ–ç­–ç•¥**ï¼š

    - **ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨**ï¼šåŸºäºRustçš„å®ç°æä¾›5-10å€åŠ é€Ÿ
    - **æ‰¹å¤„ç†**ï¼šæ˜¾è‘—å‡å°‘å¼€é”€
    - **å°½å¯èƒ½é¢„è®¡ç®—**ï¼šç¼“å­˜åˆ†è¯ç»“æœ
    - **ä¼˜åŒ–å¡«å……**ï¼šä½¿ç”¨åŠ¨æ€å¡«å……å‡å°‘æµªè´¹çš„è®¡ç®—
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

    ### é—®é¢˜1ï¼šåˆ†è¯å™¨-æ¨¡å‹ä¸åŒ¹é…

    ```python
    def detect_tokenizer_mismatch():
        from transformers import AutoModel

        # æ•…æ„ä¸åŒ¹é…
        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        model = AutoModel.from_pretrained('roberta-base')

        text = "This demonstrates tokenizer mismatch."

        try:
            inputs = tokenizer(text, return_tensors='pt')
            outputs = model(**inputs)
            logger.warning("æ¨¡å‹å¤„ç†äº†ä¸åŒ¹é…çš„è¾“å…¥ - ç»“æœä¸å¯é ï¼")
        except Exception as e:
            logger.error(f"ä¸åŒ¹é…é”™è¯¯: {e}")

        # æ­£ç¡®æ–¹æ³•
        logger.info("\\n=== æ­£ç¡®åŒ¹é… ===")
        model_name = 'roberta-base'
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)

        inputs = tokenizer(text, return_tensors='pt')
        outputs = model(**inputs)
        logger.info(f"æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {outputs.last_hidden_state.shape}")
    ```

    **å…³é”®è§„åˆ™**ï¼šå§‹ç»ˆä»åŒä¸€ä¸ªæ£€æŸ¥ç‚¹åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹ã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### é—®é¢˜2ï¼šå¤„ç†é•¿æ–‡æ¡£

    ```python
    def handle_long_documents():
        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        max_length = 512

        # åˆ›å»ºé•¿æ–‡æ¡£
        long_doc = " ".join(["This is a sentence."] * 200)

        # ç­–ç•¥1ï¼šç®€å•æˆªæ–­
        truncated = tokenizer(
            long_doc,
            max_length=max_length,
            truncation=True,
            return_tensors='pt'
        )

        logger.info(f"æ–‡æ¡£é•¿åº¦: {len(long_doc)} å­—ç¬¦")
        logger.info(f"æˆªæ–­ä¸º: {truncated['input_ids'].shape[1]} token")

        # ç­–ç•¥2ï¼šæ»‘åŠ¨çª—å£
        stride = 256
        chunks = []

        tokens = tokenizer.tokenize(long_doc)

        for i in range(0, len(tokens), stride):
            chunk = tokens[i:i + max_length - 2]  # ä¸ºç‰¹æ®Štokenä¿ç•™ç©ºé—´
            chunk_ids = tokenizer.convert_tokens_to_ids(chunk)
            chunk_ids = [tokenizer.cls_token_id] + chunk_ids + [tokenizer.sep_token_id]
            chunks.append(chunk_ids)

        logger.info(f"\\næ»‘åŠ¨çª—å£åˆ›å»ºäº† {len(chunks)} ä¸ªå—")
        logger.info(f"é‡å : {max_length - stride} ä¸ªtokenåœ¨å—ä¹‹é—´")

        return chunks
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **é•¿æ–‡æ¡£ç­–ç•¥**ï¼š

    - **æˆªæ–­**ï¼šå¿«é€Ÿä½†ä¼šä¸¢å¤±ä¿¡æ¯
    - **æ»‘åŠ¨çª—å£**ï¼šä¿ç•™æ‰€æœ‰å†…å®¹ä½†æœ‰é‡å 
    - **åˆ†å±‚å¤„ç†**ï¼šåˆ†åˆ«å¤„ç†å„éƒ¨åˆ†ç„¶ååˆå¹¶
    - **æ‘˜è¦**ï¼šåœ¨åˆ†è¯å‰å‡å°‘å†…å®¹
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## è°ƒè¯•åˆ†è¯

    æœ‰æ•ˆçš„è°ƒè¯•å¯ä»¥èŠ‚çœæ•°å°æ—¶çš„æ•…éšœæ’é™¤æ—¶é—´ï¼š

    ```python
    class TokenizationDebugger:
        def __init__(self, tokenizer):
            self.tokenizer = tokenizer

        def analyze_text(self, text):
            logger.info(f"\\n=== Analyzing: '{text}' ===")

            tokens = self.tokenizer.tokenize(text)
            token_ids = self.tokenizer.encode(text)

            logger.info(f"Character count: {len(text)}")
            logger.info(f"Token count: {len(tokens)}")
            logger.info(f"Compression ratio: {len(text)/len(tokens):.2f} chars/token")

            # Check for unknown tokens
            unk_count = tokens.count(self.tokenizer.unk_token)
            if unk_count > 0:
                logger.warning(f"Found {unk_count} unknown tokens!")

            return {
                'tokens': tokens,
                'token_ids': token_ids,
                'char_count': len(text),
                'token_count': len(tokens),
                'unk_count': unk_count
            }
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **è°ƒè¯•æ£€æŸ¥æ¸…å•**ï¼š

    - [ ] éªŒè¯åˆ†è¯å™¨ä¸æ¨¡å‹åŒ¹é…
    - [ ] æ£€æŸ¥è¿‡å¤šçš„æœªçŸ¥token
    - [ ] ç›‘æ§åºåˆ—é•¿åº¦
    - [ ] éªŒè¯ç‰¹æ®Štokenå¤„ç†
    - [ ] æµ‹è¯•è¾¹ç¼˜æƒ…å†µï¼ˆç©ºå­—ç¬¦ä¸²ã€ç‰¹æ®Šå­—ç¬¦ï¼‰
    - [ ] ä¸é¢„æœŸè¾“å‡ºæ¯”è¾ƒ
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## ä¸“ä¸šé¢†åŸŸçš„è‡ªå®šä¹‰åˆ†è¯å™¨

    æœ‰æ—¶é¢„è®­ç»ƒçš„åˆ†è¯å™¨ä¸é€‚åˆæ‚¨çš„é¢†åŸŸã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰åˆ†è¯å™¨ï¼š

    ```python
    def train_custom_medical_tokenizer():
        from tokenizers import Tokenizer, models, trainers, pre_tokenizers

        # Medical corpus (in practice, use larger dataset)
        medical_texts = [
            "Patient presents with acute myocardial infarction.",
            "Diagnosis: Type 2 diabetes mellitus with neuropathy.",
            "Prescribed metformin 500mg twice daily.",
            "MRI shows L4-L5 disc herniation with radiculopathy.",
            "Post-operative recovery following cholecystectomy.",
            "Chronic obstructive pulmonary disease exacerbation.",
            "Administered epinephrine for anaphylactic reaction.",
            "ECG reveals atrial fibrillation with rapid ventricular response."
        ]

        # Initialize BPE tokenizer
        tokenizer = Tokenizer(models.BPE())
        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

        # Configure trainer
        trainer = trainers.BpeTrainer(
            vocab_size=10000,
            special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
            min_frequency=2
        )

        # Train on medical corpus
        tokenizer.train_from_iterator(medical_texts, trainer=trainer)

        # Test on medical terms
        test_terms = [
            "myocardial infarction",
            "cholecystectomy",
            "pneumonia",
            "diabetes mellitus"
        ]

        logger.info("=== Custom Medical Tokenizer Results ===")
        for term in test_terms:
            encoding = tokenizer.encode(term)
            logger.info(f"\\n'{term}':")
            logger.info(f"  Tokens: {encoding.tokens}")
            logger.info(f"  IDs: {encoding.ids}")

        return tokenizer
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **è‡ªå®šä¹‰åˆ†è¯å™¨ä¼˜åŠ¿**ï¼š

    - **æ›´å¥½çš„è¦†ç›–ç‡**ï¼šä¿æŒé¢†åŸŸæœ¯è¯­å®Œæ•´
    - **æ›´å°çš„è¯æ±‡è¡¨**ï¼šä¸“æ³¨äºç›¸å…³æœ¯è¯­
    - **æé«˜å‡†ç¡®æ€§**ï¼šæ›´å¥½åœ°è¡¨ç¤ºé¢†åŸŸè¯­è¨€
    - **å‡å°‘Tokenæ•°**ï¼šæ›´é«˜æ•ˆçš„å¤„ç†
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### æ¯”è¾ƒé€šç”¨åˆ†è¯å™¨ä¸è‡ªå®šä¹‰åˆ†è¯å™¨

    ```python
    def compare_medical_tokenization():
        # Generic tokenizer
        generic = AutoTokenizer.from_pretrained('bert-base-uncased')

        # Medical terms that generic tokenizers fragment
        medical_terms = [
            "pneumonoultramicroscopicsilicovolcanoconiosis",
            "electroencephalography",
            "thrombocytopenia",
            "gastroesophageal"
        ]

        logger.info("=== Generic vs Domain Tokenization ===")

        for term in medical_terms:
            generic_tokens = generic.tokenize(term)

            logger.info(f"\\n'{term}':")
            logger.info(f"  Generic: {generic_tokens} ({len(generic_tokens)} tokens)")
            # Custom tokenizer would show fewer tokens

            # Calculate efficiency loss
            if len(generic_tokens) > 3:
                logger.warning(f"  âš ï¸ Excessive fragmentation: {len(generic_tokens)} pieces")
    ```

    ç¨åæˆ‘ä»¬å°†æ¯”è¾ƒé€šç”¨åˆ†è¯å™¨ä¸åŒ»å­¦æœ¯è¯­ä¸“ç”¨åˆ†è¯å™¨ã€‚è¯·é˜…è¯»åˆ°æœ€åï¼Œçœ‹çœ‹å®ƒä»¬çš„æ¯”è¾ƒç»“æœã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## è¾¹ç¼˜æƒ…å†µå’Œè§£å†³æ–¹æ¡ˆ

    ç°å®ä¸–ç•Œçš„æ–‡æœ¬å‘ˆç°è®¸å¤šæŒ‘æˆ˜ï¼š

    ```python
    def handle_edge_cases():
        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

        edge_cases = {
            "Empty string": "",
            "Only spaces": "     ",
            "Mixed languages": "Hello ä¸–ç•Œ Bonjour",
            "Emojis": "Great job! ğŸ‘ğŸ‰",
            "Code": "def func(x): return x**2",
            "URLs": "Visit <https://example.com/page>",
            "Special chars": "Price: $99.99 (â†‘15%)",
            "Long word": "a" * 100
        }

        logger.info("=== Edge Case Handling ===")

        for case_name, text in edge_cases.items():
            logger.info(f"\\n{case_name}: '{text[:50]}{'...' if len(text) > 50 else ''}'")

            try:
                tokens = tokenizer.tokenize(text)
                encoding = tokenizer(text, add_special_tokens=True)

                logger.info(f"  Success: {len(tokens)} tokens")

                # Check for issues
                if not tokens and text:
                    logger.warning("  âš ï¸ No tokens produced from non-empty text")

                if tokenizer.unk_token in tokens:
                    unk_count = tokens.count(tokenizer.unk_token)
                    logger.warning(f"  âš ï¸ Contains {unk_count} unknown tokens")

            except Exception as e:
                logger.error(f"  âŒ Error: {str(e)}")
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    **å¸¸è§è¾¹ç¼˜æƒ…å†µ**ï¼š

    - **ç©º/ç©ºç™½å­—ç¬¦**ï¼šè¿”å›ç©ºtokenåˆ—è¡¨æˆ–å¡«å……token
    - **æ··åˆæ–‡å­—**ï¼šå¯èƒ½äº§ç”ŸæœªçŸ¥token
    - **è¡¨æƒ…ç¬¦å·**ï¼šæ¯ä¸ªåˆ†è¯å™¨å¤„ç†æ–¹å¼ä¸åŒ
    - **URL/é‚®ç®±**ï¼šç»å¸¸è¢«é”™è¯¯åˆ†å‰²
    - **è¶…é•¿å•è¯**ï¼šå¯èƒ½è¶…è¿‡tokené™åˆ¶
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## å…³é”®è¦ç‚¹

    ### æ ¸å¿ƒæ¦‚å¿µ

    - **åˆ†è¯è¿æ¥æ–‡æœ¬å’Œç¥ç»ç½‘ç»œ** â€” è¿™æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½çš„å…³é”®ç¬¬ä¸€æ­¥
    - **ç®—æ³•é€‰æ‹©å¾ˆé‡è¦** â€” BPEã€WordPieceå’ŒUnigramå„è‡ªåœ¨ä¸åŒåº”ç”¨ä¸­æœ‰ä¼˜åŠ¿
    - **å§‹ç»ˆåŒ¹é…åˆ†è¯å™¨å’Œæ¨¡å‹** â€” ä¸åŒ¹é…ä¼šå¯¼è‡´é™é»˜å¤±è´¥å’Œç³Ÿç³•ç»“æœ
    - **ç‰¹æ®Štokenæä¾›ç»“æ„** â€” [CLS]ã€[SEP]ç­‰å¸®åŠ©æ¨¡å‹ç†è§£åºåˆ—
    - **ç”Ÿäº§éœ€è¦ä¼˜åŒ–** â€” ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨å’Œæ‰¹å¤„ç†æé«˜æ•ˆç‡
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

    - [ ] è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ç›¸åŒçš„åˆ†è¯å™¨
    - [ ] ä¼˜é›…å¤„ç†è¾¹ç¼˜æƒ…å†µï¼ˆç©ºå­—ç¬¦ä¸²ã€ç‰¹æ®Šå­—ç¬¦ï¼‰
    - [ ] å®ç°é€‚å½“çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    - [ ] é’ˆå¯¹ç”Ÿäº§çº¦æŸä¼˜åŒ–ï¼ˆé€Ÿåº¦vså‡†ç¡®æ€§ï¼‰
    - [ ] ä½¿ç”¨çœŸå®ä¸–ç•Œæ•°æ®æµ‹è¯•ï¼ŒåŒ…æ‹¬è¾¹ç¼˜æƒ…å†µ
    - [ ] ç›‘æ§åˆ†è¯æŒ‡æ ‡ï¼ˆæœªçŸ¥tokenç‡ã€åºåˆ—é•¿åº¦ï¼‰
    - [ ] è€ƒè™‘ä¸“ä¸šåº”ç”¨çš„é¢†åŸŸç‰¹å®šåˆ†è¯å™¨
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### å¿«é€Ÿå‚è€ƒ

    ```python
    # æ ‡å‡†è®¾ç½®
    from transformers import AutoTokenizer

    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

    # åŸºç¡€ç”¨æ³•
    tokens = tokenizer.tokenize("Hello world")
    encoding = tokenizer("Hello world", return_tensors='pt')

    # ç”Ÿäº§ç”¨æ³•
    encoding = tokenizer(
        texts,                    # å­—ç¬¦ä¸²åˆ—è¡¨
        padding=True,            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
        truncation=True,         # æˆªæ–­åˆ°max_length
        max_length=512,         # æœ€å¤§åºåˆ—é•¿åº¦
        return_tensors='pt',    # è¿”å›PyTorchå¼ é‡
        return_attention_mask=True,  # è¿”å›æ³¨æ„åŠ›æ©ç 
        return_offsets_mapping=True  # ç”¨äºNERä»»åŠ¡
    )

    # è®¿é—®ç»“æœ
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## ä¸‹ä¸€æ­¥

    1. **åœ¨æ‚¨çš„æ•°æ®ä¸Šå®éªŒä¸åŒçš„åˆ†è¯å™¨**
    2. **æµ‹é‡æ‚¨ç”¨ä¾‹çš„åˆ†è¯æŒ‡æ ‡**
    3. **å¦‚éœ€è¦æ„å»ºè‡ªå®šä¹‰åˆ†è¯å™¨**
    4. **ä¸æ‚¨çš„æ¨¡å‹ç®¡é“é›†æˆ**
    5. **ç›‘æ§ç”Ÿäº§æ€§èƒ½**

    åˆ†è¯å¯èƒ½çœ‹èµ·æ¥ç®€å•ï¼Œä½†å®ƒæ˜¯æ¯ä¸ªNLPç³»ç»Ÿçš„åŸºç¡€ã€‚æŒæ¡å®ƒï¼Œæ‚¨å°†æ„å»ºæ›´å¼ºå¤§å’Œé«˜æ•ˆçš„åº”ç”¨ç¨‹åºã€‚

    ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®é™…ä½¿ç”¨è¿™äº›ç¤ºä¾‹ã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## ç³»åˆ—æ–‡ç« 

    å¦‚æœæ‚¨å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·åœ¨LinkedInæˆ–Mediumä¸Šå…³æ³¨Rickï¼Œè·å–æ›´å¤šä¼ä¸šAIå’ŒAIæ´å¯Ÿã€‚

    è¯·åŠ¡å¿…æŸ¥çœ‹æœ¬ç³»åˆ—çš„å‰å››ç¯‡æ–‡ç« ï¼š

    1. [**Hugging Face Transformerså’ŒAIé©å‘½**ï¼ˆç¬¬1ç¯‡ï¼‰](https://medium.com/@richardhightower/transformers-and-the-ai-revolution-the-role-of-hugging-face-f185f574b91b)
    2. [**Hugging Faceï¼šä¸ºä»€ä¹ˆè¯­è¨€å¯¹AIæ¥è¯´å¾ˆå›°éš¾ï¼ŸTransformerå¦‚ä½•æ”¹å˜è¿™ä¸€ç‚¹**ï¼ˆç¬¬2ç¯‡ï¼‰](https://medium.com/@richardhightower/why-language-is-hard-for-ai-and-how-transformers-changed-everything-d8a1fa299f1e)
    3. [**Hugging Faceå®è·µï¼šæ„å»ºæ‚¨çš„AIå·¥ä½œç©ºé—´**ï¼ˆç¬¬3ç¯‡ï¼‰](https://medium.com/@richardhightower/hands-on-with-hugging-face-building-your-ai-workspace-b23c7e9be3a7)
    4. [**Transformerå†…éƒ¨ï¼šæ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶æ­ç§˜**ï¼ˆç¬¬4ç¯‡ï¼‰](https://medium.com/@richardhightower/inside-the-transformer-architecture-and-attention-demystified-39b2c13130bd)
    5. **åˆ†è¯ â€” å°†æ–‡æœ¬è½¬æ¢ä¸ºç¥ç»ç½‘ç»œçš„æ•°å­—**ï¼ˆç¬¬5ç¯‡ - æœ¬ç¯‡ï¼‰
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## GitHubä»“åº“ä½¿ç”¨è¯´æ˜

    ### åˆ†è¯ â€” å°†æ–‡æœ¬è½¬æ¢ä¸ºç¥ç»ç½‘ç»œçš„æ•°å­—

    æœ¬é¡¹ç›®åŒ…å«Hugging Face Transformersç³»åˆ—ç¬¬5ç¯‡æ–‡ç« ï¼šåˆ†è¯çš„å·¥ä½œç¤ºä¾‹ã€‚

    ğŸ”— **GitHubä»“åº“**: https://github.com/RichardHightower/art_hug_05

    ### å‰ç½®è¦æ±‚

    - Python 3.12ï¼ˆé€šè¿‡pyenvç®¡ç†ï¼‰
    - Poetryç”¨äºä¾èµ–ç®¡ç†
    - Go Taskç”¨äºæ„å»ºè‡ªåŠ¨åŒ–
    - æ‰€éœ€æœåŠ¡çš„APIå¯†é’¥ï¼ˆå‚è§.env.exampleï¼‰
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### è®¾ç½®æ­¥éª¤

    1. **å…‹éš†ä»“åº“**ï¼š
    ```bash
    git clone git@github.com:RichardHightower/art_hug_05.git
    cd art_hug_05
    ```

    2. **è¿è¡Œè®¾ç½®ä»»åŠ¡**ï¼š
    ```bash
    task setup
    ```

    3. **é…ç½®ç¯å¢ƒ**ï¼š
    ```bash
    cp .env.example .env
    # æ ¹æ®éœ€è¦é…ç½®.envæ–‡ä»¶
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### é¡¹ç›®ç»“æ„

    ```
    .
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ config.py              # é…ç½®å’Œå·¥å…·
    â”‚   â”œâ”€â”€ main.py                # åŒ…å«æ‰€æœ‰ç¤ºä¾‹çš„å…¥å£ç‚¹
    â”‚   â”œâ”€â”€ tokenization_examples.py       # åŸºç¡€åˆ†è¯ç¤ºä¾‹
    â”‚   â”œâ”€â”€ tokenization_algorithms.py     # BPEã€WordPieceå’ŒUnigramæ¯”è¾ƒ
    â”‚   â”œâ”€â”€ custom_tokenization.py         # è®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨
    â”‚   â”œâ”€â”€ tokenization_debugging.py      # è°ƒè¯•å’Œå¯è§†åŒ–å·¥å…·
    â”‚   â”œâ”€â”€ multimodal_tokenization.py     # å›¾åƒå’ŒCLIPåˆ†è¯
    â”‚   â”œâ”€â”€ advanced_tokenization.py       # é«˜çº§åˆ†è¯æŠ€æœ¯
    â”‚   â”œâ”€â”€ model_loading.py               # æ¨¡å‹åŠ è½½ç¤ºä¾‹
    â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•°
    â”œâ”€â”€ tests/
    â”‚   â””â”€â”€ test_examples.py       # å•å…ƒæµ‹è¯•
    â”œâ”€â”€ .env.example               # ç¯å¢ƒæ¨¡æ¿
    â”œâ”€â”€ Taskfile.yml               # ä»»åŠ¡è‡ªåŠ¨åŒ–
    â””â”€â”€ pyproject.toml             # Poetryé…ç½®
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### è¿è¡Œç¤ºä¾‹

    **è¿è¡Œæ‰€æœ‰ç¤ºä¾‹**ï¼š
    ```bash
    task run
    ```

    **æˆ–è¿è¡Œå•ä¸ªæ¨¡å—**ï¼š
    ```bash
    task run-tokenization          # è¿è¡ŒåŸºç¡€åˆ†è¯ç¤ºä¾‹
    task run-algorithms            # è¿è¡Œåˆ†è¯ç®—æ³•æ¯”è¾ƒ
    task run-custom                # è¿è¡Œè‡ªå®šä¹‰åˆ†è¯å™¨è®­ç»ƒ
    task run-debugging             # è¿è¡Œåˆ†è¯è°ƒè¯•å·¥å…·
    task run-multimodal            # è¿è¡Œå¤šæ¨¡æ€åˆ†è¯
    task run-advanced              # è¿è¡Œé«˜çº§åˆ†è¯æŠ€æœ¯
    task run-model-loading         # è¿è¡Œæ¨¡å‹åŠ è½½ç¤ºä¾‹
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### åŠ è½½ç¬”è®°æœ¬

    å¯åŠ¨Jupyterç¬”è®°æœ¬ï¼š
    ```bash
    task notebook
    ```

    è¿™å°†å¯åŠ¨ä¸€ä¸ªJupyteræœåŠ¡å™¨ï¼Œæ‚¨å¯ä»¥ï¼š

    - åˆ›å»ºäº¤äº’å¼ç¬”è®°æœ¬è¿›è¡Œå®éªŒ
    - é€æ­¥è¿è¡Œä»£ç å•å…ƒ
    - å¯è§†åŒ–åˆ†è¯ç»“æœ
    - äº¤äº’å¼æµ‹è¯•ä¸åŒçš„åˆ†è¯å™¨
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### å¯ç”¨ä»»åŠ¡

    - `task setup` - è®¾ç½®Pythonç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
    - `task run` - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    - `task run-tokenization` - è¿è¡ŒåŸºç¡€åˆ†è¯ç¤ºä¾‹
    - `task run-algorithms` - è¿è¡Œç®—æ³•æ¯”è¾ƒç¤ºä¾‹
    - `task run-custom` - è¿è¡Œè‡ªå®šä¹‰åˆ†è¯å™¨è®­ç»ƒ
    - `task run-debugging` - è¿è¡Œè°ƒè¯•å’Œå¯è§†åŒ–å·¥å…·
    - `task run-multimodal` - è¿è¡Œå¤šæ¨¡æ€åˆ†è¯ç¤ºä¾‹
    - `task run-advanced` - è¿è¡Œé«˜çº§åˆ†è¯æŠ€æœ¯
    - `task run-model-loading` - è¿è¡Œæ¨¡å‹åŠ è½½ç¤ºä¾‹
    - `task notebook` - å¯åŠ¨Jupyterç¬”è®°æœ¬æœåŠ¡å™¨
    - `task test` - è¿è¡Œå•å…ƒæµ‹è¯•
    - `task format` - ä½¿ç”¨Blackå’ŒRuffæ ¼å¼åŒ–ä»£ç 
    - `task lint` - è¿è¡Œä»£ç æ£€æŸ¥ï¼ˆBlackã€Ruffã€mypyï¼‰
    - `task clean` - æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## åœ¨Macå’ŒWindowsä¸Šè®¾ç½®Pythonå’ŒGo Task

    ### å®‰è£…Python

    #### åœ¨macOSä¸Š

    1. **ä½¿ç”¨Homebrewï¼ˆæ¨èï¼‰**ï¼š
    ```bash
    brew install pyenv
    ```

    2. **ä½¿ç”¨pyenvå®‰è£…Python 3.12**ï¼š
    ```bash
    pyenv install 3.12.0
    pyenv global 3.12.0
    ```

    3. **éªŒè¯å®‰è£…**ï¼š
    ```bash
    python --version
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    #### åœ¨Windowsä¸Š

    1. **ä»Python.orgä¸‹è½½å®‰è£…ç¨‹åº**
    2. **è¿è¡Œå®‰è£…ç¨‹åºå¹¶ç¡®ä¿å‹¾é€‰"Add Python to PATH"**
    3. **æ‰“å¼€å‘½ä»¤æç¤ºç¬¦å¹¶éªŒè¯å®‰è£…**ï¼š
    ```cmd
    python --version
    ```

    4. **å®‰è£…pyenv for Windowsï¼ˆå¯é€‰ï¼‰**ï¼š
    ```cmd
    pip install pyenv-win
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### å®‰è£…Poetry

    #### åœ¨macOSä¸Š

    1. **ä½¿ç”¨å®˜æ–¹å®‰è£…ç¨‹åº**ï¼š
    ```bash
    curl -sSL https://install.python-poetry.org | python3 -
    ```

    2. **å°†Poetryæ·»åŠ åˆ°PATH**ï¼š
    ```bash
    echo 'export PATH="$HOME/.poetry/bin:$PATH"' >> ~/.zshrc
    source ~/.zshrc
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    #### åœ¨Windowsä¸Š

    1. **ä½¿ç”¨PowerShellå®‰è£…**ï¼š
    ```powershell
    (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -
    ```

    2. **å°†Poetryæ·»åŠ åˆ°PATHï¼ˆå®‰è£…ç¨‹åºåº”è¯¥è‡ªåŠ¨å®Œæˆï¼‰**

    3. **éªŒè¯å®‰è£…**ï¼š
    ```cmd
    poetry --version
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### å®‰è£…Go Task

    #### åœ¨macOSä¸Š

    1. **ä½¿ç”¨Homebrew**ï¼š
    ```bash
    brew install go-task/tap/go-task
    ```

    2. **éªŒè¯å®‰è£…**ï¼š
    ```bash
    task --version
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    #### åœ¨Windowsä¸Š

    1. **ä½¿ç”¨Scoop**ï¼š
    ```cmd
    scoop install go-task
    ```

    2. **æˆ–ä½¿ç”¨Chocolatey**ï¼š
    ```cmd
    choco install go-task
    ```

    3. **æˆ–ä»GitHub Releasesç›´æ¥ä¸‹è½½å¹¶æ·»åŠ åˆ°PATH**

    4. **éªŒè¯å®‰è£…**ï¼š
    ```cmd
    task --version
    ```

    ---

    ç°åœ¨æ‚¨å·²ç»æ‹¥æœ‰äº†å®Œæ•´çš„åˆ†è¯çŸ¥è¯†å’Œå®è·µå·¥å…·ï¼å¼€å§‹æ¢ç´¢å’Œå®éªŒå§ï¼
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## é¡¹ç›®è®¾ç½®

    å®‰è£…æ‰€æœ‰å‰ç½®è¦æ±‚åï¼Œæ‚¨å¯ä»¥æŒ‰ç…§å‰é¢éƒ¨åˆ†çš„è®¾ç½®è¯´æ˜æ¥è¿è¡Œé¡¹ç›®ã€‚

    ### å¸¸è§é—®é¢˜æ’é™¤

    - **Pythonæœªæ‰¾åˆ°**ï¼šç¡®ä¿Pythonæ­£ç¡®æ·»åŠ åˆ°PATHå˜é‡
    - **Poetryå‘½ä»¤ä¸å·¥ä½œ**ï¼šé‡å¯ç»ˆç«¯æˆ–å°†Poetry binç›®å½•æ·»åŠ åˆ°PATH
    - **Taskæœªæ‰¾åˆ°**ï¼šéªŒè¯Taskå®‰è£…å’ŒPATHè®¾ç½®
    - **ä¾èµ–é”™è¯¯**ï¼šè¿è¡Œ`poetry update`è§£å†³ä¾èµ–å†²çª
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## åŒ»å­¦åˆ†è¯ç¤ºä¾‹

    æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ¯”è¾ƒä¸“ä¸šåŒ»å­¦åˆ†è¯ä¸éåŒ»å­¦åˆ†è¯çš„ç¤ºä¾‹ã€‚

    ```bash
    task run-medical
    ```

    **è¾“å‡ºç¤ºä¾‹**ï¼š
    ```
    INFO:__main__:ğŸ¥ Medical Tokenization Examples
    INFO:__main__:==================================================
    INFO:__main__:
    === Generic vs Domain Tokenization ===
    INFO:__main__:
    'pneumonoultramicroscopicsilicovolcanoconiosis':
    INFO:__main__:  Generic: ['p', '##ne', '##um', '##ono', '##ult', '##ram', '##ic', '##ros', '##copic', '##sil', '##ico', '##vo', '##lc', '##ano', '##con', '##ios', '##is'] (17 tokens)
    WARNING:__main__:  âš ï¸ Excessive fragmentation: 17 pieces

    'electroencephalography':
    INFO:__main__:  Generic: ['electro', '##ence', '##pha', '##log', '##raphy'] (5 tokens)
    WARNING:__main__:  âš ï¸ Excessive fragmentation: 5 pieces
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### MedCPT vs é€šç”¨BERTæ¯”è¾ƒç»“æœ

    ```
    === Comparison with Generic BERT ===

    'diabetes insipidus':
      MedCPT: 4 tokens
      Generic BERT: 5 tokens
      âœ… MedCPT is 1 tokens more efficient

    'vasopressinergic neurons':
      MedCPT: 3 tokens
      Generic BERT: 6 tokens
      âœ… MedCPT is 3 tokens more efficient

    'hypothalamic destruction':
      MedCPT: 2 tokens
      Generic BERT: 6 tokens
      âœ… MedCPT is 4 tokens more efficient

    'polyuria and polydipsia':
      MedCPT: 6 tokens
      Generic BERT: 7 tokens
      âœ… MedCPT is 1 tokens more efficient
    ```

    å¯ä»¥çœ‹åˆ°ä¸“ä¸šæ¨¡å‹åœ¨åŒ»å­¦æœ¯è¯­æ–¹é¢æ¯”é€šç”¨æ¨¡å‹æ›´é«˜æ•ˆã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### åŒ»å­¦åˆ†è¯æ¼”ç¤ºä»£ç 

    è®©æˆ‘ä»¬æ£€æŸ¥é©±åŠ¨åŒ»å­¦åˆ†è¯æ¼”ç¤ºçš„ä»£ç ã€‚ä¸‹é¢çš„è„šæœ¬æ¯”è¾ƒäº†ä¸“ä¸šåŒ»å­¦åˆ†è¯å™¨å¦‚ä½•å¤„ç†å¤æ‚åŒ»å­¦æœ¯è¯­ä¸é€šç”¨åˆ†è¯å™¨çš„å¯¹æ¯”ï¼š

    ```python
    # Medical Tokenization Demo
    # Standalone script to run medical tokenization examples

    from transformers import AutoTokenizer, AutoModel
    import torch
    import logging

    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)


    def compare_medical_tokenization():
        # Generic tokenizer
        generic = AutoTokenizer.from_pretrained('bert-base-uncased')

        # Medical terms that generic tokenizers fragment
        medical_terms = [
            "pneumonoultramicroscopicsilicovolcanoconiosis",
            "electroencephalography",
            "thrombocytopenia",
            "gastroesophageal"
        ]

        logger.info("\\n=== Generic vs Domain Tokenization ===")

        for term in medical_terms:
            generic_tokens = generic.tokenize(term)

            logger.info(f"\\n'{term}':")
            logger.info(f"  Generic: {generic_tokens} ({len(generic_tokens)} tokens)")

            # Calculate efficiency loss
            if len(generic_tokens) > 3:
                logger.warning(f"  âš ï¸ Excessive fragmentation: {len(generic_tokens)} pieces")
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ```python
    def medcpt_encoder_example():
        logger.info("\\n=== MedCPT Biomedical Text Encoder Example ===")

        try:
            # Load MedCPT Article Encoder
            logger.info("Loading MedCPT Article Encoder...")
            model = AutoModel.from_pretrained("ncbi/MedCPT-Article-Encoder")
            tokenizer = AutoTokenizer.from_pretrained("ncbi/MedCPT-Article-Encoder")

            # Example medical articles
            articles = [
                [
                    "Diagnosis and Management of Central Diabetes Insipidus in Adults",
                    "Central diabetes insipidus (CDI) is a clinical syndrome...",
                ],
                [
                    "Adipsic diabetes insipidus",
                    "Adipsic diabetes insipidus (ADI) is a rare but devastating disorder...",
                ],
                [
                    "Nephrogenic diabetes insipidus: a comprehensive overview",
                    "Nephrogenic diabetes insipidus (NDI) is characterized by...",
                ],
            ]

            # Format articles for the model
            formatted_articles = [f"{title}. {abstract}" for title, abstract in articles]

            with torch.no_grad():
                # Tokenize the articles
                encoded = tokenizer(
                    formatted_articles,
                    truncation=True,
                    padding=True,
                    return_tensors='pt',
                    max_length=512,
                )

                # Encode the articles
                embeds = model(**encoded).last_hidden_state[:, 0, :]

                logger.info(f"\\nEmbedding shape: {embeds.shape}")
                logger.info(f"Embedding dimension: {embeds.shape[1]}")
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ```python
                # Show tokenization comparison for medical terms
                logger.info("\\n=== MedCPT Tokenization of Medical Terms ===")

                medical_terms = [
                    "diabetes insipidus",
                    "vasopressinergic neurons",
                    "hypothalamic destruction",
                    "polyuria and polydipsia"
                ]

                for term in medical_terms:
                    tokens = tokenizer.tokenize(term)
                    logger.info(f"\\n'{term}':")
                    logger.info(f"  Tokens: {tokens} ({len(tokens)} tokens)")

                # Compare with generic BERT tokenizer
                generic = AutoTokenizer.from_pretrained('bert-base-uncased')
                logger.info("\\n=== Comparison with Generic BERT ===")

                for term in medical_terms:
                    medcpt_tokens = tokenizer.tokenize(term)
                    generic_tokens = generic.tokenize(term)

                    logger.info(f"\\n'{term}':")
                    logger.info(f"  MedCPT: {len(medcpt_tokens)} tokens")
                    logger.info(f"  Generic BERT: {len(generic_tokens)} tokens")

                    if len(generic_tokens) > len(medcpt_tokens):
                        logger.info(f"  âœ… MedCPT is {len(generic_tokens) - len(medcpt_tokens)} tokens more efficient")

        except Exception as e:
            logger.error(f"Error loading MedCPT model: {e}")
            logger.info("Install with: pip install transformers torch")
            logger.info("Note: MedCPT model requires downloading ~440MB")


    def main():
        logger.info("ğŸ¥ Medical Tokenization Examples")
        logger.info("=" * 50)

        # Run generic vs domain comparison
        compare_medical_tokenization()

        # Run MedCPT encoder example
        medcpt_encoder_example()

        logger.info("\\nâœ… Medical tokenization examples completed!")


    if __name__ == "__main__":
        main()
    ```
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### ç¤ºä¾‹åˆ†æ

    è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†ä¸“ä¸šåŒ»å­¦åˆ†è¯ä¸é€šç”¨åˆ†è¯çš„å·¥ä½œåŸç†å¯¹æ¯”ã€‚è®©æˆ‘ä»¬åˆ†è§£ä¸€ä¸‹ï¼š

    **ç¤ºä¾‹åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†**ï¼š

    1. **é€šç”¨vsé¢†åŸŸåˆ†è¯æ¯”è¾ƒ**ï¼šæ˜¾ç¤ºæ ‡å‡†åˆ†è¯å™¨å¦‚ä½•å°†å¤æ‚åŒ»å­¦æœ¯è¯­åˆ†è§£ä¸ºè®¸å¤šå°ç‰‡æ®µï¼ˆtokenï¼‰
    2. **MedCPTç¼–ç å™¨ç¤ºä¾‹**ï¼šæ¼”ç¤ºä¸“é—¨çš„åŒ»å­¦æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ï¼Œæ›´å¥½åœ°ç†è§£åŒ»å­¦æœ¯è¯­
    3. **åˆ†è¯å™¨ä¹‹é—´çš„æ¯”è¾ƒ**ï¼šç›´æ¥æ¯”è¾ƒä½¿ç”¨ä¸¤ç§åˆ†è¯å™¨å¤„ç†ç›¸åŒåŒ»å­¦çŸ­è¯­éœ€è¦å¤šå°‘token

    **ç»“æœæ¸…æ¥šæ˜¾ç¤º**ï¼š

    - é€šç”¨åˆ†è¯å™¨åœ¨åŒ»å­¦æœ¯è¯­æ–¹é¢è¡¨ç°å›°éš¾
    - ä¾‹å¦‚ï¼Œå®ƒä»¬å°†"hypothalamic destruction"åˆ†å‰²ä¸º6ä¸ªtokenï¼Œè€ŒåŒ»å­¦åˆ†è¯å™¨åªéœ€è¦2ä¸ªtoken
    - æ›´å°‘çš„tokenæ„å‘³ç€æ›´é«˜æ•ˆçš„å¤„ç†ï¼ˆèŠ‚çœæ—¶é—´å’Œè®¡ç®—èµ„æºï¼‰
    - æ›´å¥½çš„åˆ†è¯å¯¼è‡´æ›´å¥½çš„æ–‡æœ¬å«ä¹‰ç†è§£
    - ä¸“ä¸šæ¨¡å‹å¯ä»¥åœ¨tokené™åˆ¶å†…å¤„ç†æ›´é•¿çš„åŒ»å­¦æ–‡æœ¬

    **ç¤ºä¾‹åŠ è½½ä¸¤ç§ä¸åŒçš„åˆ†è¯å™¨**ï¼š

    - é€šç”¨çš„"bert-base-uncased"ï¼Œé€‚ç”¨äºæ—¥å¸¸è¯­è¨€
    - ä¸“é—¨çš„"MedCPT-Article-Encoder"ï¼Œä¸“é—¨åœ¨åŒ»å­¦æ–‡æœ¬ä¸Šè®­ç»ƒ

    ç»“æœç¡®è®¤äº†æ–‡ç« è®¨è®ºçš„å†…å®¹ï¼šé¢†åŸŸç‰¹å®šåˆ†è¯å¯¹ä¸“ä¸šæ–‡æœ¬æ˜¾è‘—æ›´é«˜æ•ˆï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å‡å°‘é«˜è¾¾66%çš„tokenæ•°é‡ï¼Œç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½å’Œæˆæœ¬ã€‚
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## ç¬”è®°æœ¬ç¤ºä¾‹

    æœ‰å‡ ä¸ªç¬”è®°æœ¬å¯ä»¥è®©æ‚¨æµè§ˆæœ¬æ–‡ä¸­çš„å¤§éƒ¨åˆ†ç¤ºä¾‹ã€‚åªéœ€ä»ä¸Šè¿°ä»“åº“ä¸‹è½½æºä»£ç ï¼Œç„¶åè¿è¡Œ`task notebook`ï¼Œå¯¼èˆªåˆ°notebooksæ–‡ä»¶å¤¹å¹¶åŠ è½½ç¬”è®°æœ¬ï¼Œè¿è¡Œç¤ºä¾‹ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.7zqsc57zfj.webp)

    ![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.8dx830geda.webp)
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## å…³äºä½œè€…

    Rick Hightoweræ‹¥æœ‰ä¸°å¯Œçš„ä¼ä¸šç»éªŒï¼Œæ›¾æ‹…ä»»è´¢å¯Œ100å¼ºå…¬å¸çš„é«˜ç®¡å’Œæ°å‡ºå·¥ç¨‹å¸ˆï¼Œä¸“é—¨ä»äº‹æœºå™¨å­¦ä¹ å’ŒAIè§£å†³æ–¹æ¡ˆï¼Œä¸ºå®¢æˆ·æä¾›æ™ºèƒ½ä½“éªŒã€‚ä»–çš„ä¸“ä¸šçŸ¥è¯†æ¶µç›–AIæŠ€æœ¯çš„ç†è®ºåŸºç¡€å’Œå®é™…åº”ç”¨ã€‚

    ä½œä¸ºTensorFlowè®¤è¯ä¸“ä¸šäººå£«å’Œæ–¯å¦ç¦å¤§å­¦æœºå™¨å­¦ä¹ ä¸“ä¸šåŒ–è¯¾ç¨‹çš„æ¯•ä¸šç”Ÿï¼ŒRickå°†å­¦æœ¯ä¸¥è°¨æ€§ä¸å®é™…å®æ–½ç»éªŒç›¸ç»“åˆã€‚ä»–çš„åŸ¹è®­åŒ…æ‹¬æŒæ¡ç›‘ç£å­¦ä¹ æŠ€æœ¯ã€ç¥ç»ç½‘ç»œå’Œé«˜çº§AIæ¦‚å¿µï¼Œå¹¶å·²æˆåŠŸå°†è¿™äº›åº”ç”¨äºä¼ä¸šçº§è§£å†³æ–¹æ¡ˆã€‚

    å‡­å€Ÿå¯¹AIå®æ–½çš„ä¸šåŠ¡å’ŒæŠ€æœ¯æ–¹é¢çš„æ·±å…¥ç†è§£ï¼ŒRickåœ¨ç†è®ºæœºå™¨å­¦ä¹ æ¦‚å¿µå’Œå®é™…ä¸šåŠ¡åº”ç”¨ä¹‹é—´æ¶èµ·äº†æ¡¥æ¢ï¼Œå¸®åŠ©ç»„ç»‡åˆ©ç”¨AIåˆ›é€ æœ‰å½¢ä»·å€¼ã€‚

    åœ¨LinkedInæˆ–Mediumä¸Šå…³æ³¨Rickï¼Œè·å–æ›´å¤šä¼ä¸šAIå’ŒAIæ´å¯Ÿã€‚
    """
    )
    return


if __name__ == "__main__":
    app.run()
