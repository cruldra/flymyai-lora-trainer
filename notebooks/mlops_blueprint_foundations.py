import marimo

__generated_with = "0.16.2"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # 完整的MLOps蓝图：生产环境中机器学习的背景和基础

    ## 引言

    所以，你已经训练了你的机器学习模型并测试了它的推理能力。

    接下来呢？

    你的工作完成了吗？

    并不是。

    你完成的只是一个更大旅程中的一小部分，这个旅程将在接下来展开。

    如果你计划在现实世界的应用中部署模型，还有许多额外的步骤需要考虑。这就是MLOps变得至关重要的地方，它帮助你从模型开发过渡到生产就绪的系统。

    生产环境中的机器学习运维（MLOps）是关于将ML模型集成到现实世界的软件系统中。这是机器学习与软件工程、DevOps和数据工程相遇的地方。

    目标是可靠地向最终用户大规模交付ML驱动的功能（如推荐引擎、欺诈检测器、语音助手等）。

    因此，如前所述，一个关键的认识是ML模型或算法本身只是生产ML系统的一小部分。

    ![ML系统中模型的占比](https://www.dailydoseofds.com/content/images/2025/07/image-99-1.png)

    在现实世界的部署中，需要大量的"胶水"围绕模型来构建一个完整的系统，包括数据管道、特征工程、模型服务基础设施、用户界面、监控等等。

    "ML系统"中只有很小一部分是ML代码；周围庞大的基础设施（用于数据、配置、自动化、服务、监控等）要大得多且更复杂。

    ![生产环境中ML系统概念图](https://www.dailydoseofds.com/content/images/2025/07/image-89.png)
    *生产环境中概念性ML系统，描述了ML模型代码在完整项目中的占比*

    MLOps通过将可靠的软件工程和DevOps实践应用于ML系统来管理这种复杂性，确保所有这些组件协同工作以交付价值。

    我们正在开始这个MLOps和LLMOps速成课程，为您提供全面的解释和系统级思维，以构建用于生产环境的AI模型。

    就像MCP速成课程一样，每一章都将清楚地解释必要的概念，提供示例、图表和实现。

    随着我们的进展，我们将看到如何培养将模型提升到下一个阶段所需的批判性思维，以及确切的框架应该是什么。

    在这个速成课程中，我们将从MLOps的基础开始，融入LLMOps，并随着每一章逐步加深我们的理解。

    对于第一部分，我们将专注于奠定基础，探索什么是MLOps，为什么它很重要，以及机器学习模型的生命周期。

    👉

    本课程假设您在Python编程、概率论、软件工程和测试实践以及机器学习算法、技术和评估指标的实现理解方面有扎实的基础。本系列避开这些基本概念，更多地关注ML系统的不同方面。

    让我们开始吧！

    ---
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 为什么MLOps很重要？

    在笔记本中构建一个高度准确的模型只是旅程的开始。

    一旦部署，ML模型面临着不断变化的现实世界条件：用户可能随时间表现不同，数据可能漂移，模型性能可能衰减。

    换句话说，模型的质量或有用性在部署后不会保持静态。

    现实世界在变化：用户可能随时间表现不同，对手可能试图操纵你的模型，或者简单地说，初始模型在新数据上的表现可能不如在训练的旧数据上那么好。

    ![模型性能随时间衰减](https://www.dailydoseofds.com/content/images/2025/07/image-102.png)

    在缺乏适当运维的情况下，一个准确的模型在为客户服务时可能很快变得不可靠甚至有害。

    在生产环境中，ML系统必须连续运行，通常是24/7，并处理不断演变的数据和使用模式，同时满足延迟、吞吐量和质量的要求。

    不充分的MLOps可能导致陈旧或不正确的模型在生产中徘徊，造成伤害业务的错误预测。

    例如，一个做出有缺陷的财务决策或不当内容推荐的模型。

    没有适当的MLOps，团队经常最终得到手动的、脆弱的模型部署流程。数据科学家可能手动准备数据并将模型交给工程师进行一次性生产集成，导致缓慢的迭代和容易出错的部署。

    ![手动跟踪系统](https://www.dailydoseofds.com/content/images/2025/07/image-101.png)
    *手动跟踪系统——表格、文档等，很快就变得难以管理*

    在成熟的MLOps出现之前，构建和更新ML系统是缓慢而费力的，每个新模型都需要大量资源和跨团队协调。这导致了以下问题：

    - **上市时间慢**：由于临时流程和冗长的交接，部署新模型需要数周或数月。
    - **脆弱的管道**：容易破坏的手动步骤。
    - **扩展问题**：没有自动化很难处理不断增长的数据或模型复杂性。

    有效的MLOps通过建立流程来持续监控、评估和改进部署后的模型来解决这些风险。

    👉

    MLOps这个术语本身在2015年左右由Google的一篇论文"机器学习系统中的隐藏技术债务"推广。该论文强调了ML系统如何快速积累维护挑战，如数据依赖、纠缠的代码和反馈循环，如果不加以管理，这些会像利息一样复合（技术债务）。

    总的来说，MLOps可以被认为是"ML的DevOps"，但更加细致。

    它是一套实践、工具和团队流程，旨在可靠高效地在生产中构建、部署和维护机器学习模型。

    这包括数据科学家和工程师之间的协作、ML管道的自动化，以及将软件最佳实践应用于ML，如测试、版本控制、持续集成等。

    ![MLOps概念图](https://www.dailydoseofds.com/content/images/2025/07/image-104.png)

    通过建立正确的基础设施和工作流程，MLOps使我们能够快速迭代模型，同时保持它们在实时环境中稳定运行。

    在后续章节中，我们将深入探讨生产ML系统与传统软件的不同之处以及ML系统设计的生命周期。

    ---
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## MLOps vs. DevOps和传统软件系统

    构建和部署ML系统在许多方面是传统软件工程的延伸，但有重要的差异需要强调。

    开发运维（也称为DevOps）是指软件团队用来缩短开发周期、通过持续集成/部署（CI/CD）保持质量并可靠运营服务的最佳实践。

    MLOps借鉴了这些原则，但扩展了它们以处理ML的独特挑战。让我们比较一下ML生产管道与标准软件项目的不同之处：

    ### 实验性开发 vs. 确定性开发

    编写传统软件通常是一个确定性过程，但在ML开发中，过程是高度实验性和数据驱动的。

    你尝试多种算法、特征和超参数来找到最有效的方法。

    代码本身不一定有错误，但模型可能简单地不够准确而无法部署。

    因此，跟踪实验、处理随机结果（由于随机初始化等）以确保在后续阶段需要时结果的可重现性是一个额外的挑战。

    ![ML系统中的实验迭代](https://www.dailydoseofds.com/content/images/2025/07/image-100.png)
    *ML系统中的实验迭代不仅限于代码*

    模型训练的迭代性质意味着版本控制数据和模型（不仅仅是代码）变得至关重要，这是传统DevOps默认不涵盖的。

    ### 测试复杂性

    在标准软件中，你编写单元测试和集成测试来验证功能。

    ML系统需要额外的测试：我们不仅需要对数据预处理步骤和任何其他代码进行单元测试，还需要验证数据质量并测试训练模型的性能。

    例如，我们可能想要测试模型在保留数据集上的准确性是否达到某个阈值，或者是否没有数据泄漏。

    💡

    数据泄漏发生在训练数据集之外的信息无意中被用来训练模型时，导致测试期间过于乐观的性能估计。如果来自未来的数据或在预测时不可用的信息在训练期间被使用，就会发生这种情况。它破坏了模型对新的、未见过的数据的泛化能力，导致现实世界性能不佳。必须特别小心确保只使用相关的历史数据来训练模型，特别是在时间敏感或序列数据上下文中。

    还有训练/服务偏差的概念，其中我们必须确保在生产中输入模型的数据与训练中使用的数据一致（特征分布等）。这需要在生产中进行仔细的验证和监控。

    ![训练服务偏差](https://www.dailydoseofds.com/content/images/2025/07/image-103.png)

    简而言之，ML系统的"正确性"不仅关于代码逻辑，还关于统计性能，这使得测试更加复杂。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 部署和更新

    在传统软件CI/CD中，部署应用程序的新版本是通过暂存到生产的相对直接的代码推送。

    在ML中，系统的"新版本"通常意味着用新数据或更新的参数重新训练模型，这本身就是一个多步骤管道。

    部署不仅仅是发布二进制文件；它可能涉及一个完整的自动化管道，定期重新训练和部署模型。例如，每晚用最新数据重新训练模型然后提供服务。

    自动化这个管道是MLOps的主要焦点，它除了CI/CD之外还引入了持续训练（CT）的概念。

    👉

    持续训练意味着系统可以在新数据到达或性能下降时触发重新训练，关闭数据和部署之间的循环。

    ### 生产性能退化

    在软件服务中，性能问题通常来自代码效率低下或基础设施问题。

    在ML服务中，即使代码完全高效，模型的预测性能也可能由于数据漂移（输入数据分布的变化）或概念漂移（模型试图学习的潜在关系的变化）而随时间退化。例如，用户偏好的转变或欺诈者适应其行为。

    ![模型性能退化](https://www.dailydoseofds.com/content/images/2025/07/image-102.png)

    因此，生产ML系统不仅需要监控正常运行时间，还需要监控模型随时间的质量。

    我们经常必须跟踪模型预测准确性、输出分布、数据漂移指标等指标，并在它们偏离预期范围时设置警报或触发器。

    这是DevOps软件通常不包括的"运维"的新维度。

    💡

    这里快速说明一下。我们理解这篇文章理论性较强，但这对于为您奠定基础很重要。我们将涵盖所有实现，但在此之前，正确设置基础很重要，这样当我们涵盖实现时，您可以连接点并轻松理解一切。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 生命周期复杂性

    传统软件系统的生命周期大多是线性的，与ML系统相比迭代较少。

    ML系统的生命周期是完全循环的。部署后，你经常回到数据收集和模型改进。

    例如，生产中的用户交互可以生成新的标记示例（基于反馈），这些示例输入到下一个训练迭代中。这引入了一个反馈循环，其中系统持续演进。

    我们在最近关于从零开始训练LLM的4个阶段的新闻通讯问题中看到了类似的东西，特别是在偏好微调阶段：

    ![LLM训练4个阶段](https://www.dailydoseofds.com/content/images/2025/07/4-llm-training-stages.gif)

    你一定在ChatGPT上看过这个界面，它问：你更喜欢哪个回答？

    ![ChatGPT偏好选择](https://substackcdn.com/image/fetch/$s_!grMf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F578cc71c-3ca4-4e49-8a25-428f48c7f11a_680x570.png)

    这不仅仅是为了反馈，而是有价值的人类偏好数据。

    OpenAI使用这些数据通过偏好微调来微调他们的模型。

    回到主题...

    从运维角度来看，处理这个循环意味着平台应该支持记录预测、收集结果、更新数据集和触发新的训练运行，通常被称为ML飞轮或数据飞轮。

    高级MLOps设置使这个循环能够部分或完全自动化，通常在机器学习持续交付（也称为CD4ML）的术语下，而更简单的设置可能通过定期模型刷新手动完成。

    ![机器学习持续交付](https://www.dailydoseofds.com/content/images/2025/07/image-91.png)
    *机器学习持续交付（来源：martinfowler.com）*

    一般来说，可以将MLOps视为DevOps + 数据 + 模型。

    在这个阶段需要注意的一个关键点是，部署ML注入的应用程序不仅仅是一次性交接（"这是一个模型，去部署它"）；它是一个需要结合多个领域技能的持续过程。

    MLOps流程确保在研究环境中工作的ML模型可以顺利过渡到生产环境并在其生命周期内得到维护。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 关键差异一览

    以下是总结传统软件和ML系统之间差异的一些要点：

    - **主要产物**：
        - 传统软件交付代码二进制文件
        - MLOps交付代码、训练的模型产物和数据管道。
    - **版本控制单元**：
        - 软件版本控制代码（通过Git等）。
        - MLOps必须版本控制代码、数据和模型（我们将在系列中讨论这一点）。
    - **故障模式**：
        - 软件由于错误而失败。
        - ML可能由于错误和/或模型质量问题而失败（模型逐渐变得不准确）。
    - **测试**：
        - 软件测试专注于功能正确性和性能。
        - ML测试在某些情况下添加数据验证、模型评估和偏见/公平性检查。
    - **监控**：
        - 软件监控服务健康（延迟、错误、CPU/内存）。
        - ML监控这些加上模型预测（分布偏移、通过定期真实情况检查在真实数据上的准确性等）。

    理解这些差异为设计有效的ML生产系统奠定了基础。

    进一步，让我们看看有效的MLOps旨在解决的一些关键关注点。

    ---
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 生产ML中的系统级关注点

    当将ML模型从实验室过渡到生产时，几个系统级关注点会浮现。

    这些是系统行为和环境的方面，除了模型准确性之外，还可能显著影响性能和可靠性。我们已经涉及了一些（延迟、数据漂移等），但让我们更深入地探讨关键关注点：

    ### 延迟和吞吐量

    延迟是指系统在接收输入后产生预测或响应所需的时间。

    吞吐量是指系统每单位时间可以处理多少预测/请求（例如，每秒查询数）。

    ![延迟和吞吐量](https://www.dailydoseofds.com/content/images/2025/07/image-105.png)

    在许多生产环境中，延迟和吞吐量与模型的原始准确性一样重要，甚至更重要。

    ![性能权衡](https://www.dailydoseofds.com/content/images/2023/09/image-134.png)

    我们在学习如何在生产前压缩机器学习模型时也看到了这一点：

    [Model Compression: A Critical Step Towards Efficient Machine Learning](https://www.dailydoseofds.com/model-compression-a-critical-step-towards-efficient-machine-learning/)

    对于面向用户的应用程序，延迟对用户体验至关重要。

    例如，Chip Huyen指出，如果自动完成模型建议下一个字符的时间比你打字的时间更长，那么它就是无用的。

    用户期望近乎即时的响应；每100毫秒的延迟都可以显著减少参与度。

    亚马逊著名地发现每100毫秒的额外延迟使他们损失1%的销售额。

    ![亚马逊延迟影响](https://www.dailydoseofds.com/content/images/2025/07/Screenshot-2025-07-26-at-4.04.31---PM.png)

    当你有许多用户或高容量任务时，吞吐量变得至关重要。例如，欺诈检测模型可能需要每秒评分数千笔交易。如果模型太慢，它就会成为瓶颈，导致积压或需要昂贵的并行化。

    这些关注点导致工程决策，例如：

    - 如果需要，简化模型或使用较小的模型架构（为速度牺牲一些准确性，就像模型压缩所做的那样）。
    - 使用更快的硬件或用更多实例扩展（这增加了基础设施成本）。
    - 利用优化，如模型量化和蒸馏，使推理更快。
    - 选择适当的部署架构：例如，在线推理（请求-响应）vs. 批处理。如果延迟不严格（比如，每天更新一次推荐），我们可以离线进行批量推理以减轻实时压力。如果延迟至关重要，我们可能使用低开销的在线微服务。这就是实时和批量推理的区别：

    ![实时vs批量推理](https://www.dailydoseofds.com/content/images/2025/07/Screen-Recording-2025-07-26-at-4.11.14---PM-2.gif)

    - 如果适用，为重复查询实施缓存策略。

    因此，在设计中始终考虑延迟和吞吐量很重要。一个稍微不那么准确但快十倍的模型可能比超准确但缓慢的模型更好地服务于业务（取决于应用程序）。

    👉

    有趣的注意：当一次处理一个查询时，更高的延迟意味着更低的吞吐量。然而，当批量处理查询时，更高的延迟也可能意味着更高的吞吐量。

    ## 🔴 Real-time Inference (实时推理)

    * **流程**：用户发出请求 → 模型立即处理 → 返回响应。
    * **特点**：

      1. **即时响应**：请求来了就立刻预测，适合对时效性要求高的场景（比如聊天机器人、实时推荐、金融风控）。
      2. **模型常驻内存**：模型必须一直加载在内存中，以保证随时处理请求。
      3. **可靠性要求高**：如果服务宕机或者延迟过高，会严重影响用户体验。

    ✅ 优点：响应快，用户体验好。
    ❌ 缺点：资源消耗大（模型一直在内存里），稳定性挑战更大。

    ---

    ## 🔵 Batch Inference (批量推理)

    * **流程**：用户的请求先存入数据库 → 系统在固定时间批量取出请求 → 模型集中处理 → 批量返回结果。
    * **特点**：

      1. **有延迟**：不是立刻处理，而是等到下一个批处理周期再处理（比如每天、每小时一次）。
      2. **模型按需加载**：只在批处理时加载模型，节省计算资源。
      3. **容错能力更强**：如果某一批出错，可以重试，不会像实时服务那样导致整体瘫痪。

    ✅ 优点：更节省资源、稳定性好，适合大规模请求。
    ❌ 缺点：延迟高，不能满足实时需求。

    ---

    ## 📌 对比总结

    * **实时推理**：速度快、用户体验好，但成本高、要求高。
    * **批量推理**：延迟高，但资源利用率高，更适合离线场景。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 数据和概念漂移

    一旦ML模型被部署，由于漂移，其性能可能随时间恶化。

    ![数据漂移概念](https://substackcdn.com/image/fetch/$s_!lUty!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7850ce7-2f65-4f4b-9fb1-9d72ebda2403_1000x408.jpeg)

    数据漂移通常是指输入数据分布的变化，而概念漂移是指输入和输出之间关系的变化（模型试图预测的潜在概念）。

    简单来说，世界在变化，你的模型可能变得不那么相关。

    这里有一个可视化数据和概念漂移的类比：

    ![数据和概念漂移类比](https://www.dailydoseofds.com/content/images/2025/07/image-96.png)
    *来源：kDimensions*

    数据漂移的一个定义是"生产数据与在生产中部署之前用于测试和验证模型的数据的变化"。如果传入数据的特征开始与训练数据不同，模型可能在其学习边界之外运行。

    ![数据漂移示例](https://www.dailydoseofds.com/content/images/2025/07/image-94.png)
    *数据漂移*

    例如，在夏季照片上训练的图像识别模型如果突然用户发送的是冬季照片，可能会遇到困难。

    时间也是这里的一个关键因素，因为随着时间的推移，数据可能自然漂移。训练数据收集时间与当前时刻之间的差距可能引入显著的漂移。

    类似地，为了详细说明概念漂移，输入分布保持相同，但从输入到输出的映射发生变化。如下所示：

    ![概念漂移示例](https://www.dailydoseofds.com/content/images/2025/07/image-97.png)
    *概念漂移*

    一个经典的例子可能是预测客户流失的模型。也许在疫情之前，某些用户行为表明流失，但疫情之后，这些信号发生了变化。模型的"可能性"概念发生了漂移，因为世界发生了变化。

    #### 漂移的影响

    如果漂移未被识别或未得到解决，模型的预测可能变得错误且可能有害。

    有时影响是轻微的（推荐稍微不那么相关的内容），但也可能是重大的（医疗诊断模型中的漂移可能导致误诊）。

    未能检测到漂移意味着预测将出错，基于它们的业务决策可能产生负面影响。

    漂移持续的时间越长，情况可能变得越糟。在受监管的行业中，专业人员通常需要监控漂移作为模型风险管理的一部分。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### 处理漂移

    这是MLOps的一个重要部分：

    - **监控**：我们设置监控来检测漂移。这可以包括跟踪输入的统计属性（特征分布、均值、类别频率等），看看它们是否与训练集显著偏移。有统计测试和距离度量来量化漂移。
    - **阈值和警报**：我们为可接受的漂移定义阈值。例如，如果输入数据中"年龄"的分布偏移超过X（PSI或K-S测试p值），则发出警报。许多MLOps平台允许设置这些触发器。
    - **定期重新训练**：一个直接的策略是定期（每周、每月）用最新数据重新训练模型，假设最新数据分布代表当前现实。
    - **在线学习**：在某些情况下，模型可以在生产中随着新数据的到来而增量更新（使用在线学习算法）。这可以快速解决漂移，但要小心，如果数据有噪声或有短期波动，在线更新实际上可能降低性能。
    - **人在回路和后备计划**：对于关键系统，如果检测到漂移，我们可能有协议回退到更简单的系统或让人类审查。

    值得注意的是，并非所有漂移都是相等的。特征漂移（输入分布变化）如果模型能够泛化，可能不总是损害性能，但标签漂移或概念漂移直接损害性能。此外，漂移可能是突然的（制度变化）或渐进的（缓慢趋势）。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 反馈循环

    某些ML系统的一个迷人且棘手的方面是它们可以影响自己未来的输入数据。这被称为反馈循环。

    ![反馈循环概念](https://www.dailydoseofds.com/content/images/2025/07/image-109.png)

    当模型的预测或输出影响环境或用户行为时就会发生这种情况，然后生成模型稍后会看到的新数据。

    让我们考虑一个这种情况的微妙例子。

    ![ChatGPT偏好选择示例](https://substackcdn.com/image/fetch/$s_!grMf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F578cc71c-3ca4-4e49-8a25-428f48c7f11a_680x570.png)

    如上图所示，ChatGPT的两个响应呈现给用户。但想象一个场景，两个输出都不够好，都没有完全解决用户的需求或有效解决问题。

    尽管如此，用户被迫选择一个作为"首选"（因为系统要求他们在两个选项之间选择）。

    这种选择，即使它只是"不那么糟糕"的响应，也成为将用于进一步微调或强化模型行为的数据集的一部分。

    大规模地，这创建了一个反馈循环：

    - 次优响应被持续选择出于必要。
    - 模型将这些选择解释为积极信号。
    - 下一代模型在这个有缺陷的信号上得到训练。
    - 质量停滞——甚至退化——尽管持续训练。

    这是通过被误解为认可的用户交互强化低质量模式的经典例子。

    这表明反馈循环可能偏向数据、强化某些行为，或者如果不加管理甚至导致不稳定。

    作为另一个例子，假设你有一个新闻文章推荐模型，它学习用户喜欢什么。如果模型开始推荐更多某种类型的文章，用户会点击这些而不会看到其他类型的文章。

    随着时间的推移，收集的数据将偏向于所显示的内容。所以模型可能永远不会学到一些用户会喜欢其他内容，因为他们从未看到过（一种选择偏见的形式）。

    本质上，模型的输出缩小了它稍后获得的数据，创造了一个自我强化的模式。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### 管理反馈循环

    - **探索-利用权衡**：在推荐或个性化中，有意注入一些随机性或新颖性（探索），而不是总是利用模型当前的最佳猜测。这有助于收集关于替代方案的无偏数据。
    - **训练中的数据去偏**：考虑模型自身行为引入的偏见。有技术如反事实分析或因果推理来调整训练数据是在先前模型策略影响下收集的事实。
    - **部署前的模拟**：对于与世界交互的系统（如定价模型），运行模拟或A/B测试可以在完全推出之前揭示反馈循环效应。

    反馈循环是一个高级主题，接近因果ML和强化学习的领域。我们现在不会深入因果数学（如果你想的话，我们已经做了一个关于这个的速成课程），但保持这种直觉：当你的模型输出影响未来输入时，要谨慎。

    不仅监控模型的性能，还监控可以指示反馈循环的次要指标（如显示内容的多样性，或输入随时间的分布）。

    [A Crash Course on Causality – Part 1](https://www.dailydoseofds.com/a-crash-course-on-causality-part-1/)

    有时最安全的方法是打破循环。例如，定期独立于模型的行为收集新数据以重新定位模型。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 可重现性

    顾名思义，可重现性意味着你（或其他人，或一个过程）可以可靠地重新创建模型或管道的结果。无论是训练模型以获得相同的准确性，还是在稍后的时间给定相同输入生成相同的预测。

    ![可重现性概念](https://www.dailydoseofds.com/content/images/2025/07/image-106.png)

    可重现性是构建可靠机器学习的关键方面之一。它是团队成员之间协作以及在不同时间点或环境中比较模型的基础。

    想象一下：某些东西在一个系统上工作但在另一个系统上不工作，这反映了糟糕的可重现性实践。

    ![协作问题](https://www.dailydoseofds.com/content/images/2025/07/image-107.png)

    然而，明确的可重现性实践确保结果可以被他人复制和验证，这提高了我们工作的整体可信度。

    为什么这在生产中很重要？

    - **调试**：当出现问题时（很可能会出现），你需要能够追踪原因。
    - **环境间的一致性**：你在工作站或笔记本中开发模型，但在生产中，它在服务器上运行。如果环境不同，模型可能表现不同。我们通过环境管理（例如，Docker容器）、在训练期间固定随机种子和仔细跟踪版本来处理这个问题。
    - **协作**：如果有一个工程师和科学家团队，不确保可重现性意味着结果不能轻易共享和构建。

    ![缺乏可重现性的协作问题](https://www.dailydoseofds.com/content/images/2025/07/image-93.png)
    *由于缺乏可重现性导致的协作问题*

    实现可重现性需要纪律和工具：

    - **版本控制一切**：代码、数据和模型产物。这样，生产中的任何模型都可以追溯："模型v1.3是用代码的提交X、数据集版本Y和随机种子Z训练的。"

    ![不版本控制数据的情况](https://www.dailydoseofds.com/content/images/2025/07/image-92.png)
    *如果我们不版本控制数据的情况*

    - **容器化**：环境（操作系统、库）可以通过Docker或类似工具捕获，这样1月份的训练与6月份在新机器上的重新训练不会因为库更新等原因引入差异。

    ![容器化概念](https://www.dailydoseofds.com/content/images/2025/07/image-108.png)

    - **数据和模型测试**：这可能听起来奇怪，但为你的数据处理编写测试（例如，数据管道在样本输入上是否产生相同输出）甚至为模型训练编写测试（例如，在小的已知数据集上训练并验证模型达到预期性能）有助于捕获不可重现性。如果代码更改导致先前通过的测试失败（也许现在指标不同），你知道有些东西改变了。

    值得注意的是，绝对可重现性（逐位）有时可能不必要地严格。通常，我们关心在容差范围内重现性能或行为。但作为基线，你应该能够重新运行整个管道并获得可比较的结果。如果不能，你就没有控制，而生产系统需要控制。

    现在我们已经建立了基本的思维过程，接下来让我们走过ML系统的生命周期，从数据摄取和训练到部署和监控。

    ---
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 机器学习系统生命周期

    当将ML模型部署到生产时，从端到端生命周期或管道的角度思考是有用的。不同的来源可能以各种方式分解这个过程，但它们通常包括类似的组件。

    一个有用的观点是Martin Fowler的机器学习持续交付（CD4ML）概念，它概述了一个循环过程：从开发模型，到测试它，部署它，并通过反馈循环监控它回到开发。

    ![机器学习持续交付](https://www.dailydoseofds.com/content/images/2025/07/image-91.png)
    *机器学习持续交付（来源：martinfowler.com）*

    另一个常见的分解是：范围界定 → 数据 → 建模 → 部署和监控（根据需要循环回去）。

    ![简化的ML项目生命周期](https://www.dailydoseofds.com/content/images/2025/07/mlcycle.svg)
    *简化的ML项目生命周期*

    让我们快速浏览ML项目生命周期中的典型阶段序列：

    - **项目范围界定**：决定基于ML的解决方案是否适合问题并定义成功标准。这发生在任何模型工作之前，确保你需要ML解决方案以及要求是什么。

    - **数据处理**
        - **数据摄取和收集**：从各种来源收集所需的数据。这可能涉及拉取日志、查询数据库、使用第三方API，甚至设置数据收集过程（传感器、用户反馈等）。
        - **数据准备（ETL）和特征工程**：将原始数据清理和预处理成适合建模的形式。这包括处理缺失值、标准化、从多个来源连接数据、创建新特征等。输出通常是一个策划的训练数据集。

    - **建模**
        - **模型训练和实验**：使用准备好的数据训练一个或多个ML模型。这个阶段通常是迭代和实验性的。我们可能需要尝试不同的算法或架构，调整超参数，并在验证集上评估。跟踪哪个模型/版本根据成功标准（例如，准确性、F1分数等）表现最好。
        - **模型评估和验证**：彻底评估所选模型的性能。确保它满足业务要求并且没有明显的问题（如过拟合或偏见）。这可能涉及在保留数据上进行额外测试，甚至与真实用户进行试验测试。
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    - **部署**：将训练好的模型部署到生产环境中。这可能意味着通过API服务公开它，将其嵌入到应用程序中，或部署到边缘设备。部署还包括设置必要的推理基础设施，即运行模型进行预测的计算环境。

    - **监控和可观察性**：一旦模型上线，我们需要持续监控它。跟踪其运营指标（延迟、吞吐量、错误）以及其在真实数据上的预测性能：
        - 输入的分布是否仍然像训练时一样？
        - 根据反馈估计，它可能多久错一次？
        - 是否有漂移的迹象？

    👉

    如果你需要复习，延迟是指从接收查询到返回结果所需的时间。吞吐量是指在特定时间段内（比如1秒）处理多少查询。

    - **维护和持续改进**：基于监控和新需求，更新模型或系统。这可能涉及收集新数据，也许模型在新场景中出错。因此，收集这些示例并重新训练，调整特征，或者如果需要甚至制定新模型。这循环回到早期阶段。

    在成熟的MLOps设置中，这个循环可能部分/完全自动化，或者可能在定期计划中有人工分析在循环中。

    关键是，这些步骤不是严格线性的。这是一个迭代循环，如CD4ML和简化图表所示。部署后，你可能通过监控发现模型的性能正在下降，这促使你回到数据收集或模型开发来改进它。

    这个持续的循环就是我们所说的ML生命周期或"ML飞轮"。许多公司旨在缩短这个循环并使其尽可能自动化，以持续向模型交付改进（类似于软件中的持续交付）。

    我们将在这个速成课程系列的后续部分更深入地重新审视ML生命周期。

    ---
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 结论

    通过这篇基础文章，我们迈出了理解MLOps真正包含什么的第一步，超越了模型本身，进入了它所生存的更广泛系统。

    我们确立了为什么MLOps很重要，它如何与传统DevOps不同，以及塑造生产就绪ML的各种系统级关注点。我们还走过了ML系统的典型生命周期，强调其循环和演进的性质。

    本章旨在将读者的思维从将ML视为以模型为中心的练习转变为系统工程学科，其中可重现性、自动化和监控是一等公民。

    随着我们在这个系列中前进，即将到来的章节将深入探讨：

    - ML生命周期的深入探讨
    - 来自行业的现实世界案例研究
    - 版本控制和可重现性的实践工具
    - 性能监控
    - LLMOps的特殊考虑
    - 端到端项目

    在这里，我们还想强调一个非常重要的方面。当涉及到MLOps/LLMOps中的实际工具和实践工作时，对于不同的用例，事情可能变得相当具体和可变。在一个堆栈中可能合适的东西在另一个堆栈中可能不合适。

    因此，理论基础在实现中基本保持一致，使它们成为必须涵盖的内容。虽然这个系列肯定会在必要时包括实践模拟，但正是潜在的理论将作为骨干。

    工具可能演进或不同，但构建健壮、生产就绪ML系统的核心原则和方法会持续更长时间。因此，虽然我们会在需要时模拟设置，但这个系列将大量倾向于理论方面，以提供持久和适应性的理解。

    因此，展望未来，我们将更多地关注ML系统的各个方面，因为如前所述，我们的主要目标是弥合实验和生产之间的知识和理解差距，帮助你培养成熟的思维方式，并为你提供构建持久的ML/AI系统所需的通用和适应性框架。

    一如既往，感谢阅读！

    ### 📚 系列预告

    在接下来的章节中，我们将涵盖：

    1. **ML生命周期深入探讨** - 详细分析每个阶段的最佳实践
    2. **版本控制和实验跟踪** - 管理代码、数据和模型版本的工具和技术
    3. **数据管道和特征存储** - 构建可靠的数据基础设施
    4. **模型部署策略** - 从批处理到实时推理的各种部署模式
    5. **监控和可观察性** - 检测漂移、性能退化和系统健康
    6. **LLMOps特殊考虑** - 大语言模型的独特挑战和解决方案
    7. **端到端案例研究** - 完整的MLOps项目实现

    每一章都将结合理论基础和实践示例，帮助你构建生产级ML系统的完整知识体系。

    ---

    **下一章预告：深入ML生命周期 - 从数据到部署的完整流程**
    """
    )
    return


if __name__ == "__main__":
    app.run()
