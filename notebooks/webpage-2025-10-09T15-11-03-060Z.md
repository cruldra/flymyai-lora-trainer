### [​**Build a Reasoning LLM using GRPO**​](https://lightning.ai/lightning-purchase-test/studios/build-a-reasoning-llm-from-scratch-using-grpo?utm_campaign=akshay&utm_medium=newsletter&ref=dailydoseofds.com)

Group Relative Policy Optimization is a reinforcement learning method that fine-tunes LLMs for math and reasoning tasks using deterministic reward functions, eliminating the need for labeled data.

Here's a brief overview of GRPO:

![](https://substackcdn.com/image/fetch/$s_!Wu3x!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf3c0aa5-dd9f-4e3a-a77c-a99297fa4ee3_1080x1029.gif)

-   Start with a dataset and add a reasoning-focused system prompt (e.g., “Think step by step…”).
-   The LLM generates multiple candidate responses using a sampling engine.
-   Each response is assigned rewards, which are aggregated to produce a score for every generated response.
-   A GRPO loss function uses these rewards to calculate gradients, backpropagation updates the LLM, and the model improves its reasoning ability over time.

Let’s dive into the code to see how we can use GRPO to turn any model into a reasoning powerhouse without any labeled data or human intervention.

We’ll use:

-   UnslothAI for efficient fine-tuning.
-   HuggingFace TRL to apply GRPO.

The code is available here: [​**Build a reasoning LLM from scratch using GRPO**​](https://lightning.ai/lightning-purchase-test/studios/build-a-reasoning-llm-from-scratch-using-grpo?utm_campaign=akshay&utm_medium=newsletter&ref=dailydoseofds.com). You can run it without any installations by reproducing our environment below:

![](https://substackcdn.com/image/fetch/$s_!9kUF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750cccf0-bca6-4be0-a474-ea72436f3927_980x628.png)

Let’s begin!

---

#### Load the model

We start by loading Qwen3-4B-Base and its tokenizer using Unsloth.

You can use any other open-weight LLM here.

![](https://substackcdn.com/image/fetch/$s_!TNb8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18690195-724f-4793-a5d3-b332a4db8bb2_1200x816.png)

#### Define LoRA config

We'll use LoRA to avoid fine-tuning the entire model weights. In this code, we use Unsloth's PEFT by specifying:

![](https://substackcdn.com/image/fetch/$s_!806u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c1fe46-f058-4d74-934f-abf22c898f95_1200x836.png)

-   The model
-   LoRA low-rank (r)
-   Modules for fine-tuning, etc.

#### Create the dataset

We load the Open R1 Math dataset (a math problem dataset) and format it for reasoning.

![](https://substackcdn.com/image/fetch/$s_!IKlO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b29df44-0bea-43eb-85fd-048354d3e9f2_1200x900.png)

Each sample includes:

-   A system prompt enforcing structured reasoning
-   A question from the dataset
-   The answer in the required format

#### Define reward functions

In GRPO, we use deterministic functions to validate the response and assign a reward. No manual labelling required!

The reward functions:

![](https://substackcdn.com/image/fetch/$s_!KW5A!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e468db3-a49e-4cb8-a709-97530f34c0a0_1200x1195.png)

-   Match format exactly
-   Match format approximately
-   Check the answer
-   Check numbers

#### Use GRPO and start training

Now that we have the dataset and reward functions ready, it's time to apply GRPO.

HuggingFace TRL provides everything we described in the GRPO diagram, out of the box, in the form of the GRPOConfig and GRPOTrainer.

![](https://substackcdn.com/image/fetch/$s_!w8UN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536dab32-fadb-4be3-9978-0d526c314600_1200x914.png)

#### Comparison

We can see how GRPO turned a base model into a reasoning powerhouse:

![](https://substackcdn.com/image/fetch/$s_!DIHV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80a78edb-f811-4289-a50c-28f8b7fbfdee_1963x1197.png)

Before we conclude, let’s address an important question:

When should you use reinforcement fine-tuning (RFT) versus supervised fine-tuning (SFT)?

We created this diagram to provide an answer:

![](https://substackcdn.com/image/fetch/$s_!ESV_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71c0c145-dc7b-4193-9b97-fe01948fb30b_1200x772.png)

Finally, we'll leave you with an overview of the GRPO process.

![](https://substackcdn.com/image/fetch/$s_!Wu3x!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf3c0aa5-dd9f-4e3a-a77c-a99297fa4ee3_1080x1029.gif)

Let us know what other techniques you have used for fine-tuning LLMs.

The code is available here: [​**Build a reasoning LLM from scratch using GRPO**​](https://lightning.ai/lightning-purchase-test/studios/build-a-reasoning-llm-from-scratch-using-grpo?utm_campaign=akshay&utm_medium=newsletter&ref=dailydoseofds.com). You can run it without any installations by reproducing our environment below:

![](https://substackcdn.com/image/fetch/$s_!9kUF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750cccf0-bca6-4be0-a474-ea72436f3927_980x628.png)

Thanks for reading!