import marimo

__generated_with = "0.16.5"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # å®Œæ•´çš„MLOpsè“å›¾ï¼šæ•°æ®å’Œç®¡é“å·¥ç¨‹â€”ç¬¬Béƒ¨åˆ†ï¼ˆå«å®ç°ï¼‰

    MLOpså’ŒLLMOpsé€Ÿæˆè¯¾ç¨‹â€”ç¬¬6éƒ¨åˆ†

    ## å›é¡¾

    åœ¨è¿™ä¸ªMLOpså’ŒLLMOpsé€Ÿæˆè¯¾ç¨‹çš„ç¬¬5éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ‰€æœ‰æœºå™¨å­¦ä¹ ç³»ç»ŸåŸºçŸ³çš„åŸºç¡€ï¼šæ•°æ®å’Œç®¡é“ã€‚

    ![æ•°æ®ç®¡é“åŸºç¡€](https://www.dailydoseofds.com/content/images/2025/08/image-143.png)

    æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®æ™¯è§‚ï¼Œå¹¶æ¢ç´¢äº†ä¸ºä»€ä¹ˆæ•°æ®åŠå…¶å¤„ç†åœ¨MLOpsä¸–ç•Œä¸­å¦‚æ­¤é‡è¦ã€‚

    ![æ•°æ®é‡è¦æ€§](https://www.dailydoseofds.com/content/images/2025/08/image-144.png)

    ç„¶åæˆ‘ä»¬ç»§ç»­æ¢ç´¢ç”Ÿäº§MLç³»ç»Ÿä¸­é‡è¦çš„ä¸åŒç±»å‹æ•°æ®æºï¼Œå¦‚ç”¨æˆ·è¾“å…¥æ•°æ®ã€ç³»ç»Ÿæ—¥å¿—ã€å†…éƒ¨æ•°æ®åº“å’Œç¬¬ä¸‰æ–¹æºã€‚

    ![æ•°æ®æºç±»å‹](https://www.dailydoseofds.com/content/images/2025/08/image-145.png)

    æˆ‘ä»¬è¿˜è®¨è®ºäº†æ•°æ®æºçš„å„ç§åˆ†ç±»ï¼Œå¦‚æ–‡æœ¬vsäºŒè¿›åˆ¶å’Œè¡Œä¸»åºvsåˆ—ä¸»åºæ ¼å¼ã€‚æˆ‘ä»¬ç ”ç©¶äº†å®ƒä»¬çš„å·®å¼‚ä»¥åŠå„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚

    ![æ•°æ®æ ¼å¼å¯¹æ¯”](https://www.dailydoseofds.com/content/images/2025/08/image-146.png)

    æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›å…¥äº†ç¬¬5éƒ¨åˆ†çš„æ ¸å¿ƒæ¦‚å¿µç„¦ç‚¹ï¼Œæ¶µç›–äº†ETLå’ŒELTç®¡é“ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§æ–¹æ³•ï¼Œå¹¶æ¸…æ¥šåœ°ç†è§£äº†å®ƒä»¬çš„å·®å¼‚ï¼ŒåŒæ—¶ä»ç„¶ç›¸äº’è¡¥å……ã€‚

    ![ETL vs ELT](https://www.dailydoseofds.com/content/images/2025/08/image-147.png)

    ä»é‚£é‡Œï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®é™…æ“ä½œã€‚æˆ‘ä»¬è¯¦ç»†æ¼”ç»ƒäº†æ··åˆETL/ELTç®¡é“çš„æ¨¡æ‹Ÿï¼Œæ¨¡æ‹Ÿäº†å¤šä¸ªæ•°æ®æºä»¥åŠæå–ã€éªŒè¯ã€è½¬æ¢å’ŒåŠ è½½é˜¶æ®µã€‚

    ![å®é™…å®ç°](https://www.dailydoseofds.com/content/images/2025/08/image-148.png)

    å¦‚æœä½ è¿˜æ²¡æœ‰æ¢ç´¢ç¬¬5éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®å…ˆé˜…è¯»å®ƒï¼Œå› ä¸ºå®ƒå¥ å®šäº†æ¦‚å¿µæ¡†æ¶å’Œå®ç°ç†è§£ï¼Œè¿™å°†å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£æˆ‘ä»¬å³å°†æ·±å…¥çš„å†…å®¹ã€‚

    åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­MLç³»ç»Ÿä¸­çš„æ•°æ®å¤„ç†å’Œç®¡ç†ï¼Œæ·±å…¥æ¢è®¨æ¦‚å¿µå’Œå®é™…å®ç°ã€‚

    æˆ‘ä»¬å°†æ¶µç›–è“å›¾ï¼Œå­¦ä¹ å¦‚ä½•ä¸“é—¨ä¸ºæœºå™¨å­¦ä¹ è®¾è®¡å’Œé‡‡æ ·æ•°æ®ï¼Œå¹¶æ·±å…¥æ¢è®¨æ•°æ®æ³„æ¼è¿™ä¸€å±é™©é™·é˜±ã€‚ç„¶åæˆ‘ä»¬å°†åœ¨ç‰¹å¾å­˜å‚¨ä¸­é›†ä¸­æˆ‘ä»¬çš„å·¥ä½œï¼Œè¿™æ˜¯ç¡®ä¿è®­ç»ƒå’ŒæœåŠ¡ä¹‹é—´ä¸€è‡´æ€§çš„ä¸­å¿ƒã€‚

    ä¸€å¦‚æ—¢å¾€ï¼Œæ¯ä¸ªæ¦‚å¿µéƒ½å°†å¾—åˆ°å…·ä½“ç¤ºä¾‹ã€æ¼”ç»ƒå’Œå®ç”¨æŠ€å·§çš„æ”¯æŒï¼Œå¸®åŠ©ä½ æŒæ¡æƒ³æ³•å’Œå®ç°ã€‚

    è®©æˆ‘ä»¬å¼€å§‹å§ï¼

    ---

    ## é‡‡æ ·ç­–ç•¥

    é‡‡æ ·æ˜¯ä»æ›´å¤§çš„æ•°æ®æ± ä¸­é€‰æ‹©æ•°æ®å­é›†çš„å®è·µã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé‡‡æ ·å‘ç”Ÿåœ¨å·¥ä½œæµçš„è®¸å¤šé˜¶æ®µï¼š

    - **é€‰æ‹©è¦æ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®**æ¥æ„å»ºä½ çš„æ•°æ®é›†
    - **é€‰æ‹©å¯ç”¨æ•°æ®çš„å­é›†**ç”¨äºæ ‡æ³¨æˆ–è®­ç»ƒï¼ˆç‰¹åˆ«æ˜¯å½“ä½ æ‹¥æœ‰çš„æ•°æ®è¶…è¿‡å¯è¡Œä½¿ç”¨é‡æ—¶ï¼‰
    - **å°†æ•°æ®åˆ†å‰²**ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    - **è®­ç»ƒæœŸé—´çš„æ•°æ®é‡‡æ ·**ç”¨äºæ¯ä¸ªæ‰¹æ¬¡ï¼ˆä¾‹å¦‚ï¼Œåœ¨éšæœºæ¢¯åº¦ä¸‹é™ä¸­ï¼‰
    - **ç›‘æ§ä¸­çš„é‡‡æ ·**ï¼ˆä¾‹å¦‚ï¼Œåªè®°å½•ä¸€éƒ¨åˆ†é¢„æµ‹ç”¨äºåˆ†æï¼‰

    ![é‡‡æ ·åº”ç”¨åœºæ™¯](https://www.dailydoseofds.com/content/images/2025/08/image-149.png)

    å¯¹å¤§å¤šæ•°äººæ¥è¯´ï¼Œå¯¹é‡‡æ ·çš„å¸¸è§æ¥è§¦å¯èƒ½æ˜¯è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ†å‰²ã€‚ä½†é‡è¦çš„æ˜¯è¦æ„è¯†åˆ°ï¼Œä½ å¦‚ä½•é‡‡æ ·å¯èƒ½ä¼šå¼•å…¥åå·®å¹¶å½±å“æ¨¡å‹æ€§èƒ½ã€‚

    ### ä¸ºä»€ä¹ˆé‡‡æ ·å¾ˆé‡è¦ï¼Ÿ

    åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸èƒ½æˆ–ä¸ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ã€‚ä¹Ÿè®¸æ•°æ®å¤ªå¤§ï¼ˆåœ¨ä¸‡äº¿æ¡è®°å½•ä¸Šè®­ç»ƒä¸å¯è¡Œï¼‰ï¼Œæˆ–è€…è·å¾—æ ‡ç­¾æˆæœ¬é«˜æ˜‚ï¼Œæ‰€ä»¥æˆ‘ä»¬æ ‡æ³¨ä¸€ä¸ªå­é›†ï¼Œæˆ–è€…æˆ‘ä»¬æ•…æ„ä¸‹é‡‡æ ·ä»¥è¿›è¡Œæ›´å¿«çš„å®éªŒã€‚

    æ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œ**å¥½çš„é‡‡æ ·å¯ä»¥ä½¿æ¨¡å‹å¼€å‘é«˜æ•ˆå¹¶ç¡®ä¿æ¨¡å‹æ³›åŒ–ï¼Œè€Œå·®çš„é‡‡æ ·å¯èƒ½è¯¯å¯¼ä½ çš„ç»“æœã€‚**

    ä¾‹å¦‚ï¼Œé€‰æ‹©ä¸€ä¸ªä¸å…·ä»£è¡¨æ€§çš„å­é›†å¯èƒ½å¯¼è‡´ä½ çš„æ¨¡å‹åœ¨é‚£ä¸ªç‰¹å®šå­é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç”Ÿäº§ä¸­å¤±è´¥ã€‚

    ### é‡‡æ ·ç±»å‹

    å¹¿ä¹‰ä¸Šï¼Œé‡‡æ ·æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªå®¶æ—ï¼š

    ![é‡‡æ ·æ–¹æ³•åˆ†ç±»](https://www.dailydoseofds.com/content/images/2025/08/image-150.png)

    - **éæ¦‚ç‡é‡‡æ ·**ï¼šä¸ä¸¥æ ¼åŸºäºéšæœºæœºä¼šï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€äº›ä¸»è§‚æˆ–å®é™…æ ‡å‡†æ¥é€‰æ‹©æ•°æ®
    - **æ¦‚ç‡é‡‡æ ·**ï¼šæ€»ä½“ä¸­çš„æ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰è¢«é€‰æ‹©çš„æŸç§æ¦‚ç‡ï¼Œé€šå¸¸åŠªåŠ›è·å¾—æ— åæ ·æœ¬

    è®©æˆ‘ä»¬çœ‹çœ‹æ¯ä¸ªç±»åˆ«ä¸­çš„å¸¸è§æŠ€æœ¯åŠå…¶å«ä¹‰ï¼š

    #### éæ¦‚ç‡é‡‡æ ·æ–¹æ³•

    åœ¨éæ¦‚ç‡é‡‡æ ·æ–¹æ³•ä¸‹ï¼Œæˆ‘ä»¬æœ‰ï¼š

    ##### ä¾¿åˆ©é‡‡æ ·

    é€‰æ‹©æœ€å®¹æ˜“è·å¾—çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨æ—¥å¿—ä¸­çš„å‰10,000æ¡è®°å½•ï¼Œå› ä¸ºå®ƒä»¬éšæ‰‹å¯å¾—ï¼Œæˆ–è€…ä½¿ç”¨ä»ä¸€ä¸ªå¯è®¿é—®æºï¼ˆå¦‚ä¸€ä¸ªåŸå¸‚æˆ–ä¸€ä¸ªç”¨æˆ·ç»„ï¼‰æ”¶é›†çš„æ•°æ®é›†ï¼Œå› ä¸ºå®ƒå¾ˆæ–¹ä¾¿ã€‚

    ![ä¾¿åˆ©é‡‡æ ·](https://www.dailydoseofds.com/content/images/2025/08/image-152-1.png)
    *ä¾¿åˆ©é‡‡æ ·*

    **è¿™ç§æ–¹æ³•çš„å«ä¹‰**åŒ…æ‹¬é«˜åå·®é£é™©ï¼Œå› ä¸ºæ ·æœ¬å¯èƒ½ä¸ä»£è¡¨æ•´ä½“äººç¾¤ã€‚

    è¿™ç§æ–¹æ³•å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºæ­£å¦‚åç§°æ‰€è¯´ï¼Œå®ƒå¾ˆæ–¹ä¾¿ï¼Œä½†å®ƒå¯èƒ½æ‰­æ›²ç»“æœã€‚ä¾‹å¦‚ï¼Œåœ¨å•ä¸ªåŸå¸‚æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹å¯èƒ½æ— æ³•æ³›åŒ–åˆ°å…¶ä»–åœ°åŒºã€‚

    ##### é›ªçƒé‡‡æ ·

    ä½¿ç”¨ç°æœ‰æ ·æœ¬æ•°æ®æ¥æ‹›å‹Ÿæ›´å¤šæ•°æ®ã€‚è¿™é€šå¸¸ç”¨äºç¤¾äº¤ç½‘ç»œæˆ–å›¾ä¸­ã€‚ä¾‹å¦‚ï¼Œä½ æœ‰ä¸€äº›ç”¨æˆ·çš„æ•°æ®ï¼Œç„¶åä½ åŒ…æ‹¬ä»–ä»¬çš„æœ‹å‹ï¼Œç„¶åæœ‹å‹çš„æœ‹å‹ï¼Œç­‰ç­‰ã€‚

    ![é›ªçƒé‡‡æ ·](https://www.dailydoseofds.com/content/images/2025/08/image-155-1.png)
    *é›ªçƒé‡‡æ ·*

    **ä¼˜ç‚¹**ï¼šå¯¹äºéš¾ä»¥æ¥è§¦çš„äººç¾¤å¾ˆæœ‰ç”¨ï¼ˆä¾‹å¦‚ï¼Œç¨€æœ‰ç–¾ç—…æ‚£è€…ï¼‰ã€‚

    **ç¼ºç‚¹**ï¼šå¯èƒ½åˆ›å»ºé«˜åº¦ç›¸å…³çš„æ ·æœ¬ï¼Œå› ä¸ºç½‘ç»œä¸­çš„äººå¾€å¾€ç›¸ä¼¼ã€‚è¿™å¯èƒ½å¯¼è‡´ç¼ºä¹å¤šæ ·æ€§å’Œæ³›åŒ–é—®é¢˜ã€‚

    ##### åˆ¤æ–­é‡‡æ ·

    åŸºäºä¸“å®¶åˆ¤æ–­æˆ–ç‰¹å®šæ ‡å‡†é€‰æ‹©æ•°æ®ã€‚ä¾‹å¦‚ï¼ŒåŒ»ç”Ÿå¯èƒ½é€‰æ‹©ä»–ä»¬è®¤ä¸º"å…¸å‹"çš„ç—…ä¾‹ç”¨äºç ”ç©¶ã€‚

    **ä¼˜ç‚¹**ï¼šå¯ä»¥ç¡®ä¿åŒ…å«é‡è¦æˆ–ä»£è¡¨æ€§çš„æ¡ˆä¾‹ã€‚

    **ç¼ºç‚¹**ï¼šé«˜åº¦ä¸»è§‚ï¼Œå¯èƒ½å¼•å…¥ä¸“å®¶åè§ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### æ¦‚ç‡é‡‡æ ·æ–¹æ³•

    æ¦‚ç‡é‡‡æ ·æ–¹æ³•æä¾›äº†æ›´ä¸¥æ ¼å’Œç»Ÿè®¡ä¸Šåˆç†çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼š

    ##### ç®€å•éšæœºé‡‡æ ·

    æ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰ç›¸ç­‰çš„è¢«é€‰æ‹©æ¦‚ç‡ã€‚è¿™æ˜¯æœ€åŸºæœ¬çš„æ¦‚ç‡é‡‡æ ·å½¢å¼ã€‚

    ```python
    import pandas as pd
    import numpy as np

    # ç®€å•éšæœºé‡‡æ ·ç¤ºä¾‹
    def simple_random_sampling(data, sample_size):
        \"\"\"ä»æ•°æ®ä¸­è¿›è¡Œç®€å•éšæœºé‡‡æ ·\"\"\"
        return data.sample(n=sample_size, random_state=42)

    # ç¤ºä¾‹ä½¿ç”¨
    # sampled_data = simple_random_sampling(full_dataset, 1000)
    ```

    **ä¼˜ç‚¹**ï¼šæ— åï¼Œæ¯ä¸ªæ•°æ®ç‚¹æœºä¼šç›¸ç­‰ã€‚

    **ç¼ºç‚¹**ï¼šå¯èƒ½æ— æ³•æ•è·é‡è¦çš„å­ç»„ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæŸäº›ç»„åœ¨æ€»ä½“ä¸­å¾ˆå°‘è§ã€‚

    ##### åˆ†å±‚é‡‡æ ·

    å°†æ€»ä½“åˆ†ä¸ºå­ç»„ï¼ˆå±‚ï¼‰ï¼Œç„¶åä»æ¯å±‚ä¸­é‡‡æ ·ã€‚è¿™ç¡®ä¿æ‰€æœ‰é‡è¦å­ç»„éƒ½åœ¨æ ·æœ¬ä¸­å¾—åˆ°ä»£è¡¨ã€‚

    ```python
    from sklearn.model_selection import train_test_split

    def stratified_sampling(data, target_column, sample_size, random_state=42):
        \"\"\"åŸºäºç›®æ ‡å˜é‡è¿›è¡Œåˆ†å±‚é‡‡æ ·\"\"\"

        # è®¡ç®—æ¯ä¸ªå±‚çš„æ¯”ä¾‹
        class_proportions = data[target_column].value_counts(normalize=True)

        sampled_data = []
        for class_value, proportion in class_proportions.items():
            class_data = data[data[target_column] == class_value]
            class_sample_size = int(sample_size * proportion)

            if class_sample_size > 0:
                class_sample = class_data.sample(
                    n=min(class_sample_size, len(class_data)), 
                    random_state=random_state
                )
                sampled_data.append(class_sample)

        return pd.concat(sampled_data, ignore_index=True)
    ```

    **ä¼˜ç‚¹**ï¼šç¡®ä¿é‡è¦å­ç»„çš„ä»£è¡¨æ€§ï¼Œé€šå¸¸æ¯”ç®€å•éšæœºé‡‡æ ·æ›´å‡†ç¡®ã€‚

    **ç¼ºç‚¹**ï¼šéœ€è¦äº‹å…ˆäº†è§£ç›¸å…³çš„åˆ†å±‚å˜é‡ã€‚

    ##### ç³»ç»Ÿé‡‡æ ·

    é€‰æ‹©æ¯ç¬¬kä¸ªæ•°æ®ç‚¹ï¼Œå…¶ä¸­k = æ€»ä½“å¤§å°/æ ·æœ¬å¤§å°ã€‚

    ```python
    def systematic_sampling(data, sample_size):
        \"\"\"ç³»ç»Ÿé‡‡æ ·å®ç°\"\"\"
        n = len(data)
        k = n // sample_size  # é‡‡æ ·é—´éš”

        # éšæœºé€‰æ‹©èµ·å§‹ç‚¹
        start = np.random.randint(0, k)

        # é€‰æ‹©æ¯ç¬¬kä¸ªå…ƒç´ 
        indices = range(start, n, k)
        return data.iloc[list(indices)[:sample_size]]
    ```

    **ä¼˜ç‚¹**ï¼šç®€å•å®ç°ï¼Œåˆ†å¸ƒå‡åŒ€ã€‚

    **ç¼ºç‚¹**ï¼šå¦‚æœæ•°æ®ä¸­å­˜åœ¨å‘¨æœŸæ€§æ¨¡å¼ï¼Œå¯èƒ½å¼•å…¥åå·®ã€‚

    ##### èšç±»é‡‡æ ·

    å°†æ€»ä½“åˆ†ä¸ºèšç±»ï¼Œç„¶åéšæœºé€‰æ‹©æ•´ä¸ªèšç±»ã€‚

    ```python
    def cluster_sampling(data, cluster_column, num_clusters):
        \"\"\"èšç±»é‡‡æ ·å®ç°\"\"\"

        # è·å–æ‰€æœ‰å”¯ä¸€èšç±»
        all_clusters = data[cluster_column].unique()

        # éšæœºé€‰æ‹©èšç±»
        selected_clusters = np.random.choice(
            all_clusters, 
            size=min(num_clusters, len(all_clusters)), 
            replace=False
        )

        # è¿”å›é€‰å®šèšç±»ä¸­çš„æ‰€æœ‰æ•°æ®
        return data[data[cluster_column].isin(selected_clusters)]
    ```

    **ä¼˜ç‚¹**ï¼šå½“èšç±»å†…éƒ¨ç›¸ä¼¼ä½†èšç±»é—´ä¸åŒæ—¶å¾ˆæœ‰æ•ˆï¼Œæˆæœ¬æ•ˆç›Šé«˜ã€‚

    **ç¼ºç‚¹**ï¼šå¦‚æœèšç±»å†…éƒ¨å·®å¼‚å¾ˆå¤§ï¼Œå¯èƒ½ä¸å¤Ÿä»£è¡¨æ€§ã€‚

    ### MLä¸­çš„é‡‡æ ·æœ€ä½³å®è·µ

    #### 1. æ—¶é—´åºåˆ—æ•°æ®çš„é‡‡æ ·

    å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œ**æ°¸è¿œä¸è¦éšæœºæ‰“ä¹±**ï¼ä½¿ç”¨æ—¶é—´åŸºç¡€çš„åˆ†å‰²ï¼š

    ```python
    def temporal_split(data, time_column, train_ratio=0.7, val_ratio=0.15):
        \"\"\"æ—¶é—´åºåˆ—æ•°æ®çš„æ—¶é—´åŸºç¡€åˆ†å‰²\"\"\"

        # æŒ‰æ—¶é—´æ’åº
        data_sorted = data.sort_values(time_column)

        n = len(data_sorted)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))

        train_data = data_sorted.iloc[:train_end]
        val_data = data_sorted.iloc[train_end:val_end]
        test_data = data_sorted.iloc[val_end:]

        return train_data, val_data, test_data
    ```

    #### 2. ä¸å¹³è¡¡æ•°æ®çš„é‡‡æ ·

    å¯¹äºç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®ï¼Œè€ƒè™‘åˆ†å±‚é‡‡æ ·æˆ–ä¸“é—¨çš„é‡é‡‡æ ·æŠ€æœ¯ï¼š

    ```python
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler

    def handle_imbalanced_data(X, y, strategy='smote'):
        \"\"\"å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„é‡‡æ ·ç­–ç•¥\"\"\"

        if strategy == 'smote':
            sampler = SMOTE(random_state=42)
        elif strategy == 'undersample':
            sampler = RandomUnderSampler(random_state=42)
        else:
            raise ValueError("Strategy must be 'smote' or 'undersample'")

        X_resampled, y_resampled = sampler.fit_resample(X, y)
        return X_resampled, y_resampled
    ```

    #### 3. å¤§æ•°æ®çš„é‡‡æ ·

    å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œè€ƒè™‘åˆ†å±‚æˆ–åˆ†å—é‡‡æ ·ï¼š

    ```python
    def large_data_sampling(data, sample_size, chunk_size=10000):
        \"\"\"å¤§æ•°æ®çš„åˆ†å—é‡‡æ ·\"\"\"

        total_rows = len(data)
        sampling_ratio = sample_size / total_rows

        sampled_chunks = []

        # åˆ†å—å¤„ç†
        for chunk_start in range(0, total_rows, chunk_size):
            chunk_end = min(chunk_start + chunk_size, total_rows)
            chunk = data.iloc[chunk_start:chunk_end]

            # ä»æ¯ä¸ªå—ä¸­é‡‡æ ·
            chunk_sample_size = int(len(chunk) * sampling_ratio)
            if chunk_sample_size > 0:
                chunk_sample = chunk.sample(n=chunk_sample_size, random_state=42)
                sampled_chunks.append(chunk_sample)

        return pd.concat(sampled_chunks, ignore_index=True)
    ```

    ### é‡‡æ ·è´¨é‡è¯„ä¼°

    è¯„ä¼°é‡‡æ ·è´¨é‡çš„å…³é”®æŒ‡æ ‡ï¼š

    ```python
    def evaluate_sampling_quality(original_data, sampled_data, target_column):
        \"\"\"è¯„ä¼°é‡‡æ ·è´¨é‡\"\"\"

        results = {}

        # 1. ç±»åˆ«åˆ†å¸ƒæ¯”è¾ƒ
        orig_dist = original_data[target_column].value_counts(normalize=True)
        samp_dist = sampled_data[target_column].value_counts(normalize=True)

        results['distribution_difference'] = abs(orig_dist - samp_dist).mean()

        # 2. ç»Ÿè®¡ç‰¹å¾æ¯”è¾ƒ
        numeric_columns = original_data.select_dtypes(include=[np.number]).columns

        for col in numeric_columns:
            orig_mean = original_data[col].mean()
            samp_mean = sampled_data[col].mean()
            results[f'{col}_mean_diff'] = abs(orig_mean - samp_mean)

        # 3. é‡‡æ ·æ¯”ä¾‹
        results['sampling_ratio'] = len(sampled_data) / len(original_data)

        return results
    ```

    é‡‡æ ·æ˜¯MLç®¡é“ä¸­çš„å…³é”®æ­¥éª¤ï¼Œæ­£ç¡®çš„é‡‡æ ·ç­–ç•¥å¯ä»¥æ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€‰æ‹©åˆé€‚çš„é‡‡æ ·æ–¹æ³•å–å†³äºä½ çš„æ•°æ®ç‰¹å¾ã€ä¸šåŠ¡éœ€æ±‚å’Œè®¡ç®—èµ„æºã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ç±»åˆ«ä¸å¹³è¡¡å¤„ç†

    ç±»åˆ«ä¸å¹³è¡¡æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¸¸è§çš„æŒ‘æˆ˜ä¹‹ä¸€ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®ä¸–ç•Œçš„åº”ç”¨ä¸­ã€‚å½“æ•°æ®é›†ä¸­ä¸åŒç±»åˆ«çš„æ ·æœ¬æ•°é‡å·®å¼‚å¾ˆå¤§æ—¶ï¼Œå°±ä¼šå‡ºç°ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

    ### ä»€ä¹ˆæ˜¯ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ

    ç±»åˆ«ä¸å¹³è¡¡æŒ‡çš„æ˜¯åˆ†ç±»é—®é¢˜ä¸­å„ç±»åˆ«çš„æ ·æœ¬æ•°é‡åˆ†å¸ƒä¸å‡åŒ€çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼š

    - **æ¬ºè¯ˆæ£€æµ‹**ï¼šæ­£å¸¸äº¤æ˜“å 99%ï¼Œæ¬ºè¯ˆäº¤æ˜“å 1%
    - **åŒ»ç–—è¯Šæ–­**ï¼šå¥åº·æ‚£è€…å 95%ï¼Œæ‚£ç—…æ‚£è€…å 5%
    - **åƒåœ¾é‚®ä»¶æ£€æµ‹**ï¼šæ­£å¸¸é‚®ä»¶å 90%ï¼Œåƒåœ¾é‚®ä»¶å 10%
    - **å®¢æˆ·æµå¤±é¢„æµ‹**ï¼šç•™å­˜å®¢æˆ·å 85%ï¼Œæµå¤±å®¢æˆ·å 15%

    ### ä¸ºä»€ä¹ˆç±»åˆ«ä¸å¹³è¡¡æ˜¯é—®é¢˜ï¼Ÿ

    #### 1. **å‡†ç¡®ç‡è¯¯å¯¼**
    ```python
    # ç¤ºä¾‹ï¼š99%æ­£å¸¸ï¼Œ1%å¼‚å¸¸çš„æ•°æ®
    # ä¸€ä¸ªæ€»æ˜¯é¢„æµ‹"æ­£å¸¸"çš„æ¨¡å‹ä¼šæœ‰99%çš„å‡†ç¡®ç‡
    # ä½†å®ƒå®Œå…¨æ— æ³•æ£€æµ‹å¼‚å¸¸æƒ…å†µï¼

    def accuracy_paradox_example():
        \"\"\"å±•ç¤ºå‡†ç¡®ç‡æ‚–è®º\"\"\"

        # æ¨¡æ‹Ÿä¸å¹³è¡¡æ•°æ®
        total_samples = 10000
        positive_samples = 100  # 1%
        negative_samples = 9900  # 99%

        # "æ„šè ¢"åˆ†ç±»å™¨ï¼šæ€»æ˜¯é¢„æµ‹å¤šæ•°ç±»
        always_negative_predictions = [0] * total_samples
        true_labels = [1] * positive_samples + [0] * negative_samples

        # è®¡ç®—å‡†ç¡®ç‡
        correct_predictions = sum(1 for pred, true in zip(always_negative_predictions, true_labels) if pred == true)
        accuracy = correct_predictions / total_samples

        print(f"æ€»æ˜¯é¢„æµ‹è´Ÿç±»çš„å‡†ç¡®ç‡: {accuracy:.1%}")
        print(f"ä½†æ˜¯å¬å›ç‡ï¼ˆæ£€æµ‹åˆ°çš„æ­£ç±»ï¼‰: {0:.1%}")

        return accuracy

    # accuracy_paradox_example()
    ```

    #### 2. **æ¨¡å‹åå‘å¤šæ•°ç±»**
    å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•è¢«è®¾è®¡ä¸ºæœ€å°åŒ–æ•´ä½“é”™è¯¯ç‡ï¼Œè¿™è‡ªç„¶å¯¼è‡´å®ƒä»¬åå‘äºå¤šæ•°ç±»ã€‚

    #### 3. **å°‘æ•°ç±»å­¦ä¹ ä¸è¶³**
    ç”±äºå°‘æ•°ç±»æ ·æœ¬å¤ªå°‘ï¼Œæ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°è¶³å¤Ÿçš„æ¨¡å¼æ¥å‡†ç¡®è¯†åˆ«è¿™äº›ç±»åˆ«ã€‚

    ### ç±»åˆ«ä¸å¹³è¡¡çš„æ£€æµ‹

    ```python
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from collections import Counter

    def analyze_class_imbalance(y, class_names=None):
        \"\"\"åˆ†æç±»åˆ«ä¸å¹³è¡¡ç¨‹åº¦\"\"\"

        # è®¡ç®—ç±»åˆ«åˆ†å¸ƒ
        class_counts = Counter(y)
        total_samples = len(y)

        print("=== ç±»åˆ«ä¸å¹³è¡¡åˆ†æ ===")
        print(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"ç±»åˆ«æ•°é‡: {len(class_counts)}")
        print()

        # è®¡ç®—ä¸å¹³è¡¡æ¯”ç‡
        sorted_counts = sorted(class_counts.values(), reverse=True)
        majority_count = sorted_counts[0]
        minority_count = sorted_counts[-1]
        imbalance_ratio = majority_count / minority_count

        print(f"ä¸å¹³è¡¡æ¯”ç‡: {imbalance_ratio:.1f}:1")
        print(f"å¤šæ•°ç±»å æ¯”: {majority_count/total_samples:.1%}")
        print(f"å°‘æ•°ç±»å æ¯”: {minority_count/total_samples:.1%}")
        print()

        # è¯¦ç»†åˆ†å¸ƒ
        print("è¯¦ç»†ç±»åˆ«åˆ†å¸ƒ:")
        for class_label, count in sorted(class_counts.items()):
            percentage = count / total_samples * 100
            class_name = class_names[class_label] if class_names else f"ç±»åˆ« {class_label}"
            print(f"  {class_name}: {count:,} ({percentage:.1f}%)")

        # ä¸å¹³è¡¡ä¸¥é‡ç¨‹åº¦è¯„ä¼°
        if imbalance_ratio < 2:
            severity = "è½»å¾®ä¸å¹³è¡¡"
        elif imbalance_ratio < 10:
            severity = "ä¸­ç­‰ä¸å¹³è¡¡"
        elif imbalance_ratio < 100:
            severity = "ä¸¥é‡ä¸å¹³è¡¡"
        else:
            severity = "æåº¦ä¸å¹³è¡¡"

        print(f"\\nä¸å¹³è¡¡ä¸¥é‡ç¨‹åº¦: {severity}")

        return {
            'imbalance_ratio': imbalance_ratio,
            'class_distribution': class_counts,
            'severity': severity
        }
    ```

    ### å¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æ–¹æ³•

    å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ä¸»è¦æœ‰ä¸¤å¤§ç±»æ–¹æ³•ï¼š**æ•°æ®å±‚é¢çš„æ–¹æ³•**å’Œ**ç®—æ³•å±‚é¢çš„æ–¹æ³•**ã€‚

    #### æ•°æ®å±‚é¢çš„æ–¹æ³•

    ##### 1. **è¿‡é‡‡æ ·ï¼ˆOversamplingï¼‰**

    å¢åŠ å°‘æ•°ç±»æ ·æœ¬çš„æ•°é‡ï¼š

    ```python
    from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler

    def oversampling_techniques(X, y):
        \"\"\"ä¸åŒçš„è¿‡é‡‡æ ·æŠ€æœ¯å¯¹æ¯”\"\"\"

        techniques = {
            'Random Oversampling': RandomOverSampler(random_state=42),
            'SMOTE': SMOTE(random_state=42),
            'ADASYN': ADASYN(random_state=42)
        }

        results = {}

        for name, sampler in techniques.items():
            try:
                X_resampled, y_resampled = sampler.fit_resample(X, y)

                results[name] = {
                    'original_shape': X.shape,
                    'resampled_shape': X_resampled.shape,
                    'original_distribution': Counter(y),
                    'resampled_distribution': Counter(y_resampled)
                }

                print(f"\\n{name}:")
                print(f"  åŸå§‹æ•°æ®: {X.shape[0]} æ ·æœ¬")
                print(f"  é‡é‡‡æ ·å: {X_resampled.shape[0]} æ ·æœ¬")
                print(f"  åŸå§‹åˆ†å¸ƒ: {dict(Counter(y))}")
                print(f"  é‡é‡‡æ ·åˆ†å¸ƒ: {dict(Counter(y_resampled))}")

            except Exception as e:
                print(f"{name} å¤±è´¥: {e}")
                results[name] = None

        return results
    ```

    **SMOTEï¼ˆSynthetic Minority Oversampling Techniqueï¼‰**æ˜¯æœ€æµè¡Œçš„è¿‡é‡‡æ ·æŠ€æœ¯ï¼š

    ```python
    def smote_explanation():
        \"\"\"SMOTEç®—æ³•åŸç†è§£é‡Š\"\"\"

        print("=== SMOTEç®—æ³•åŸç† ===")
        print("1. å¯¹äºæ¯ä¸ªå°‘æ•°ç±»æ ·æœ¬:")
        print("   - æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»ï¼ˆé€šå¸¸k=5ï¼‰")
        print("   - éšæœºé€‰æ‹©å…¶ä¸­ä¸€ä¸ªé‚»å±…")
        print("   - åœ¨è¯¥æ ·æœ¬å’Œé€‰å®šé‚»å±…ä¹‹é—´çš„çº¿æ®µä¸Šéšæœºç”Ÿæˆæ–°æ ·æœ¬")
        print()
        print("2. æ•°å­¦è¡¨ç¤º:")
        print("   new_sample = sample + Î» Ã— (neighbor - sample)")
        print("   å…¶ä¸­ Î» æ˜¯ [0,1] ä¹‹é—´çš„éšæœºæ•°")
        print()
        print("3. ä¼˜ç‚¹:")
        print("   - ç”Ÿæˆåˆç†çš„åˆæˆæ ·æœ¬")
        print("   - ä¸æ˜¯ç®€å•å¤åˆ¶ï¼Œè€Œæ˜¯åˆ›é€ æ–°çš„å˜åŒ–")
        print("   - åœ¨ç‰¹å¾ç©ºé—´ä¸­å¡«è¡¥å°‘æ•°ç±»åŒºåŸŸ")
        print()
        print("4. ç¼ºç‚¹:")
        print("   - å¯èƒ½åœ¨å™ªå£°åŒºåŸŸç”Ÿæˆæ ·æœ¬")
        print("   - å¯¹é«˜ç»´æ•°æ®æ•ˆæœå¯èƒ½ä¸ä½³")
        print("   - å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ")

    # smote_explanation()
    ```

    ##### 2. **æ¬ é‡‡æ ·ï¼ˆUndersamplingï¼‰**

    å‡å°‘å¤šæ•°ç±»æ ·æœ¬çš„æ•°é‡ï¼š

    ```python
    from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours

    def undersampling_techniques(X, y):
        \"\"\"ä¸åŒçš„æ¬ é‡‡æ ·æŠ€æœ¯\"\"\"

        techniques = {
            'Random Undersampling': RandomUnderSampler(random_state=42),
            'Tomek Links': TomekLinks(),
            'Edited Nearest Neighbours': EditedNearestNeighbours()
        }

        results = {}

        for name, sampler in techniques.items():
            try:
                X_resampled, y_resampled = sampler.fit_resample(X, y)

                results[name] = {
                    'data_reduction': 1 - (X_resampled.shape[0] / X.shape[0]),
                    'class_distribution': Counter(y_resampled)
                }

                print(f"\\n{name}:")
                print(f"  æ•°æ®å‡å°‘: {results[name]['data_reduction']:.1%}")
                print(f"  æœ€ç»ˆåˆ†å¸ƒ: {dict(Counter(y_resampled))}")

            except Exception as e:
                print(f"{name} å¤±è´¥: {e}")
                results[name] = None

        return results
    ```

    ##### 3. **æ··åˆé‡‡æ ·**

    ç»“åˆè¿‡é‡‡æ ·å’Œæ¬ é‡‡æ ·ï¼š

    ```python
    from imblearn.combine import SMOTETomek, SMOTEENN

    def combined_sampling(X, y):
        \"\"\"æ··åˆé‡‡æ ·æŠ€æœ¯\"\"\"

        techniques = {
            'SMOTE + Tomek': SMOTETomek(random_state=42),
            'SMOTE + ENN': SMOTEENN(random_state=42)
        }

        for name, sampler in techniques.items():
            X_resampled, y_resampled = sampler.fit_resample(X, y)

            print(f"\\n{name}:")
            print(f"  åŸå§‹: {Counter(y)}")
            print(f"  å¤„ç†å: {Counter(y_resampled)}")
            print(f"  æ ·æœ¬å˜åŒ–: {X.shape[0]} â†’ {X_resampled.shape[0]}")
    ```

    #### ç®—æ³•å±‚é¢çš„æ–¹æ³•

    ##### 1. **ç±»åˆ«æƒé‡è°ƒæ•´**

    ```python
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.utils.class_weight import compute_class_weight

    def class_weight_methods(X, y):
        \"\"\"ç±»åˆ«æƒé‡è°ƒæ•´æ–¹æ³•\"\"\"

        # è®¡ç®—ç±»åˆ«æƒé‡
        classes = np.unique(y)
        class_weights = compute_class_weight('balanced', classes=classes, y=y)
        class_weight_dict = dict(zip(classes, class_weights))

        print("è®¡ç®—çš„ç±»åˆ«æƒé‡:")
        for class_label, weight in class_weight_dict.items():
            print(f"  ç±»åˆ« {class_label}: {weight:.2f}")

        # ä¸åŒç®—æ³•çš„ç±»åˆ«æƒé‡ä½¿ç”¨
        models = {
            'Logistic Regression': LogisticRegression(class_weight='balanced', random_state=42),
            'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),
            'Custom Weights': LogisticRegression(class_weight=class_weight_dict, random_state=42)
        }

        return models, class_weight_dict
    ```

    ##### 2. **é˜ˆå€¼è°ƒæ•´**

    ```python
    from sklearn.metrics import precision_recall_curve, roc_curve
    import matplotlib.pyplot as plt

    def threshold_optimization(y_true, y_proba, metric='f1'):
        \"\"\"ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼\"\"\"

        # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡
        precision, recall, thresholds_pr = precision_recall_curve(y_true, y_proba)

        # è®¡ç®—F1åˆ†æ•°
        f1_scores = 2 * (precision * recall) / (precision + recall)
        f1_scores = np.nan_to_num(f1_scores)  # å¤„ç†é™¤é›¶æƒ…å†µ

        # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
        if metric == 'f1':
            best_idx = np.argmax(f1_scores)
            best_threshold = thresholds_pr[best_idx]
            best_score = f1_scores[best_idx]

            print(f"æœ€ä½³F1é˜ˆå€¼: {best_threshold:.3f}")
            print(f"æœ€ä½³F1åˆ†æ•°: {best_score:.3f}")
            print(f"å¯¹åº”ç²¾ç¡®ç‡: {precision[best_idx]:.3f}")
            print(f"å¯¹åº”å¬å›ç‡: {recall[best_idx]:.3f}")

        return best_threshold, best_score
    ```

    ##### 3. **é›†æˆæ–¹æ³•**

    ```python
    from sklearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier
    from imblearn.ensemble import EasyEnsembleClassifier

    def ensemble_methods_for_imbalance():
        \"\"\"ä¸“é—¨å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„é›†æˆæ–¹æ³•\"\"\"

        methods = {
            'Balanced Random Forest': BalancedRandomForestClassifier(random_state=42),
            'Balanced Bagging': BalancedBaggingClassifier(random_state=42),
            'Easy Ensemble': EasyEnsembleClassifier(random_state=42)
        }

        print("ä¸å¹³è¡¡æ•°æ®çš„é›†æˆæ–¹æ³•:")
        for name, model in methods.items():
            print(f"\\n{name}:")
            print(f"  åŸç†: {model.__class__.__doc__.split('.')[0] if model.__class__.__doc__ else 'ä¸“é—¨å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„é›†æˆæ–¹æ³•'}")

        return methods
    ```

    ### è¯„ä¼°ä¸å¹³è¡¡æ•°æ®æ¨¡å‹

    å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼Œå‡†ç¡®ç‡ä¸æ˜¯å¥½çš„è¯„ä¼°æŒ‡æ ‡ã€‚åº”è¯¥ä½¿ç”¨ï¼š

    ```python
    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score

    def evaluate_imbalanced_model(y_true, y_pred, y_proba=None):
        \"\"\"è¯„ä¼°ä¸å¹³è¡¡æ•°æ®æ¨¡å‹çš„å®Œæ•´æŒ‡æ ‡\"\"\"

        print("=== ä¸å¹³è¡¡æ•°æ®æ¨¡å‹è¯„ä¼° ===")

        # 1. åˆ†ç±»æŠ¥å‘Š
        print("\\n1. åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred))

        # 2. æ··æ·†çŸ©é˜µ
        print("\\n2. æ··æ·†çŸ©é˜µ:")
        cm = confusion_matrix(y_true, y_pred)
        print(cm)

        # 3. å…³é”®æŒ‡æ ‡è®¡ç®—
        tn, fp, fn, tp = cm.ravel()

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        print(f"\\n3. å…³é”®æŒ‡æ ‡:")
        print(f"   ç²¾ç¡®ç‡ (Precision): {precision:.3f}")
        print(f"   å¬å›ç‡ (Recall): {recall:.3f}")
        print(f"   ç‰¹å¼‚æ€§ (Specificity): {specificity:.3f}")
        print(f"   F1åˆ†æ•°: {f1:.3f}")

        # 4. AUCæŒ‡æ ‡
        if y_proba is not None:
            auc_roc = roc_auc_score(y_true, y_proba)
            auc_pr = average_precision_score(y_true, y_proba)

            print(f"\\n4. AUCæŒ‡æ ‡:")
            print(f"   ROC-AUC: {auc_roc:.3f}")
            print(f"   PR-AUC: {auc_pr:.3f}")

        return {
            'precision': precision,
            'recall': recall,
            'specificity': specificity,
            'f1': f1,
            'auc_roc': auc_roc if y_proba is not None else None,
            'auc_pr': auc_pr if y_proba is not None else None
        }
    ```

    ç±»åˆ«ä¸å¹³è¡¡æ˜¯ç°å®ä¸–ç•ŒMLé¡¹ç›®ä¸­çš„å¸¸è§æŒ‘æˆ˜ã€‚é€‰æ‹©åˆé€‚çš„å¤„ç†æ–¹æ³•å–å†³äºä½ çš„å…·ä½“é—®é¢˜ã€æ•°æ®ç‰¹å¾å’Œä¸šåŠ¡éœ€æ±‚ã€‚é€šå¸¸å»ºè®®å°è¯•å¤šç§æ–¹æ³•å¹¶ä½¿ç”¨é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡æ¥é€‰æ‹©æœ€ä½³æ–¹æ¡ˆã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## æ•°æ®æ³„æ¼ï¼šMLä¸­çš„éšå½¢æ€æ‰‹

    æ•°æ®æ³„æ¼æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å±é™©çš„é™·é˜±ä¹‹ä¸€ã€‚å®ƒä¼šè®©ä½ çš„æ¨¡å‹åœ¨è®­ç»ƒå’ŒéªŒè¯æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®Œå…¨å¤±æ•ˆã€‚ç†è§£å’Œé˜²æ­¢æ•°æ®æ³„æ¼å¯¹äºæ„å»ºå¯é çš„MLç³»ç»Ÿè‡³å…³é‡è¦ã€‚

    ### ä»€ä¹ˆæ˜¯æ•°æ®æ³„æ¼ï¼Ÿ

    **æ•°æ®æ³„æ¼**æŒ‡çš„æ˜¯è®­ç»ƒæ•°æ®ä¸­åŒ…å«äº†åœ¨å®é™…é¢„æµ‹æ—¶ä¸åº”è¯¥å¯ç”¨çš„ä¿¡æ¯ã€‚æ¢å¥è¯è¯´ï¼Œæ¨¡å‹"å·çœ‹"äº†æœªæ¥çš„ä¿¡æ¯æˆ–ç›®æ ‡å˜é‡çš„ç›´æ¥ä»£ç†ã€‚

    ![æ•°æ®æ³„æ¼æ¦‚å¿µ](https://www.dailydoseofds.com/content/images/2025/08/image-170.png)

    ### æ•°æ®æ³„æ¼çš„ç±»å‹

    #### 1. **æ—¶é—´æ³„æ¼ï¼ˆTemporal Leakageï¼‰**

    è¿™æ˜¯æœ€å¸¸è§çš„æ³„æ¼ç±»å‹ï¼Œå‘ç”Ÿåœ¨ä½¿ç”¨æœªæ¥ä¿¡æ¯é¢„æµ‹è¿‡å»äº‹ä»¶æ—¶ã€‚

    ```python
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta

    def temporal_leakage_example():
        \"\"\"æ—¶é—´æ³„æ¼ç¤ºä¾‹\"\"\"

        # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
        dates = pd.date_range('2023-01-01', periods=1000, freq='D')

        # æ¨¡æ‹Ÿè‚¡ç¥¨ä»·æ ¼æ•°æ®
        np.random.seed(42)
        prices = 100 + np.cumsum(np.random.randn(1000) * 0.5)

        df = pd.DataFrame({
            'date': dates,
            'price': prices,
            'volume': np.random.randint(1000, 10000, 1000)
        })

        # âŒ é”™è¯¯ï¼šä½¿ç”¨æœªæ¥7å¤©çš„å¹³å‡ä»·æ ¼ä½œä¸ºç‰¹å¾
        df['future_7day_avg'] = df['price'].rolling(window=7, center=True).mean()

        # âŒ é”™è¯¯ï¼šä½¿ç”¨å…¨å±€ç»Ÿè®¡ä¿¡æ¯
        df['price_zscore_global'] = (df['price'] - df['price'].mean()) / df['price'].std()

        # âœ… æ­£ç¡®ï¼šåªä½¿ç”¨å†å²ä¿¡æ¯
        df['past_7day_avg'] = df['price'].rolling(window=7, min_periods=1).mean().shift(1)
        df['price_zscore_rolling'] = df['price'].rolling(window=30, min_periods=1).apply(
            lambda x: (x.iloc[-1] - x.mean()) / x.std() if len(x) > 1 else 0
        ).shift(1)

        print("æ—¶é—´æ³„æ¼ç¤ºä¾‹:")
        print("âŒ é”™è¯¯ç‰¹å¾:")
        print("  - future_7day_avg: ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯")
        print("  - price_zscore_global: ä½¿ç”¨äº†å…¨å±€ç»Ÿè®¡ä¿¡æ¯")
        print("âœ… æ­£ç¡®ç‰¹å¾:")
        print("  - past_7day_avg: åªä½¿ç”¨å†å²ä¿¡æ¯")
        print("  - price_zscore_rolling: ä½¿ç”¨æ»šåŠ¨çª—å£ç»Ÿè®¡")

        return df

    # temporal_leakage_example()
    ```

    #### 2. **ç›®æ ‡æ³„æ¼ï¼ˆTarget Leakageï¼‰**

    ç‰¹å¾ç›´æ¥åŒ…å«ç›®æ ‡å˜é‡çš„ä¿¡æ¯æˆ–å…¶å¼ºä»£ç†ã€‚

    ```python
    def target_leakage_examples():
        \"\"\"ç›®æ ‡æ³„æ¼ç¤ºä¾‹\"\"\"

        examples = {
            "ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹": {
                "âŒ æ³„æ¼ç‰¹å¾": [
                    "transaction_flagged_by_bank",  # é“¶è¡Œå·²ç»æ ‡è®°ä¸ºå¯ç–‘
                    "customer_account_frozen",      # è´¦æˆ·è¢«å†»ç»“ï¼ˆç»“æœçš„ç›´æ¥æŒ‡ç¤ºï¼‰
                    "fraud_investigation_opened"    # å·²å¼€å§‹æ¬ºè¯ˆè°ƒæŸ¥
                ],
                "âœ… æ­£ç¡®ç‰¹å¾": [
                    "transaction_amount",
                    "merchant_category",
                    "time_of_day",
                    "days_since_last_transaction"
                ]
            },

            "åŒ»ç–—è¯Šæ–­": {
                "âŒ æ³„æ¼ç‰¹å¾": [
                    "prescribed_medication",        # å¤„æ–¹è¯ç‰©ï¼ˆè¯Šæ–­åçš„ç»“æœï¼‰
                    "specialist_referral",          # ä¸“ç§‘è½¬è¯Šï¼ˆè¯Šæ–­çš„ç»“æœï¼‰
                    "treatment_plan"               # æ²»ç–—è®¡åˆ’ï¼ˆè¯Šæ–­ååˆ¶å®šï¼‰
                ],
                "âœ… æ­£ç¡®ç‰¹å¾": [
                    "patient_symptoms",
                    "vital_signs",
                    "medical_history",
                    "lab_test_results"
                ]
            },

            "å®¢æˆ·æµå¤±é¢„æµ‹": {
                "âŒ æ³„æ¼ç‰¹å¾": [
                    "account_closure_date",         # è´¦æˆ·å…³é—­æ—¥æœŸ
                    "final_bill_amount",           # æœ€ç»ˆè´¦å•é‡‘é¢
                    "retention_call_made"          # æŒ½ç•™ç”µè¯ï¼ˆæµå¤±åçš„è¡ŒåŠ¨ï¼‰
                ],
                "âœ… æ­£ç¡®ç‰¹å¾": [
                    "monthly_usage_trend",
                    "customer_service_calls",
                    "payment_history",
                    "contract_length"
                ]
            }
        }

        for scenario, features in examples.items():
            print(f"\\n=== {scenario} ===")
            print("âŒ æ³„æ¼ç‰¹å¾ï¼ˆä¸åº”ä½¿ç”¨ï¼‰:")
            for feature in features["âŒ æ³„æ¼ç‰¹å¾"]:
                print(f"  - {feature}")
            print("âœ… æ­£ç¡®ç‰¹å¾ï¼ˆå¯ä»¥ä½¿ç”¨ï¼‰:")
            for feature in features["âœ… æ­£ç¡®ç‰¹å¾"]:
                print(f"  - {feature}")

        return examples

    # target_leakage_examples()
    ```

    #### 3. **è®­ç»ƒ-æµ‹è¯•æ±¡æŸ“ï¼ˆTrain-Test Contaminationï¼‰**

    åœ¨æ•°æ®åˆ†å‰²ä¹‹å‰è¿›è¡Œé¢„å¤„ç†ï¼Œå¯¼è‡´æµ‹è¯•é›†ä¿¡æ¯æ³„æ¼åˆ°è®­ç»ƒé›†ã€‚

    ```python
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split

    def train_test_contamination_demo():
        \"\"\"è®­ç»ƒ-æµ‹è¯•æ±¡æŸ“æ¼”ç¤º\"\"\"

        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        np.random.seed(42)
        X = np.random.randn(1000, 5)
        y = (X[:, 0] + X[:, 1] > 0).astype(int)

        print("=== è®­ç»ƒ-æµ‹è¯•æ±¡æŸ“æ¼”ç¤º ===\\n")

        # âŒ é”™è¯¯æ–¹æ³•ï¼šå…ˆæ ‡å‡†åŒ–ï¼Œååˆ†å‰²
        print("âŒ é”™è¯¯æ–¹æ³•ï¼šå…ˆæ ‡å‡†åŒ–ï¼Œååˆ†å‰²")
        scaler_wrong = StandardScaler()
        X_scaled_wrong = scaler_wrong.fit_transform(X)  # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        X_train_wrong, X_test_wrong, y_train, y_test = train_test_split(
            X_scaled_wrong, y, test_size=0.2, random_state=42
        )

        print(f"è®­ç»ƒé›†å‡å€¼: {X_train_wrong.mean(axis=0)}")
        print(f"æµ‹è¯•é›†å‡å€¼: {X_test_wrong.mean(axis=0)}")
        print("é—®é¢˜ï¼šæµ‹è¯•é›†çš„ç»Ÿè®¡ä¿¡æ¯å·²ç»æ³„æ¼åˆ°æ ‡å‡†åŒ–è¿‡ç¨‹ä¸­\\n")

        # âœ… æ­£ç¡®æ–¹æ³•ï¼šå…ˆåˆ†å‰²ï¼Œåæ ‡å‡†åŒ–
        print("âœ… æ­£ç¡®æ–¹æ³•ï¼šå…ˆåˆ†å‰²ï¼Œåæ ‡å‡†åŒ–")
        X_train_raw, X_test_raw, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        scaler_correct = StandardScaler()
        X_train_correct = scaler_correct.fit_transform(X_train_raw)  # åªç”¨è®­ç»ƒé›†è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        X_test_correct = scaler_correct.transform(X_test_raw)        # ç”¨è®­ç»ƒé›†ç»Ÿè®¡ä¿¡æ¯è½¬æ¢æµ‹è¯•é›†

        print(f"è®­ç»ƒé›†å‡å€¼: {X_train_correct.mean(axis=0)}")
        print(f"æµ‹è¯•é›†å‡å€¼: {X_test_correct.mean(axis=0)}")
        print("æ­£ç¡®ï¼šæµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œè½¬æ¢")

        return {
            'wrong_method': (X_train_wrong, X_test_wrong),
            'correct_method': (X_train_correct, X_test_correct)
        }

    # train_test_contamination_demo()
    ```

    #### 4. **æ•°æ®æ”¶é›†/æ ‡æ³¨è¿‡ç¨‹ä¸­çš„æ³„æ¼**

    æ•°æ®æ”¶é›†æˆ–æ ‡æ³¨è¿‡ç¨‹ä¸­å¼•å…¥çš„åå·®ã€‚

    ```python
    def data_collection_leakage_examples():
        \"\"\"æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­çš„æ³„æ¼ç¤ºä¾‹\"\"\"

        examples = [
            {
                "åœºæ™¯": "åŒ»å­¦å½±åƒè¯Šæ–­",
                "é—®é¢˜": "ç–¾ç—…æ‚£è€…çš„Xå…‰ç‰‡æ¥è‡ªç‰¹å®šåŒ»é™¢ï¼Œå¥åº·äººçš„Xå…‰ç‰‡æ¥è‡ªå¦ä¸€åŒ»é™¢",
                "æ³„æ¼": "æ¨¡å‹å­¦ä¼šäº†è¯†åˆ«åŒ»é™¢æ ‡è®°è€Œä¸æ˜¯ç–¾ç—…ç‰¹å¾",
                "è§£å†³æ–¹æ¡ˆ": "ç¡®ä¿æ­£è´Ÿæ ·æœ¬æ¥è‡ªç›¸åŒçš„æ•°æ®æºå’Œè®¾å¤‡"
            },
            {
                "åœºæ™¯": "COVID-19æ£€æµ‹",
                "é—®é¢˜": "é˜³æ€§ç—…ä¾‹ä¸»è¦æ¥è‡ªæŸäº›åŒ»é™¢ï¼Œé˜´æ€§ç—…ä¾‹æ¥è‡ªå…¶ä»–åŒ»é™¢",
                "æ³„æ¼": "æ¨¡å‹å­¦ä¼šäº†åŒ»é™¢æ¥æºè€Œä¸æ˜¯å®é™…æ‚£è€…æ•°æ®",
                "è§£å†³æ–¹æ¡ˆ": "å¹³è¡¡ä¸åŒæ¥æºçš„æ­£è´Ÿæ ·æœ¬åˆ†å¸ƒ"
            },
            {
                "åœºæ™¯": "æ–‡æœ¬åˆ†ç±»",
                "é—®é¢˜": "ä¸åŒç±»åˆ«çš„æ–‡æœ¬åœ¨ä¸åŒæ—¶é—´æ”¶é›†ï¼ŒåŒ…å«æ—¶é—´æˆ³ä¿¡æ¯",
                "æ³„æ¼": "æ¨¡å‹å¯èƒ½å­¦ä¼šäº†æ—¶é—´æ¨¡å¼è€Œä¸æ˜¯æ–‡æœ¬å†…å®¹",
                "è§£å†³æ–¹æ¡ˆ": "ç§»é™¤æˆ–éšæœºåŒ–æ—¶é—´ç›¸å…³çš„æ ‡è¯†ç¬¦"
            }
        ]

        print("=== æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­çš„æ³„æ¼ç¤ºä¾‹ ===\\n")

        for i, example in enumerate(examples, 1):
            print(f"{i}. {example['åœºæ™¯']}")
            print(f"   é—®é¢˜: {example['é—®é¢˜']}")
            print(f"   æ³„æ¼: {example['æ³„æ¼']}")
            print(f"   è§£å†³æ–¹æ¡ˆ: {example['è§£å†³æ–¹æ¡ˆ']}\\n")

        return examples

    # data_collection_leakage_examples()
    ```

    ### æ£€æµ‹æ•°æ®æ³„æ¼

    #### 1. **æ€§èƒ½å¼‚å¸¸æ£€æµ‹**

    ```python
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, roc_auc_score

    def detect_leakage_by_performance(X_train, X_test, y_train, y_test):
        \"\"\"é€šè¿‡å¼‚å¸¸é«˜çš„æ€§èƒ½æ£€æµ‹æ•°æ®æ³„æ¼\"\"\"

        # è®­ç»ƒç®€å•æ¨¡å‹
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)

        # è¯„ä¼°æ€§èƒ½
        train_pred = model.predict(X_train)
        test_pred = model.predict(X_test)

        train_accuracy = accuracy_score(y_train, train_pred)
        test_accuracy = accuracy_score(y_test, test_pred)

        print("=== æ€§èƒ½å¼‚å¸¸æ£€æµ‹ ===")
        print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
        print(f"æ€§èƒ½å·®å¼‚: {abs(train_accuracy - test_accuracy):.4f}")

        # æ³„æ¼æ£€æµ‹è§„åˆ™
        if test_accuracy > 0.95:
            print("âš ï¸  è­¦å‘Šï¼šæµ‹è¯•å‡†ç¡®ç‡å¼‚å¸¸é«˜ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼")

        if abs(train_accuracy - test_accuracy) < 0.01 and test_accuracy > 0.9:
            print("âš ï¸  è­¦å‘Šï¼šè®­ç»ƒå’Œæµ‹è¯•æ€§èƒ½è¿‡äºæ¥è¿‘ï¼Œå¯èƒ½å­˜åœ¨æ³„æ¼")

        return train_accuracy, test_accuracy
    ```

    #### 2. **ç‰¹å¾é‡è¦æ€§åˆ†æ**

    ```python
    def analyze_feature_importance_for_leakage(model, feature_names, threshold=0.5):
        \"\"\"é€šè¿‡ç‰¹å¾é‡è¦æ€§åˆ†ææ£€æµ‹æ³„æ¼\"\"\"

        # è·å–ç‰¹å¾é‡è¦æ€§
        importances = model.feature_importances_

        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        print("=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
        print("å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾:")
        print(feature_importance_df.head(10))

        # æ£€æµ‹å¼‚å¸¸é‡è¦çš„ç‰¹å¾
        max_importance = importances.max()
        if max_importance > threshold:
            dominant_feature = feature_importance_df.iloc[0]['feature']
            print(f"\\nâš ï¸  è­¦å‘Šï¼šç‰¹å¾ '{dominant_feature}' é‡è¦æ€§å¼‚å¸¸é«˜ ({max_importance:.3f})")
            print("   è¿™å¯èƒ½è¡¨æ˜å­˜åœ¨æ•°æ®æ³„æ¼")

        # æ£€æµ‹é‡è¦æ€§åˆ†å¸ƒ
        top_3_importance = feature_importance_df.head(3)['importance'].sum()
        if top_3_importance > 0.8:
            print("\\nâš ï¸  è­¦å‘Šï¼šå‰3ä¸ªç‰¹å¾å æ®äº†è¿‡é«˜çš„é‡è¦æ€§")
            print("   å»ºè®®æ£€æŸ¥è¿™äº›ç‰¹å¾æ˜¯å¦åŒ…å«ç›®æ ‡ä¿¡æ¯")

        return feature_importance_df
    ```

    #### 3. **æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥**

    ```python
    def temporal_consistency_check(data, time_column, target_column):
        \"\"\"æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥\"\"\"

        print("=== æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥ ===")

        # æ£€æŸ¥æœªæ¥ä¿¡æ¯
        data_sorted = data.sort_values(time_column)

        # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯
        suspicious_features = []

        for col in data.columns:
            if col not in [time_column, target_column]:
                # æ£€æŸ¥ç‰¹å¾å€¼æ˜¯å¦ä¸æœªæ¥ç›®æ ‡å€¼é«˜åº¦ç›¸å…³
                correlation_with_future = data_sorted[col].corr(
                    data_sorted[target_column].shift(-1)  # æœªæ¥ç›®æ ‡å€¼
                )

                if abs(correlation_with_future) > 0.7:
                    suspicious_features.append((col, correlation_with_future))

        if suspicious_features:
            print("âš ï¸  å‘ç°å¯ç–‘ç‰¹å¾ï¼ˆä¸æœªæ¥ç›®æ ‡å€¼é«˜åº¦ç›¸å…³ï¼‰:")
            for feature, corr in suspicious_features:
                print(f"   {feature}: ç›¸å…³æ€§ = {corr:.3f}")
        else:
            print("âœ… æœªå‘ç°æ˜æ˜¾çš„æ—¶é—´æ³„æ¼")

        return suspicious_features
    ```

    ### é˜²æ­¢æ•°æ®æ³„æ¼çš„æœ€ä½³å®è·µ

    #### 1. **ä¸¥æ ¼çš„æ•°æ®åˆ†å‰²æµç¨‹**

    ```python
    def leakage_safe_pipeline():
        \"\"\"é˜²æ³„æ¼çš„å®‰å…¨ç®¡é“\"\"\"

        pipeline_steps = [
            "1. æ•°æ®æ”¶é›†å’Œåˆæ­¥æ¸…ç†",
            "2. æ—©æœŸæ•°æ®åˆ†å‰²ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰",
            "3. åªåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œç‰¹å¾å·¥ç¨‹",
            "4. åªåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç»Ÿè®¡ä¿¡æ¯",
            "5. å°†è®­ç»ƒé›†çš„è½¬æ¢åº”ç”¨åˆ°éªŒè¯/æµ‹è¯•é›†",
            "6. æ¨¡å‹è®­ç»ƒï¼ˆåªä½¿ç”¨è®­ç»ƒé›†ï¼‰",
            "7. è¶…å‚æ•°è°ƒä¼˜ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰",
            "8. æœ€ç»ˆè¯„ä¼°ï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼Œåªè¯„ä¼°ä¸€æ¬¡ï¼‰"
        ]

        print("=== é˜²æ³„æ¼çš„å®‰å…¨ç®¡é“ ===")
        for step in pipeline_steps:
            print(f"  {step}")

        print("\\nå…³é”®åŸåˆ™:")
        print("  - æµ‹è¯•é›†æ˜¯ç¥åœ£çš„ï¼šæ°¸è¿œä¸è¦ç”¨äºè®­ç»ƒæˆ–è°ƒä¼˜")
        print("  - æ—¶é—´é¡ºåºï¼šå¯¹äºæ—¶é—´åºåˆ—ï¼Œä¸¥æ ¼æŒ‰æ—¶é—´åˆ†å‰²")
        print("  - ç»Ÿè®¡ä¿¡æ¯ï¼šåªä½¿ç”¨è®­ç»ƒé›†è®¡ç®—å‡å€¼ã€æ–¹å·®ç­‰")
        print("  - ç‰¹å¾å·¥ç¨‹ï¼šåœ¨åˆ†å‰²åè¿›è¡Œï¼Œé¿å…ä¿¡æ¯æ³„æ¼")

        return pipeline_steps
    ```

    #### 2. **è‡ªåŠ¨åŒ–æ³„æ¼æ£€æµ‹**

    ```python
    class LeakageDetector:
        \"\"\"è‡ªåŠ¨åŒ–æ•°æ®æ³„æ¼æ£€æµ‹å™¨\"\"\"

        def __init__(self):
            self.checks = []
            self.warnings = []

        def add_performance_check(self, train_score, test_score, threshold=0.95):
            \"\"\"æ·»åŠ æ€§èƒ½æ£€æŸ¥\"\"\"
            if test_score > threshold:
                self.warnings.append(f"æµ‹è¯•æ€§èƒ½å¼‚å¸¸é«˜: {test_score:.3f}")

            if abs(train_score - test_score) < 0.01 and test_score > 0.9:
                self.warnings.append("è®­ç»ƒå’Œæµ‹è¯•æ€§èƒ½è¿‡äºæ¥è¿‘")

        def add_feature_importance_check(self, importances, feature_names, threshold=0.5):
            \"\"\"æ·»åŠ ç‰¹å¾é‡è¦æ€§æ£€æŸ¥\"\"\"
            max_idx = np.argmax(importances)
            max_importance = importances[max_idx]

            if max_importance > threshold:
                feature_name = feature_names[max_idx]
                self.warnings.append(f"ç‰¹å¾ '{feature_name}' é‡è¦æ€§å¼‚å¸¸é«˜: {max_importance:.3f}")

        def add_temporal_check(self, data, time_col, target_col):
            \"\"\"æ·»åŠ æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥\"\"\"
            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾ååŒ…å«"future"ç­‰å…³é”®è¯
            future_keywords = ['future', 'next', 'after', 'post', 'following']

            for col in data.columns:
                if any(keyword in col.lower() for keyword in future_keywords):
                    self.warnings.append(f"ç‰¹å¾åå¯ç–‘: '{col}' å¯èƒ½åŒ…å«æœªæ¥ä¿¡æ¯")

        def generate_report(self):
            \"\"\"ç”Ÿæˆæ³„æ¼æ£€æµ‹æŠ¥å‘Š\"\"\"
            print("=== æ•°æ®æ³„æ¼æ£€æµ‹æŠ¥å‘Š ===")

            if not self.warnings:
                print("âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®æ³„æ¼è¿¹è±¡")
            else:
                print(f"âš ï¸  å‘ç° {len(self.warnings)} ä¸ªæ½œåœ¨é—®é¢˜:")
                for i, warning in enumerate(self.warnings, 1):
                    print(f"   {i}. {warning}")

            print("\\nå»ºè®®:")
            print("  - ä»”ç»†æ£€æŸ¥æ ‡è®°çš„ç‰¹å¾")
            print("  - éªŒè¯æ•°æ®åˆ†å‰²æµç¨‹")
            print("  - ç¡®è®¤ç‰¹å¾å·¥ç¨‹çš„æ—¶é—´æ­£ç¡®æ€§")
            print("  - è€ƒè™‘ä½¿ç”¨æ—¶é—´åŸºç¡€çš„éªŒè¯")

            return len(self.warnings) == 0
    ```

    æ•°æ®æ³„æ¼æ˜¯MLé¡¹ç›®å¤±è´¥çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚é€šè¿‡ç†è§£ä¸åŒç±»å‹çš„æ³„æ¼ã€å®æ–½æ£€æµ‹æœºåˆ¶å’Œéµå¾ªæœ€ä½³å®è·µï¼Œä½ å¯ä»¥æ„å»ºçœŸæ­£å¯é çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿã€‚è®°ä½ï¼š**åœ¨é¢„æµ‹æ—¶åˆ»ï¼Œåªä½¿ç”¨é‚£æ—¶çœŸæ­£å¯ç”¨çš„æ•°æ®**ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## å¤§è§„æ¨¡ç‰¹å¾å­˜å‚¨ï¼ˆFeastï¼‰

    éšç€æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æˆç†Ÿï¼Œç»„ç»‡å‘ç°è‡ªå·±åœ¨æ¨¡å‹ä¹‹é—´é‡ç”¨ç‰¹å¾ï¼Œå¹¶éœ€è¦è®­ç»ƒå’ŒæœåŠ¡çš„ä¸€è‡´æ•°æ®ã€‚ç‰¹å¾å­˜å‚¨åº”è¿è€Œç”Ÿæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

    **ç‰¹å¾å­˜å‚¨**æ˜¯MLç‰¹å¾çš„é›†ä¸­æ•°æ®å­˜å‚¨å’Œç®¡ç†ç³»ç»Ÿã€‚Feastæ˜¯ä¸€ä¸ªæµè¡Œçš„å¼€æºç‰¹å¾å­˜å‚¨ã€‚

    ![ç‰¹å¾å­˜å‚¨æ¶æ„](https://www.dailydoseofds.com/content/images/2025/08/image-180.png)

    ### ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾å­˜å‚¨ï¼Ÿ

    åœ¨ç”Ÿäº§MLç³»ç»Ÿä¸­ï¼Œä½ å¯èƒ½æœ‰è®¸å¤šæ¨¡å‹ä½¿ç”¨é‡å çš„ç‰¹å¾ã€‚å¦‚æœæ¯ä¸ªç®¡é“ç‹¬ç«‹è®¡ç®—ç‰¹å¾ï¼Œä½ ä¼šå¾—åˆ°é‡å¤ã€ä¸ä¸€è‡´å’Œç»´æŠ¤å¤´ç—›ã€‚

    **ç‰¹å¾å­˜å‚¨åˆ›å»ºäº†å•ä¸€çš„çœŸå®æ¥æº**ï¼šè®¡ç®—ç‰¹å¾Xä¸€æ¬¡ï¼Œåœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨å®ƒã€‚å®ƒè¿˜æä¾›åœ¨çº¿æŸ¥æ‰¾ï¼Œæ‰€ä»¥ä½ çš„æœåŠ¡ä»£ç å¯ä»¥ç”¨ä½å»¶è¿ŸæŒ‰é”®æŸ¥è¯¢ç‰¹å¾ã€‚è¿™é¿å…äº†åœ¨ç”Ÿäº§è¯·æ±‚ä¸­å³æ—¶é‡æ–°è®¡ç®—ç‰¹å¾ï¼Œè¿™å¯èƒ½å¤ªæ…¢æˆ–ä¸ä¸€è‡´ã€‚

    #### ç‰¹å¾å­˜å‚¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜

    ```python
    def feature_store_problems():
        \"\"\"ç‰¹å¾å­˜å‚¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜\"\"\"

        problems = {
            "ç‰¹å¾é‡å¤è®¡ç®—": {
                "é—®é¢˜": "å¤šä¸ªå›¢é˜Ÿé‡å¤å®ç°ç›¸åŒçš„ç‰¹å¾é€»è¾‘",
                "åæœ": "æµªè´¹è®¡ç®—èµ„æºï¼Œç»´æŠ¤æˆæœ¬é«˜ï¼Œç»“æœä¸ä¸€è‡´",
                "è§£å†³æ–¹æ¡ˆ": "é›†ä¸­åŒ–ç‰¹å¾å®šä¹‰å’Œè®¡ç®—"
            },

            "è®­ç»ƒ-æœåŠ¡åå·®": {
                "é—®é¢˜": "è®­ç»ƒæ—¶çš„ç‰¹å¾è®¡ç®—ä¸æœåŠ¡æ—¶ä¸åŒ",
                "åæœ": "æ¨¡å‹åœ¨ç”Ÿäº§ä¸­æ€§èƒ½ä¸‹é™",
                "è§£å†³æ–¹æ¡ˆ": "ç»Ÿä¸€çš„ç‰¹å¾å®šä¹‰ç¡®ä¿ä¸€è‡´æ€§"
            },

            "ç‰¹å¾å‘ç°å›°éš¾": {
                "é—®é¢˜": "å›¢é˜Ÿä¸çŸ¥é“å·²æœ‰å“ªäº›ç‰¹å¾å¯ç”¨",
                "åæœ": "é‡å¤å·¥ä½œï¼Œé”™å¤±æœ‰ä»·å€¼çš„ç‰¹å¾",
                "è§£å†³æ–¹æ¡ˆ": "ç‰¹å¾æ³¨å†Œè¡¨å’Œæ–‡æ¡£"
            },

            "æ•°æ®æ³„æ¼é£é™©": {
                "é—®é¢˜": "ç‰¹å¾è®¡ç®—ä¸­ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯",
                "åæœ": "æ¨¡å‹åœ¨ç”Ÿäº§ä¸­å¤±æ•ˆ",
                "è§£å†³æ–¹æ¡ˆ": "æ—¶é—´ç‚¹æ­£ç¡®æ€§ä¿è¯"
            },

            "å®æ—¶ç‰¹å¾æœåŠ¡": {
                "é—®é¢˜": "ç”Ÿäº§ç¯å¢ƒéœ€è¦ä½å»¶è¿Ÿç‰¹å¾æŸ¥è¯¢",
                "åæœ": "ç”¨æˆ·ä½“éªŒå·®ï¼Œç³»ç»Ÿå“åº”æ…¢",
                "è§£å†³æ–¹æ¡ˆ": "åœ¨çº¿ç‰¹å¾å­˜å‚¨"
            }
        }

        print("=== ç‰¹å¾å­˜å‚¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜ ===\\n")

        for problem, details in problems.items():
            print(f"ğŸ“‹ {problem}")
            print(f"   é—®é¢˜: {details['é—®é¢˜']}")
            print(f"   åæœ: {details['åæœ']}")
            print(f"   è§£å†³æ–¹æ¡ˆ: {details['è§£å†³æ–¹æ¡ˆ']}\\n")

        return problems

    # feature_store_problems()
    ```

    ### Feastæ¶æ„æ¦‚è¿°

    Feastå°†ç‰¹å¾è®¡ç®—å’Œå­˜å‚¨è§£è€¦ã€‚æˆ‘ä»¬åœ¨ç‰¹å¾ä»“åº“ä¸­å®šä¹‰ç‰¹å¾ã€‚

    #### æ ¸å¿ƒæ¦‚å¿µ

    ```python
    def feast_core_concepts():
        \"\"\"Feastæ ¸å¿ƒæ¦‚å¿µè§£é‡Š\"\"\"

        concepts = {
            "Entityï¼ˆå®ä½“ï¼‰": {
                "å®šä¹‰": "ç‰¹å¾çš„ä¸»é”®ï¼Œå…·æœ‰å€¼ç±»å‹",
                "ç¤ºä¾‹": "customer_id, product_id, user_id",
                "ä½œç”¨": "ç”¨äºç‰¹å¾æŸ¥è¯¢å’Œè¿æ¥çš„å”¯ä¸€æ ‡è¯†ç¬¦"
            },

            "Feature Viewï¼ˆç‰¹å¾è§†å›¾ï¼‰": {
                "å®šä¹‰": "ä¸€ç»„ç‰¹å¾çš„å®šä¹‰ï¼ˆå…·æœ‰ç‰¹å®šæ¨¡å¼å’Œå®ä½“ï¼‰ä»¥åŠå¦‚ä½•è·å–æ•°æ®",
                "ç¤ºä¾‹": "ç”¨æˆ·è¡Œä¸ºç‰¹å¾ã€äº§å“ç»Ÿè®¡ç‰¹å¾",
                "ä½œç”¨": "å®šä¹‰ç‰¹å¾çš„è®¡ç®—é€»è¾‘å’Œæ•°æ®æº"
            },

            "Offline Storeï¼ˆç¦»çº¿å­˜å‚¨ï¼‰": {
                "å®šä¹‰": "å­˜å‚¨å†å²ç‰¹å¾æ•°æ®çš„åœ°æ–¹",
                "ç¤ºä¾‹": "BigQuery, Redshift, æ–‡ä»¶ç³»ç»Ÿ",
                "ä½œç”¨": "ç”¨äºè®­ç»ƒæ•°æ®ç”Ÿæˆå’Œæ‰¹é‡ç‰¹å¾è®¡ç®—"
            },

            "Online Storeï¼ˆåœ¨çº¿å­˜å‚¨ï¼‰": {
                "å®šä¹‰": "ç”¨äºæœåŠ¡ç‰¹å¾ç»™æ¨¡å‹çš„å¿«é€Ÿé”®å€¼å­˜å‚¨",
                "ç¤ºä¾‹": "Redis, DynamoDB, SQLite",
                "ä½œç”¨": "ä½å»¶è¿Ÿçš„å®æ—¶ç‰¹å¾æŸ¥è¯¢"
            },

            "Feature Serviceï¼ˆç‰¹å¾æœåŠ¡ï¼‰": {
                "å®šä¹‰": "ä¸ºæ–¹ä¾¿æ£€ç´¢è€Œåˆ†ç»„çš„ç‰¹å¾",
                "ç¤ºä¾‹": "æ¨èç³»ç»Ÿç‰¹å¾åŒ…ã€é£æ§ç‰¹å¾åŒ…",
                "ä½œç”¨": "ç®€åŒ–ç‰¹å¾æŸ¥è¯¢å’Œç®¡ç†"
            }
        }

        print("=== Feastæ ¸å¿ƒæ¦‚å¿µ ===\\n")

        for concept, details in concepts.items():
            print(f"ğŸ”§ {concept}")
            print(f"   å®šä¹‰: {details['å®šä¹‰']}")
            print(f"   ç¤ºä¾‹: {details['ç¤ºä¾‹']}")
            print(f"   ä½œç”¨: {details['ä½œç”¨']}\\n")

        return concepts

    # feast_core_concepts()
    ```

    ### Feastå·¥ä½œæµç¨‹

    ```python
    def feast_workflow():
        \"\"\"Feastå…¸å‹å·¥ä½œæµç¨‹\"\"\"

        workflow_steps = [
            {
                "æ­¥éª¤": "1. å®šä¹‰ç‰¹å¾",
                "æè¿°": "åœ¨Pythonä»£ç ä¸­å®šä¹‰å®ä½“ã€ç‰¹å¾è§†å›¾å’Œæ•°æ®æº",
                "ä»£ç ç¤ºä¾‹": '''
    from feast import Entity, FeatureView, Field
    from feast.types import Float32, Int64

    # å®šä¹‰å®ä½“
    customer = Entity(name="customer_id", value_type=ValueType.INT64)

    # å®šä¹‰ç‰¹å¾è§†å›¾
    customer_features = FeatureView(
        name="customer_features",
        entities=["customer_id"],
        schema=[
            Field(name="age", dtype=Float32),
            Field(name="income", dtype=Float32),
        ],
        source=FileSource(path="customer_data.parquet")
    )
                '''
            },

            {
                "æ­¥éª¤": "2. åº”ç”¨é…ç½®",
                "æè¿°": "å°†ç‰¹å¾å®šä¹‰éƒ¨ç½²åˆ°Feastæ³¨å†Œè¡¨",
                "ä»£ç ç¤ºä¾‹": "feast apply"
            },

            {
                "æ­¥éª¤": "3. ç‰©åŒ–ç‰¹å¾",
                "æè¿°": "å°†ç‰¹å¾æ•°æ®ä»ç¦»çº¿å­˜å‚¨åŠ è½½åˆ°åœ¨çº¿å­˜å‚¨",
                "ä»£ç ç¤ºä¾‹": "feast materialize-incremental $(date)"
            },

            {
                "æ­¥éª¤": "4. ç”Ÿæˆè®­ç»ƒæ•°æ®",
                "æè¿°": "åˆ›å»ºæ—¶é—´ç‚¹æ­£ç¡®çš„è®­ç»ƒæ•°æ®é›†",
                "ä»£ç ç¤ºä¾‹": '''
    training_df = store.get_historical_features(
        entity_df=entity_df,
        features=["customer_features:age", "customer_features:income"]
    ).to_df()
                '''
            },

            {
                "æ­¥éª¤": "5. åœ¨çº¿ç‰¹å¾æœåŠ¡",
                "æè¿°": "åœ¨ç”Ÿäº§ä¸­æŸ¥è¯¢å®æ—¶ç‰¹å¾",
                "ä»£ç ç¤ºä¾‹": '''
    online_features = store.get_online_features(
        features=["customer_features:age", "customer_features:income"],
        entity_rows=[{"customer_id": 123}]
    ).to_dict()
                '''
            }
        ]

        print("=== Feastå…¸å‹å·¥ä½œæµç¨‹ ===\\n")

        for step in workflow_steps:
            print(f"ğŸ“‹ {step['æ­¥éª¤']}")
            print(f"   æè¿°: {step['æè¿°']}")
            if 'feast' in step['ä»£ç ç¤ºä¾‹'] and len(step['ä»£ç ç¤ºä¾‹']) < 50:
                print(f"   å‘½ä»¤: {step['ä»£ç ç¤ºä¾‹']}")
            else:
                print(f"   ä»£ç ç¤ºä¾‹:")
                print(f"   ```python{step['ä»£ç ç¤ºä¾‹']}   ```")
            print()

        return workflow_steps

    # feast_workflow()
    ```

    ### å®é™…Feastå®ç°ç¤ºä¾‹

    è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„å®¢æˆ·æµå¤±é¢„æµ‹ç¤ºä¾‹æ¥æ¼”ç¤ºFeastçš„ä½¿ç”¨ï¼š

    ```python
    # æ³¨æ„ï¼šè¿™æ˜¯æ¼”ç¤ºä»£ç ï¼Œå®é™…è¿è¡Œéœ€è¦å®‰è£…Feast
    def feast_implementation_example():
        \"\"\"Feastå®ç°ç¤ºä¾‹ï¼ˆæ¼”ç¤ºä»£ç ï¼‰\"\"\"

        print("=== Feastå®ç°ç¤ºä¾‹ ===\\n")

        # 1. å®‰è£…å’Œè®¾ç½®
        setup_code = '''
    # å®‰è£…Feast
    pip install feast[redis]

    # åˆå§‹åŒ–Feasté¡¹ç›®
    feast init customer_churn_project
    cd customer_churn_project
        '''

        print("1. é¡¹ç›®è®¾ç½®:")
        print(setup_code)

        # 2. å®šä¹‰ç‰¹å¾
        feature_definition = '''
    # feature_store.py
    from feast import Entity, FeatureView, Field, FileSource, FeatureStore
    from feast.types import Float32, Int64, String
    from datetime import timedelta

    # å®šä¹‰å®ä½“
    customer = Entity(
        name="customer_id",
        value_type=ValueType.STRING,
        description="å®¢æˆ·å”¯ä¸€æ ‡è¯†ç¬¦"
    )

    # å®šä¹‰å®¢æˆ·åŸºç¡€ç‰¹å¾è§†å›¾
    customer_demographics = FeatureView(
        name="customer_demographics",
        entities=["customer_id"],
        ttl=timedelta(days=365),
        schema=[
            Field(name="age", dtype=Int64),
            Field(name="gender", dtype=String),
            Field(name="income", dtype=Float32),
            Field(name="tenure_months", dtype=Int64),
        ],
        source=FileSource(
            path="data/customer_demographics.parquet",
            timestamp_field="event_timestamp"
        ),
        tags={"team": "ml", "domain": "customer"}
    )

    # å®šä¹‰å®¢æˆ·è¡Œä¸ºç‰¹å¾è§†å›¾
    customer_behavior = FeatureView(
        name="customer_behavior",
        entities=["customer_id"],
        ttl=timedelta(days=30),
        schema=[
            Field(name="monthly_charges", dtype=Float32),
            Field(name="total_charges", dtype=Float32),
            Field(name="support_calls_30d", dtype=Int64),
            Field(name="login_frequency_30d", dtype=Int64),
        ],
        source=FileSource(
            path="data/customer_behavior.parquet",
            timestamp_field="event_timestamp"
        ),
        tags={"team": "ml", "domain": "behavior"}
    )

    # å®šä¹‰ç‰¹å¾æœåŠ¡
    churn_prediction_service = FeatureService(
        name="churn_prediction",
        features=[
            "customer_demographics:age",
            "customer_demographics:gender",
            "customer_demographics:income",
            "customer_demographics:tenure_months",
            "customer_behavior:monthly_charges",
            "customer_behavior:total_charges",
            "customer_behavior:support_calls_30d",
            "customer_behavior:login_frequency_30d",
        ]
    )
        '''

        print("2. ç‰¹å¾å®šä¹‰:")
        print(feature_definition)

        # 3. è®­ç»ƒæ•°æ®ç”Ÿæˆ
        training_code = '''
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    from feast import FeatureStore
    import pandas as pd

    store = FeatureStore(repo_path=".")

    # å‡†å¤‡å®ä½“DataFrameï¼ˆåŒ…å«å®¢æˆ·IDã€æ—¶é—´æˆ³å’Œæ ‡ç­¾ï¼‰
    entity_df = pd.DataFrame({
        "customer_id": ["CUST_001", "CUST_002", "CUST_003"],
        "event_timestamp": pd.to_datetime([
            "2023-01-01", "2023-01-02", "2023-01-03"
        ]),
        "churn_label": [0, 1, 0]  # ç›®æ ‡å˜é‡
    })

    # è·å–å†å²ç‰¹å¾ï¼ˆæ—¶é—´ç‚¹æ­£ç¡®ï¼‰
    training_df = store.get_historical_features(
        entity_df=entity_df,
        features=[
            "customer_demographics:age",
            "customer_demographics:income",
            "customer_behavior:monthly_charges",
            "customer_behavior:support_calls_30d"
        ]
    ).to_df()

    print("è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ")
    print(training_df.head())
        '''

        print("3. è®­ç»ƒæ•°æ®ç”Ÿæˆ:")
        print(training_code)

        # 4. åœ¨çº¿æœåŠ¡
        serving_code = '''
    # ç‰©åŒ–ç‰¹å¾åˆ°åœ¨çº¿å­˜å‚¨
    from datetime import datetime

    # ç‰©åŒ–æœ€æ–°ç‰¹å¾
    store.materialize_incremental(end_date=datetime.now())

    # åœ¨çº¿ç‰¹å¾æŸ¥è¯¢
    online_features = store.get_online_features(
        features=[
            "customer_demographics:age",
            "customer_demographics:income",
            "customer_behavior:monthly_charges",
            "customer_behavior:support_calls_30d"
        ],
        entity_rows=[
            {"customer_id": "CUST_001"},
            {"customer_id": "CUST_002"}
        ]
    )

    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    feature_dict = online_features.to_dict()
    print("åœ¨çº¿ç‰¹å¾æŸ¥è¯¢ç»“æœ:")
    print(feature_dict)
        '''

        print("4. åœ¨çº¿ç‰¹å¾æœåŠ¡:")
        print(serving_code)

        return {
            'setup': setup_code,
            'features': feature_definition,
            'training': training_code,
            'serving': serving_code
        }

    # feast_implementation_example()
    ```

    ### Feastçš„ä¼˜åŠ¿

    ```python
    def feast_advantages():
        \"\"\"Feastçš„ä¸»è¦ä¼˜åŠ¿\"\"\"

        advantages = {
            "æ—¶é—´ç‚¹æ­£ç¡®æ€§": {
                "æè¿°": "ç¡®ä¿è®­ç»ƒæ•°æ®ä¸­çš„ç‰¹å¾å€¼æ˜¯å†å²ä¸Šè¯¥æ—¶é—´ç‚¹çœŸå®å¯ç”¨çš„",
                "ä»·å€¼": "é˜²æ­¢æ•°æ®æ³„æ¼ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”Ÿäº§ä¸­çš„å¯é æ€§",
                "å®ç°": "è‡ªåŠ¨å¤„ç†æ—¶é—´æˆ³ï¼Œç¡®ä¿ç‰¹å¾-æ ‡ç­¾å¯¹é½"
            },

            "è®­ç»ƒ-æœåŠ¡ä¸€è‡´æ€§": {
                "æè¿°": "è®­ç»ƒå’ŒæœåŠ¡ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾å®šä¹‰å’Œè®¡ç®—é€»è¾‘",
                "ä»·å€¼": "æ¶ˆé™¤è®­ç»ƒ-æœåŠ¡åå·®ï¼Œæé«˜æ¨¡å‹ç”Ÿäº§æ€§èƒ½",
                "å®ç°": "ç»Ÿä¸€çš„ç‰¹å¾è§†å›¾å®šä¹‰"
            },

            "ç‰¹å¾é‡ç”¨": {
                "æè¿°": "ä¸€æ¬¡å®šä¹‰ï¼Œå¤šå¤„ä½¿ç”¨çš„ç‰¹å¾ç®¡ç†",
                "ä»·å€¼": "å‡å°‘é‡å¤å·¥ä½œï¼Œæé«˜å¼€å‘æ•ˆç‡",
                "å®ç°": "é›†ä¸­åŒ–çš„ç‰¹å¾æ³¨å†Œè¡¨"
            },

            "å¯æ‰©å±•æ€§": {
                "æè¿°": "æ”¯æŒä»å°è§„æ¨¡åˆ°ä¼ä¸šçº§çš„ç‰¹å¾ç®¡ç†",
                "ä»·å€¼": "éšä¸šåŠ¡å¢é•¿è€Œæ‰©å±•",
                "å®ç°": "çµæ´»çš„å­˜å‚¨åç«¯é€‰æ‹©"
            },

            "ç‰ˆæœ¬æ§åˆ¶": {
                "æè¿°": "ç‰¹å¾å®šä¹‰çš„ç‰ˆæœ¬ç®¡ç†å’Œå›æ»šèƒ½åŠ›",
                "ä»·å€¼": "æ”¯æŒå®éªŒå’Œå®‰å…¨éƒ¨ç½²",
                "å®ç°": "Gité›†æˆå’Œç‰¹å¾ç‰ˆæœ¬è·Ÿè¸ª"
            }
        }

        print("=== Feastçš„ä¸»è¦ä¼˜åŠ¿ ===\\n")

        for advantage, details in advantages.items():
            print(f"ğŸš€ {advantage}")
            print(f"   æè¿°: {details['æè¿°']}")
            print(f"   ä»·å€¼: {details['ä»·å€¼']}")
            print(f"   å®ç°: {details['å®ç°']}\\n")

        return advantages

    # feast_advantages()
    ```

    ### ç‰¹å¾å­˜å‚¨æœ€ä½³å®è·µ

    ```python
    def feature_store_best_practices():
        \"\"\"ç‰¹å¾å­˜å‚¨æœ€ä½³å®è·µ\"\"\"

        practices = {
            "ç‰¹å¾å‘½åè§„èŒƒ": [
                "ä½¿ç”¨æè¿°æ€§åç§°ï¼šcustomer_age_years è€Œä¸æ˜¯ age",
                "åŒ…å«æ—¶é—´çª—å£ï¼špurchases_30d, clicks_7d",
                "ä½¿ç”¨ä¸€è‡´çš„å‘½åçº¦å®šï¼šsnake_case",
                "é¿å…ç¼©å†™ï¼šmonthly_revenue è€Œä¸æ˜¯ mon_rev"
            ],

            "ç‰¹å¾ç»„ç»‡": [
                "æŒ‰ä¸šåŠ¡åŸŸåˆ†ç»„ï¼šcustomer_features, product_features",
                "æŒ‰æ›´æ–°é¢‘ç‡åˆ†ç»„ï¼šdaily_features, realtime_features",
                "æŒ‰æ•°æ®æºåˆ†ç»„ï¼šdatabase_features, api_features",
                "ä½¿ç”¨æ ‡ç­¾è¿›è¡Œåˆ†ç±»å’Œæœç´¢"
            ],

            "æ•°æ®è´¨é‡": [
                "å®æ–½ç‰¹å¾éªŒè¯ï¼šæ•°æ®ç±»å‹ã€èŒƒå›´æ£€æŸ¥",
                "ç›‘æ§ç‰¹å¾åˆ†å¸ƒå˜åŒ–ï¼šæ•°æ®æ¼‚ç§»æ£€æµ‹",
                "è®¾ç½®æ•°æ®è´¨é‡è­¦æŠ¥ï¼šç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼",
                "å®šæœŸå®¡æŸ¥ç‰¹å¾ä½¿ç”¨æƒ…å†µ"
            ],

            "æ€§èƒ½ä¼˜åŒ–": [
                "é€‰æ‹©åˆé€‚çš„TTLï¼šå¹³è¡¡æ–°é²œåº¦å’Œå­˜å‚¨æˆæœ¬",
                "ä¼˜åŒ–æ‰¹å¤„ç†çª—å£ï¼šå‡å°‘è®¡ç®—å¼€é”€",
                "ä½¿ç”¨é€‚å½“çš„åˆ†åŒºç­–ç•¥ï¼šæé«˜æŸ¥è¯¢æ€§èƒ½",
                "ç›‘æ§å­˜å‚¨å’Œè®¡ç®—æˆæœ¬"
            ],

            "å®‰å…¨å’Œæ²»ç†": [
                "å®æ–½è®¿é—®æ§åˆ¶ï¼šåŸºäºè§’è‰²çš„ç‰¹å¾è®¿é—®",
                "æ•°æ®è¡€ç¼˜è·Ÿè¸ªï¼šäº†è§£ç‰¹å¾æ¥æºå’Œä¾èµ–",
                "åˆè§„æ€§æ£€æŸ¥ï¼šç¡®ä¿ç¬¦åˆæ•°æ®ä¿æŠ¤æ³•è§„",
                "å®¡è®¡æ—¥å¿—ï¼šè·Ÿè¸ªç‰¹å¾ä½¿ç”¨å’Œä¿®æ”¹"
            ]
        }

        print("=== ç‰¹å¾å­˜å‚¨æœ€ä½³å®è·µ ===\\n")

        for category, items in practices.items():
            print(f"ğŸ“‹ {category}")
            for item in items:
                print(f"   â€¢ {item}")
            print()

        return practices

    # feature_store_best_practices()
    ```

    ç‰¹å¾å­˜å‚¨å¦‚Feastä¸ºMLç³»ç»Ÿå¸¦æ¥äº†ç»„ç»‡æ€§å’Œå¯é æ€§ã€‚å®ƒä»¬ç¡®ä¿åˆ›å»ºç‰¹å¾çš„è¾›å‹¤å·¥ä½œä¸ä¼šåœ¨è®­ç»ƒå’ŒæœåŠ¡ä¹‹é—´è¢«é‡å¤æˆ–æŸåã€‚é€šè¿‡æä¾›æ—¶é—´ç‚¹æ­£ç¡®æ€§ã€è®­ç»ƒ-æœåŠ¡ä¸€è‡´æ€§å’Œç‰¹å¾é‡ç”¨ï¼Œç‰¹å¾å­˜å‚¨æˆä¸ºç°ä»£MLOpsæ¶æ„çš„å…³é”®ç»„ä»¶ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ç»“è®º

    åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å…³æ³¨ç”Ÿäº§å°±ç»ªMLå·¥ä½œæµçš„å››ä¸ªå…³é”®æ¦‚å¿µï¼Œæ‰©å±•äº†å¯¹æ•°æ®ç®¡é“çš„æ¢ç´¢ï¼š**é‡‡æ ·ç­–ç•¥ã€ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ã€æ•°æ®æ³„æ¼é˜²æŠ¤å’Œç‰¹å¾å­˜å‚¨**ã€‚

    ### ğŸ¯ **å…³é”®å­¦ä¹ æˆæœ**

    #### ğŸ“Š **é‡‡æ ·ç­–ç•¥æŒæ¡**
    æˆ‘ä»¬ç ”ç©¶äº†é‡‡æ ·æŠ€æœ¯çš„å…¨è°±ï¼ŒåŒ…æ‹¬æ¦‚ç‡å’Œéæ¦‚ç‡æ–¹æ³•ï¼š

    - **æ¦‚ç‡é‡‡æ ·**ï¼šç®€å•éšæœºã€åˆ†å±‚ã€ç³»ç»Ÿå’Œèšç±»é‡‡æ ·çš„åŸç†å’Œåº”ç”¨
    - **éæ¦‚ç‡é‡‡æ ·**ï¼šä¾¿åˆ©ã€é›ªçƒå’Œåˆ¤æ–­é‡‡æ ·çš„ä½¿ç”¨åœºæ™¯å’Œé™åˆ¶
    - **MLç‰¹å®šè€ƒè™‘**ï¼šæ—¶é—´åºåˆ—æ•°æ®çš„æ—¶é—´åŸºç¡€åˆ†å‰²ã€å¤§æ•°æ®çš„åˆ†å—é‡‡æ ·
    - **è´¨é‡è¯„ä¼°**ï¼šé‡‡æ ·è´¨é‡çš„é‡åŒ–è¯„ä¼°æ–¹æ³•

    **æ ¸å¿ƒæ´å¯Ÿ**ï¼šæ­£ç¡®çš„é‡‡æ ·ç­–ç•¥æ˜¯æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŸºç¡€ï¼Œé”™è¯¯çš„é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸¥é‡çš„åå·®å’Œç”Ÿäº§å¤±è´¥ã€‚

    #### âš–ï¸ **ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ç²¾é€š**
    æˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†æ•°æ®å’Œç®—æ³•å±‚é¢çš„æ–¹æ³•ï¼Œå¦‚é‡é‡‡æ ·ã€ç±»åˆ«æƒé‡å’Œç„¦ç‚¹æŸå¤±ï¼Œæ„å»ºä¼˜å…ˆè€ƒè™‘çœŸå®ä¸–ç•Œå½±å“è€Œéè¡¨é¢å‡†ç¡®ç‡çš„æ¨¡å‹ï¼š

    - **æ•°æ®å±‚é¢æ–¹æ³•**ï¼š
      - è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEã€ADASYNï¼‰çš„åŸç†å’Œå®ç°
      - æ¬ é‡‡æ ·æ–¹æ³•ï¼ˆéšæœºã€Tomek Linksã€ENNï¼‰çš„åº”ç”¨
      - æ··åˆé‡‡æ ·ç­–ç•¥çš„ä¼˜åŠ¿

    - **ç®—æ³•å±‚é¢æ–¹æ³•**ï¼š
      - ç±»åˆ«æƒé‡è°ƒæ•´çš„æ•°å­¦åŸç†
      - é˜ˆå€¼ä¼˜åŒ–æŠ€æœ¯
      - ä¸“é—¨çš„é›†æˆæ–¹æ³•

    - **è¯„ä¼°ç­–ç•¥**ï¼šè¶…è¶Šå‡†ç¡®ç‡çš„ç»¼åˆè¯„ä¼°æŒ‡æ ‡ä½“ç³»

    **æ ¸å¿ƒæ´å¯Ÿ**ï¼šç±»åˆ«ä¸å¹³è¡¡æ˜¯ç°å®ä¸–ç•Œçš„å¸¸æ€ï¼Œéœ€è¦ä¸“é—¨çš„æŠ€æœ¯å’Œè¯„ä¼°æ–¹æ³•æ¥ç¡®ä¿æ¨¡å‹çš„å®ç”¨æ€§ã€‚

    #### ğŸ›¡ï¸ **æ•°æ®æ³„æ¼é˜²æŠ¤ä¸“ä¸šçŸ¥è¯†**
    æˆ‘ä»¬å‰–æäº†æ•°æ®æ³„æ¼çš„é™·é˜±ï¼Œç†è§£äº†å¾®å¦™çš„ç–å¿½å¦‚ä½•ä½¿æ•´ä¸ªç®¡é“å¤±æ•ˆï¼š

    - **æ³„æ¼ç±»å‹è¯†åˆ«**ï¼š
      - æ—¶é—´æ³„æ¼ï¼šä½¿ç”¨æœªæ¥ä¿¡æ¯çš„å±é™©
      - ç›®æ ‡æ³„æ¼ï¼šç‰¹å¾ä¸­åŒ…å«ç›®æ ‡ä¿¡æ¯
      - è®­ç»ƒ-æµ‹è¯•æ±¡æŸ“ï¼šé¢„å¤„ç†é¡ºåºçš„é‡è¦æ€§
      - æ•°æ®æ”¶é›†æ³„æ¼ï¼šæºå¤´åå·®çš„å½±å“

    - **æ£€æµ‹æœºåˆ¶**ï¼š
      - æ€§èƒ½å¼‚å¸¸æ£€æµ‹ï¼šè¯†åˆ«"å¤ªå¥½"çš„ç»“æœ
      - ç‰¹å¾é‡è¦æ€§åˆ†æï¼šå‘ç°å¯ç–‘çš„ä¸»å¯¼ç‰¹å¾
      - æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥ï¼šéªŒè¯æ—¶é—´é€»è¾‘
      - è‡ªåŠ¨åŒ–æ£€æµ‹ç³»ç»Ÿï¼šæŒç»­ç›‘æ§

    - **é˜²æŠ¤æœ€ä½³å®è·µ**ï¼šä¸¥æ ¼çš„æ•°æ®åˆ†å‰²æµç¨‹å’Œ"é¢„æµ‹æ—¶åˆ»æ­£ç¡®æ€§"åŸåˆ™

    **æ ¸å¿ƒæ´å¯Ÿ**ï¼šæ•°æ®æ³„æ¼æ˜¯MLé¡¹ç›®å¤±è´¥çš„éšå½¢æ€æ‰‹ï¼Œé¢„é˜²èƒœäºæ²»ç–—ï¼Œç³»ç»Ÿæ€§çš„æ£€æµ‹å’Œé˜²æŠ¤æœºåˆ¶æ˜¯å¿…éœ€çš„ã€‚

    #### ğŸ—ï¸ **ç‰¹å¾å­˜å‚¨æ¶æ„ç†è§£**
    æˆ‘ä»¬å°†è¿™äº›ç»éªŒæ•™è®­é›†ä¸­åœ¨Feastçš„å®é™…æ¼”ç»ƒä¸­ï¼Œå±•ç¤ºäº†ç‰¹å¾å­˜å‚¨å¦‚ä½•ä½œä¸ºé˜²æ³„æ¼ã€ä¸€è‡´å’Œå¯æ‰©å±•MLç®¡é“çš„éª¨å¹²ï¼š

    - **æ¶æ„ç»„ä»¶**ï¼š
      - å®ä½“ã€ç‰¹å¾è§†å›¾ã€ç¦»çº¿/åœ¨çº¿å­˜å‚¨çš„ä½œç”¨
      - ç‰¹å¾æœåŠ¡çš„ç»„ç»‡å’Œç®¡ç†
      - æ—¶é—´ç‚¹æ­£ç¡®æ€§çš„æŠ€æœ¯å®ç°

    - **å·¥ä½œæµç¨‹**ï¼š
      - ä»ç‰¹å¾å®šä¹‰åˆ°ç”Ÿäº§æœåŠ¡çš„å®Œæ•´æµç¨‹
      - è®­ç»ƒæ•°æ®ç”Ÿæˆçš„æœ€ä½³å®è·µ
      - åœ¨çº¿ç‰¹å¾æŸ¥è¯¢çš„ä¼˜åŒ–

    - **ä¼ä¸šä»·å€¼**ï¼š
      - ç‰¹å¾é‡ç”¨å’Œä¸€è‡´æ€§ä¿è¯
      - å¼€å‘æ•ˆç‡çš„æ˜¾è‘—æå‡
      - è®­ç»ƒ-æœåŠ¡åå·®çš„æ¶ˆé™¤

    **æ ¸å¿ƒæ´å¯Ÿ**ï¼šç‰¹å¾å­˜å‚¨ä¸ä»…æ˜¯æŠ€æœ¯å·¥å…·ï¼Œæ›´æ˜¯ç»„ç»‡MLèƒ½åŠ›çš„æˆ˜ç•¥èµ„äº§ã€‚

    ### ğŸ’¡ **ç³»ç»Ÿæ€§æ€ç»´çš„ä½“ç°**

    #### ğŸ”„ **ç«¯åˆ°ç«¯ä¸€è‡´æ€§**
    æœ¬ç« å¼ºè°ƒäº†MLç³»ç»Ÿä¸­å„ä¸ªç»„ä»¶çš„ç›¸äº’ä¾èµ–æ€§ï¼š

    - **é‡‡æ ·å†³ç­–**å½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
    - **ä¸å¹³è¡¡å¤„ç†**å½±å“ä¸šåŠ¡ä»·å€¼çš„å®ç°
    - **æ³„æ¼é˜²æŠ¤**å½±å“ç”Ÿäº§å¯é æ€§
    - **ç‰¹å¾å­˜å‚¨**å½±å“æ•´ä½“ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§

    #### ğŸ“ˆ **è´¨é‡ä¼˜å…ˆçš„ç†å¿µ**
    æˆ‘ä»¬çœ‹åˆ°äº†æ•°æ®è´¨é‡å¦‚ä½•è´¯ç©¿æ•´ä¸ªMLç”Ÿå‘½å‘¨æœŸï¼š

    - **é‡‡æ ·è´¨é‡**å†³å®šäº†æ•°æ®çš„ä»£è¡¨æ€§
    - **å¹³è¡¡å¤„ç†**ç¡®ä¿äº†æ¨¡å‹çš„å…¬å¹³æ€§
    - **æ³„æ¼é˜²æŠ¤**ä¿è¯äº†ç»“æœçš„çœŸå®æ€§
    - **ç‰¹å¾ç®¡ç†**ç»´æŠ¤äº†ç³»ç»Ÿçš„ä¸€è‡´æ€§

    #### ğŸ¯ **ä¸šåŠ¡ä»·å€¼å¯¼å‘**
    æ¯ä¸ªæŠ€æœ¯å†³ç­–éƒ½ä¸ä¸šåŠ¡æˆæœç´§å¯†ç›¸å…³ï¼š

    - æ­£ç¡®çš„é‡‡æ ·ç­–ç•¥ç¡®ä¿æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­æœ‰æ•ˆ
    - é€‚å½“çš„ä¸å¹³è¡¡å¤„ç†ä¼˜åŒ–ä¸šåŠ¡å…³é”®æŒ‡æ ‡
    - ä¸¥æ ¼çš„æ³„æ¼é˜²æŠ¤é¿å…ç”Ÿäº§ç¾éš¾
    - é«˜æ•ˆçš„ç‰¹å¾ç®¡ç†åŠ é€Ÿäº§å“è¿­ä»£

    ### ğŸš€ **å…³é”®è¦ç‚¹æ€»ç»“**

    **æœ¬ç« çš„å…³é”®è¦ç‚¹æ˜¯ï¼šæ•°æ®è®¾è®¡é€‰æ‹©â€”â€”é‡‡æ ·ã€ä¸å¹³è¡¡å¤„ç†å’Œæ³„æ¼é˜²æŠ¤â€”â€”ä¸æ¨¡å‹æœ¬èº«ä¸€æ ·å…³é”®ã€‚** åƒFeastè¿™æ ·çš„ç‰¹å¾å­˜å‚¨å°†è¿™äº›å®è·µä»ä¸´æ—¶ä¿®å¤æå‡ä¸ºç³»ç»Ÿçº§ä¿è¯ï¼Œç¡®ä¿å¤§è§„æ¨¡çš„å¯é MLã€‚

    è¿™ä¸ªåŸåˆ™ä½“ç°åœ¨å‡ ä¸ªå±‚é¢ï¼š

    #### ğŸ”§ **æŠ€æœ¯å±‚é¢**
    - ç³»ç»ŸåŒ–çš„æ–¹æ³•è®ºèƒœè¿‡ä¸´æ—¶çš„è§£å†³æ–¹æ¡ˆ
    - è‡ªåŠ¨åŒ–çš„æ£€æµ‹å’Œé˜²æŠ¤æœºåˆ¶æ˜¯å¿…éœ€çš„
    - å·¥å…·å’Œæµç¨‹çš„æ ‡å‡†åŒ–æé«˜äº†å¯é æ€§

    #### ğŸ‘¥ **ç»„ç»‡å±‚é¢**
    - è·¨å›¢é˜Ÿçš„ç‰¹å¾å…±äº«æé«˜äº†æ•ˆç‡
    - ç»Ÿä¸€çš„æœ€ä½³å®è·µå‡å°‘äº†é”™è¯¯
    - çŸ¥è¯†çš„ç³»ç»ŸåŒ–ä¼ æ‰¿åŠ é€Ÿäº†å›¢é˜Ÿæˆé•¿

    #### ğŸ’¼ **ä¸šåŠ¡å±‚é¢**
    - å¯é çš„MLç³»ç»Ÿæ”¯æ’‘ä¸šåŠ¡å†³ç­–
    - é«˜è´¨é‡çš„æ•°æ®ç®¡é“åˆ›é€ ç«äº‰ä¼˜åŠ¿
    - ç³»ç»Ÿæ€§çš„æ–¹æ³•é™ä½äº†è¿è¥é£é™©

    ### ğŸ”® **æœªæ¥å±•æœ›**

    åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­æ·±å…¥MLOpså‘¨æœŸæ•°æ®é˜¶æ®µæœ¬èº«çš„æ›´å¤šé«˜çº§æ¦‚å¿µå’Œå·¥å…·ã€‚

    åœ¨æ•°æ®é˜¶æ®µä¹‹åï¼Œæˆ‘ä»¬å°†ç»§ç»­è¿™ä¸ªé€Ÿæˆè¯¾ç¨‹çš„æ—…ç¨‹ï¼š

    #### ğŸ”„ **CI/CDå·¥ä½œæµ**
    - ä¸ºMLç³»ç»Ÿé‡èº«å®šåˆ¶çš„æŒç»­é›†æˆå’Œéƒ¨ç½²
    - è‡ªåŠ¨åŒ–æµ‹è¯•å’Œè´¨é‡ä¿è¯
    - æ¨¡å‹éƒ¨ç½²çš„æœ€ä½³å®è·µ

    #### ğŸ¢ **è¡Œä¸šæ¡ˆä¾‹ç ”ç©¶**
    - æ¥è‡ªè¡Œä¸šçš„çœŸå®ä¸–ç•Œæ¡ˆä¾‹ç ”ç©¶
    - ä¸åŒè§„æ¨¡å’Œé¢†åŸŸçš„æˆåŠŸæ¨¡å¼
    - å¤±è´¥æ¡ˆä¾‹çš„ç»éªŒæ•™è®­

    #### ğŸ¤– **æ¨¡å‹å¼€å‘å’Œå®è·µ**
    - æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯çš„é«˜çº§æŠ€æœ¯
    - è¶…å‚æ•°ä¼˜åŒ–å’ŒAutoML
    - æ¨¡å‹è§£é‡Šæ€§å’Œå¯ä¿¡AI

    #### ğŸ“Š **ç”Ÿäº§ç›‘æ§å’Œè§‚å¯Ÿ**
    - æ¨¡å‹æ€§èƒ½çš„æŒç»­ç›‘æ§
    - æ•°æ®å’Œæ¨¡å‹æ¼‚ç§»çš„æ£€æµ‹
    - å¼‚å¸¸æ£€æµ‹å’Œè‡ªåŠ¨å“åº”

    #### ğŸ§  **LLMOpsç‰¹æ®Šè€ƒè™‘**
    - å¤§è¯­è¨€æ¨¡å‹çš„ç‰¹æ®Šè¿è¥éœ€æ±‚
    - æç¤ºå·¥ç¨‹å’Œç‰ˆæœ¬æ§åˆ¶
    - æˆæœ¬ä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜

    #### ğŸ”— **å®Œæ•´ç«¯åˆ°ç«¯ç¤ºä¾‹**
    - ç»“åˆç”Ÿå‘½å‘¨æœŸæ‰€æœ‰å…ƒç´ çš„ç»¼åˆæ¡ˆä¾‹
    - ä»æ•°æ®åˆ°éƒ¨ç½²çš„å®Œæ•´å·¥ä½œæµ
    - ä¼ä¸šçº§MLOpså¹³å°çš„æ¶æ„è®¾è®¡

    ### ğŸª **æœ€ç»ˆç›®æ ‡**

    ç›®æ ‡ï¼Œä¸€å¦‚æ—¢å¾€ï¼Œæ˜¯å¸®åŠ©ä½ åŸ¹å…»æˆç†Ÿçš„ã€**ä»¥ç³»ç»Ÿä¸ºä¸­å¿ƒçš„æ€ç»´æ–¹å¼**ï¼Œå°†æœºå™¨å­¦ä¹ ä¸è§†ä¸ºç‹¬ç«‹çš„å·¥ä»¶ï¼Œè€Œæ˜¯æ›´å¹¿æ³›è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿçš„æ´»è·ƒéƒ¨åˆ†ã€‚

    é€šè¿‡æŒæ¡è¿™äº›é«˜çº§æ•°æ®å·¥ç¨‹æ¦‚å¿µï¼Œä½ å·²ç»å…·å¤‡äº†ï¼š

    - **ç³»ç»Ÿæ€§æ€ç»´**ï¼šç†è§£å„ç»„ä»¶é—´çš„ç›¸äº’ä½œç”¨
    - **è´¨é‡æ„è¯†**ï¼šä¼˜å…ˆè€ƒè™‘æ•°æ®è´¨é‡å’Œç³»ç»Ÿå¯é æ€§
    - **ä¸šåŠ¡å¯¼å‘**ï¼šå°†æŠ€æœ¯å†³ç­–ä¸ä¸šåŠ¡ä»·å€¼å¯¹é½
    - **å‰ç»æ€§è§„åˆ’**ï¼šè®¾è®¡å¯æ‰©å±•å’Œå¯ç»´æŠ¤çš„ç³»ç»Ÿ
    - **é£é™©ç®¡ç†**ï¼šè¯†åˆ«å’Œé˜²èŒƒæ½œåœ¨çš„ç³»ç»Ÿæ€§é£é™©

    è¿™äº›èƒ½åŠ›å°†ä½¿ä½ èƒ½å¤Ÿæ„å»ºçœŸæ­£ä¼ä¸šçº§çš„MLç³»ç»Ÿï¼Œä¸ä»…åœ¨å®éªŒå®¤ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ›´èƒ½åœ¨å¤æ‚çš„ç”Ÿäº§ç¯å¢ƒä¸­åˆ›é€ æŒç»­çš„ä¸šåŠ¡ä»·å€¼ã€‚

    ---

    ğŸš€ **ç»§ç»­ä½ çš„MLOpsç²¾è¿›ä¹‹æ—…ï¼Œè®°ä½ï¼šä¼˜ç§€çš„æ•°æ®å·¥ç¨‹æ˜¯å¯é MLç³»ç»Ÿçš„åŸºçŸ³ï¼Œè€Œç³»ç»Ÿæ€§çš„æ–¹æ³•æ˜¯é•¿æœŸæˆåŠŸçš„ä¿è¯ï¼**
    """
    )
    return


if __name__ == "__main__":
    app.run()
