import marimo

__generated_with = "0.15.3"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _(mo):
    mo.md(
        r"""
    # 嵌入模型：从初学者到专家的综合指南

    ## 革新您的NLP技能：掌握词嵌入、上下文化模型和前沿技术，解锁语言理解能力

    ![预览图片](https://miro.medium.com/v2/resize:fit:700/1*RqJL4Lkd_QLUduD5nQbjUw.png)

    **作者：** [Nayab Hassan](https://medium.com/@nay1228)  
    **发布时间：** 2024年8月16日（更新：2024年11月18日）

    ---

    ### 嵌入模型解释：NLP核心技术指南

    ### 引言

    嵌入模型通过将复杂数据转换为更易处理的低维表示，彻底革命了机器学习。这些模型在自然语言处理（NLP）、计算机视觉和推荐系统等领域产生了特别重大的影响。**嵌入**是指将高维数据（如文本、图像）映射到保持语义关系的密集、低维向量中。

    理解嵌入模型是释放现代AI系统力量的关键。本指南将带您了解嵌入模型的基础知识，探索BERT和GPT等最新进展，并提供真实世界的示例和最佳实践。在本指南结束时，您将深入理解嵌入模型、它们的应用以及如何有效实现它们。
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### 理解嵌入模型

    #### 什么是嵌入？

    嵌入模型的核心是**嵌入**概念，它指的是将高维数据表示为低维空间中的向量。这种转换至关重要，因为它使机器学习算法能够处理和理解复杂的输入，如单词、句子、图像，甚至图形。

    例如，像"king"和"queen"这样的单词可以表示为在向量空间中彼此接近的向量，反映它们的语义相似性。这与独热编码形成对比，在独热编码中，每个单词都表示为稀疏的二进制向量，无法捕获任何语义关系。

    嵌入通过各种机器学习技术学习，生成的向量可用于下游任务，如分类、聚类和推荐。

    #### 嵌入的类型

    **1. 词嵌入：** 这些嵌入将单个单词表示为向量。例子包括Word2Vec、GloVe和FastText。词嵌入是NLP任务的基础，使模型能够理解单词之间的语义关系。

    - **示例**：在Word2Vec中，像"king"这样的单词可能由向量`[0.5, 0.8, -0.1, 0.3, 0.9]`表示。单词"queen"会有相似的向量，显示它们的语义相似性，而像"apple"这样的单词在向量空间中会更远。

    **2. 句子嵌入：** 这些嵌入捕获整个句子或段落的含义。像通用句子编码器和Sentence-BERT（SBERT）这样的模型通过平均或池化词嵌入来生成这些嵌入，为整个句子创建单个向量。

    - **示例**：在句子"The cat sat on the mat"中，句子嵌入可能将整个句子映射到一个封装整体含义的向量，而不仅仅是单个单词。

    **3. 图像嵌入：** 这些嵌入在计算机视觉中用于将图像表示为向量。卷积神经网络（CNN）通常作为生成这些嵌入的骨干，然后可用于图像检索或分类等任务。

    - **示例**：猫的图像可能映射到在嵌入空间中接近其他猫图像的向量，帮助模型识别不同图像中的相似对象。

    **4. 图嵌入：** 在图嵌入中，节点或整个子图被映射到保持图内结构关系的向量。像DeepWalk和GraphSAGE这样的模型用于生成这些嵌入。

    - **示例**：在社交网络图中，嵌入可用于将用户表示为向量，其中向量的接近程度表示用户之间关系的强度。

    ![不同向量嵌入](https://miro.medium.com/v2/resize:fit:700/1*dLpreZU17uwvDH1lsRSSZw.png)
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### 关键嵌入模型

    #### Word2Vec

    Word2Vec由Google开发，是词嵌入领域的先驱模型之一。它使用浅层神经网络从大型文本语料库中学习词关联。Word2Vec有两种主要方法：**连续词袋（CBOW）**和**Skip-Gram**。

    - **CBOW：** 在CBOW中，模型根据周围的上下文词预测目标词。例如，在句子"The cat sat on the mat"中，模型可能根据上下文词"cat"、"on"和"the"预测"sat"。

    ![CBOW架构](https://miro.medium.com/v2/resize:fit:700/1*tEC_ZC1qoc31Loxxe2RJ_g.jpeg)

    - **Skip-Gram：** Skip-Gram模型反向工作，根据目标词预测上下文词。例如，给定单词"sat"，模型会预测"cat"、"on"和"the"。

    ![Skip-Gram架构](https://miro.medium.com/v2/resize:fit:700/1*moWjNPsIxs3l1WcALI0btQ.png)

    Word2Vec学习生成嵌入，其中语义相似的单词具有相似的向量表示。

    **示例：** 一个著名的例子涉及类比："king" — "man" + "woman" ≈ "queen"。这表明Word2Vec捕获了单词之间的语义含义和关系。
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    #### GloVe（全局词表示向量）

    GloVe由斯坦福大学开发，是另一个流行的词嵌入模型。与基于预测的Word2Vec不同，GloVe是一个基于计数的模型，利用整个语料库中单词的共现矩阵。GloVe旨在创建捕获单词共现全局统计信息的词向量。

    - **优势**：GloVe在捕获局部和全局上下文方面表现良好，使其适用于各种NLP任务。
    - **示例**：如果像"ice"和"snow"这样的单词在语料库中经常共现，GloVe将创建向量，其中"ice"和"snow"在向量空间中接近，反映它们的语义关系。

    ![构建GloVe的框架](https://miro.medium.com/v2/resize:fit:700/1*h4m0hSKcxGvrG-IMoy2Epg.png)

    #### BERT（来自Transformers的双向编码器表示）

    BERT代表了嵌入模型的重大进步。与独立处理单词的传统模型不同，BERT生成**上下文化嵌入**，这意味着同一个单词可以根据其上下文具有不同的向量。BERT基于**Transformer**架构，使用注意力机制来建模句子中单词之间的关系。

    - **关键特征：** BERT是双向的，这意味着它在训练期间查看单词的左右上下文。这使BERT能够捕获句子中单词的完整含义，不像以单向方式处理单词的模型。
    - **架构：** BERT使用多层Transformers来创建嵌入。每个Transformer层应用自注意力机制来捕获单词之间的关系，无论它们在句子中的距离如何。
    - **训练：** BERT使用两个任务进行训练：**掩码语言建模（MLM）**和**下一句预测（NSP）**。在MLM中，输入句子中的一些单词被掩码，模型学习预测缺失的单词。在NSP中，模型学习预测两个句子是否顺序相关。
    - **应用**：BERT在问答和文本分类等任务中表现出色。例如，在客户支持系统中，BERT可用于高精度地理解和响应客户查询。
    - **示例：** 在句子"The bank can guarantee the deposit will arrive tomorrow"中，BERT将根据周围上下文为单词"bank"生成不同的嵌入。它理解"bank"是指金融机构还是河岸，取决于上下文。

    ![BERT架构](https://miro.medium.com/v2/resize:fit:700/1*VdFLolJb3eXF_pIJu3cyYg.png)
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    #### GPT（生成式预训练Transformer）

    GPT由OpenAI开发，是另一个基于transformer的模型，但专注于生成任务。与主要设计用于理解文本的BERT不同，GPT在基于给定提示生成连贯文本方面表现出色。GPT使用单向transformer，这意味着它在生成文本时只考虑左侧上下文。

    - **关键特征：** GPT在大量文本数据上进行预训练，并在特定任务上进行微调，使其高度通用。其生成类人文本的能力使其在聊天机器人、内容生成和创意写作等应用中广受欢迎。
    - **架构**：GPT使用一堆transformer解码器层。每层对输入序列应用自注意力，并为序列中的下一个标记（单词或子词）生成预测。
    - **训练：** GPT使用无监督学习方法进行预训练，模型学习预测序列中的下一个单词。然后使用监督学习在特定任务上进行微调。
    - **应用：** GPT已用于各种创意应用，如生成文章、诗歌，甚至代码。例如，GPT-3可以基于简短提示生成完整的文章，文本通常与人类写作无法区分。
    - **示例**：给定像"Once upon a time"这样的提示，GPT可以生成完整的故事，利用其对语言和上下文的理解来创建连贯和创意的叙述。

    ### 嵌入模型的真实世界应用

    #### 情感分析

    嵌入模型在情感分析中至关重要，它们将单词转换为捕获情感相关特征的向量。例如，具有积极情感的单词如"happy"和"joyful"将具有相似的嵌入，帮助模型检测文本中的情感。

    - **示例**：在电影评论中，句子"The film was breathtaking and inspiring"可以表示为嵌入序列。情感分析模型处理这些嵌入以将评论分类为积极。

    ![情感分析管道图](https://miro.medium.com/v2/resize:fit:700/1*Q0sdsZxeUlMexQRn1wkwgQ.png)
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    #### 推荐系统

    推荐系统利用嵌入模型将用户和物品（如电影、产品）表示为向量。这些嵌入用于计算用户和物品之间的相似性，实现个性化推荐。

    - **示例**：在电影推荐系统中，嵌入可用于表示用户和电影。如果用户喜欢"Inception"和"Interstellar"，模型可以推荐其他具有相似嵌入的电影，如"The Matrix"。

    #### 问答系统

    嵌入模型通过将问题和潜在答案映射到共同向量空间来驱动问答（QA）系统。这使模型能够基于问题和答案嵌入之间的相似性检索最相关的答案。

    - **示例**：在QA系统中，像"What is the capital of France?"这样的问题可以嵌入为向量。模型从潜在答案数据库中检索最相似的向量，返回"Paris"。

    #### 语言翻译

    在机器翻译中，嵌入模型用于将不同语言的单词和句子表示为共享嵌入空间中的向量。这使模型能够通过找到最接近的匹配向量在语言之间进行翻译。

    - **示例：** 在翻译模型中，英语单词"cat"和西班牙语单词"gato"将具有相似的嵌入，允许模型在两种语言之间进行翻译。

    ![语言翻译GPT架构](https://miro.medium.com/v2/resize:fit:700/1*x9ZAWUBWm0z5g1tUuCbJKQ.png)
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### 最佳实践和考虑因素

    #### 选择正确的模型

    - **任务特定模型**：根据您的特定任务选择嵌入模型。例如，BERT适用于需要深度上下文理解的任务，而GPT更适合生成任务。
    - **预训练vs自定义模型**：利用预训练模型可以节省时间和资源。但是，对于特定领域的任务，可能需要微调或训练自定义模型。

    #### 针对特定领域任务的微调

    在特定领域数据上微调嵌入模型可以显著提高性能。例如，在法律文档上微调BERT用于法律QA系统将比使用通用BERT模型产生更好的结果。

    #### 计算考虑

    嵌入模型，特别是像BERT和GPT这样的大型模型，需要大量的计算资源。在生产系统中部署嵌入模型时，考虑模型大小、训练时间和推理速度之间的权衡很重要。

    #### 伦理考虑

    - 嵌入模型可能无意中捕获训练数据中存在的偏见。监控和减轻这些偏见对于确保基于嵌入模型的AI系统公平和伦理至关重要。
    - **示例**：词嵌入模型可能将像"nurse"这样的单词与女性代词更密切地关联，将"engineer"与男性代词关联，反映社会偏见。必须解决这些偏见以防止AI应用中的偏见结果。
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### 嵌入模型的最新进展

    - **RoBERTa**：BERT的变体，RoBERTa通过移除NSP任务并在更大数据集上训练来优化训练。它在各种NLP基准测试中表现出卓越的性能。
    - **GPT-4**：GPT系列的最新迭代，GPT-4进一步改进了模型的生成能力，使其在创意和复杂文本生成任务中更加通用。
    - **Sentence-BERT（SBERT）**：BERT的扩展，SBERT通过使用孪生和三元组网络为句子相似性任务微调BERT。它产生更适合语义搜索和聚类等任务的嵌入。

    ### 伦理考虑

    尽管嵌入模型有众多应用，但它们并非免疫于伦理关注。这些模型可以捕获和放大训练数据中存在的偏见，导致AI系统中不公平或歧视性的结果。识别和减轻这些偏见对于开发伦理AI解决方案至关重要。

    - **嵌入中的偏见：** 嵌入模型中的一个常见伦理问题是性别、种族或文化偏见的存在。例如，在大型文本语料库上训练的词嵌入可能将像"doctor"这样的单词与男性代词关联，将"nurse"与女性代词关联，反映社会刻板印象。当嵌入模型部署在招聘系统或推荐算法等真实世界应用中时，这些偏见可能延续歧视。

    ![偏见示例](https://miro.medium.com/v2/resize:fit:700/1*KJLkE1Wjach_eS3SLGyizA.png)

    - **减轻偏见：** 为了减轻偏见，开发人员可以采用诸如**去偏见嵌入**等技术，其中嵌入空间中的偏见维度被移除或中和。定期审计AI系统的偏见和公平性，以及使用多样化和代表性的训练数据，也可以帮助减少嵌入模型中的偏见。
    - **透明度和问责制：** 开发人员还应在嵌入模型的部署中优先考虑透明度和问责制。这包括使训练数据和模型决策对用户透明，并确保有机制来解决AI系统使用中可能出现的潜在伤害或偏见。

    嵌入模型具有变革潜力，但开发人员必须以伦理心态对待它们的使用，确保AI系统公平和负责任地为所有用户服务。
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### 嵌入模型在大型语言模型（LLMs）中的使用

    嵌入模型构成了大型语言模型（LLMs）如GPT、BERT及其变体的骨干。这些模型由于能够生成和理解类人文本而在各种AI应用中获得了突出地位。LLMs的成功很大程度上归功于使用复杂的嵌入技术，将文本转换为有意义的向量表示。

    #### 标记化和嵌入：

    在LLMs中，第一步是对输入文本进行标记化，将其分解为单词、子词或字符。然后每个标记被转换为嵌入，作为模型的输入。这些嵌入捕获标记的语义含义，并作为模型层中进一步处理的基础。

    ![标记化过程](https://miro.medium.com/v2/resize:fit:700/1*cEb_eOYDxheAw6_DhIbmSw.png)

    #### 自注意力机制：

    LLMs严重依赖**自注意力机制**，它允许模型在生成嵌入时关注输入文本的不同部分。这种机制使模型能够捕获文本中的长程依赖关系，使LLMs在需要理解上下文和单词之间关系的任务中特别有效。

    #### 上下文化嵌入：

    与为每个单词生成固定向量的传统词嵌入不同，LLMs产生**上下文化嵌入**。这意味着同一个单词可以根据周围上下文具有不同的嵌入。例如，单词"bat"在句子"The bat flew at night"和"He swung the bat"中将具有不同的嵌入。

    #### Transformer架构：

    Transformer架构是LLMs的核心。Transformers使用注意力机制层来处理输入文本并生成上下文化嵌入。每层根据标记之间的关系细化嵌入，允许模型建立对输入文本的深度理解。在像GPT这样的模型中，这个过程是单向的，模型通过预测序列中的下一个单词来生成文本。在BERT中，过程是双向的，允许模型在生成嵌入时考虑前面和后面的文本。

    ![Transformer架构](https://miro.medium.com/v2/resize:fit:700/1*TIkgMOAUMaSz69sXP4nbvg.png)
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    #### LLMs中的迁移学习：

    LLMs的关键优势之一是它们能够将从大规模预训练任务中学到的知识转移到特定的下游任务。在预训练期间，模型学习生成捕获一般语言模式的嵌入。在特定任务（如情感分析或文本分类）上的微调允许模型将这些嵌入适应新任务，通常只需要最少的训练数据。

    - **示例**：例如，GPT-3可以在客户服务对话数据集上进行微调。模型将使用其预训练的嵌入来理解上下文并生成适当的响应，尽管在预训练期间没有明确地在客户服务数据上进行训练。

    #### LLMs中嵌入模型的挑战：

    虽然LLMs中的嵌入模型推进了AI能力，但它们也带来了挑战。LLMs需要大量的计算资源进行训练和推理，使它们昂贵且能源密集。此外，LLMs生成的嵌入并不总是可解释的，这对AI系统的透明度和信任构成了挑战。

    嵌入模型是LLMs成功的组成部分，推动了文本生成、翻译和问答等AI应用的进步。随着这些模型的不断发展，嵌入技术将继续处于AI创新的前沿。

    ### 其他资源

    为了加深您对嵌入模型的理解，请考虑以下资源：

    #### 课程

    - Coursera上Andrew Ng的"深度学习专业化"
    - Udemy上的"NLP的Transformers"

    #### 书籍

    - François Chollet的《Python深度学习》
    - Lewis Tunstall、Leandro von Werra和Thomas Wolf的《使用Transformers进行自然语言处理》

    ### 结论

    嵌入模型是现代AI的骨干，使机器能够以以前不可能的方式理解和处理复杂数据。从像Word2Vec和GloVe这样的词嵌入到像BERT和GPT这样的最先进模型，嵌入为NLP、计算机视觉和推荐系统等领域开辟了新的可能性。

    随着这些模型的不断发展，它们对AI的影响只会增长。通过理解和实现嵌入模型，从业者可以构建强大的AI系统，在从情感分析到创意写作的任务中表现出色。

    通过掌握嵌入模型，您将能够很好地应对现代AI的挑战和机遇。

    ---

    **原文链接：** [Embedding Models: A Comprehensive Guide for Beginners to Experts](https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1)
    """
    )
    return


if __name__ == "__main__":
    app.run()
