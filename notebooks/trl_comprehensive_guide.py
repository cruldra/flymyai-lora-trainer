"""
TRL å®Œå…¨æŒ‡å— - Transformer Reinforcement Learning

TRLæ˜¯Hugging Faceå¼€å‘çš„å…¨æ ˆåº“ï¼Œç”¨äºè®­ç»ƒtransformerè¯­è¨€æ¨¡å‹ï¼Œ
æ”¯æŒç›‘ç£å¾®è°ƒ(SFT)ã€å¼ºåŒ–å­¦ä¹ (PPO/RLOO)ã€ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ç­‰æ–¹æ³•ã€‚

ç‰¹ç‚¹ï¼š
1. ğŸš€ å…¨æ ˆè§£å†³æ–¹æ¡ˆ - ä»SFTåˆ°RLHFçš„å®Œæ•´å·¥å…·é“¾
2. ğŸ¯ å¤šç§è®­ç»ƒæ–¹æ³• - SFTã€PPOã€DPOã€KTOã€ORPOç­‰
3. ğŸ”§ æ˜“äºé›†æˆ - ä¸Transformersã€PEFTæ— ç¼é›†æˆ
4. ğŸ“Š å¥–åŠ±å»ºæ¨¡ - å†…ç½®å¥–åŠ±æ¨¡å‹è®­ç»ƒæ”¯æŒ
5. ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒ - æ”¯æŒDeepSpeedã€FSDPç­‰

ä½œè€…: Marimo Notebook
æ—¥æœŸ: 2025-01-XX
"""

import marimo

__generated_with = "0.16.5"
app = marimo.App(width="medium", app_title="TRL å®Œå…¨æŒ‡å—")


@app.cell(hide_code=True)
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    # ğŸ¨ TRL å®Œå…¨æŒ‡å—

    ## ä»€ä¹ˆæ˜¯TRLï¼Ÿ

    **TRL (Transformer Reinforcement Learning)** æ˜¯Hugging Faceå¼€å‘çš„å…¨æ ˆåº“ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒå’Œå¾®è°ƒtransformerè¯­è¨€æ¨¡å‹ã€‚å®ƒæä¾›äº†ä»ç›‘ç£å¾®è°ƒåˆ°å¼ºåŒ–å­¦ä¹ çš„å®Œæ•´å·¥å…·é“¾ã€‚

    ### æ ¸å¿ƒç‰¹æ€§

    1. **ç›‘ç£å¾®è°ƒ (SFT)** ğŸ“
       - åŸºç¡€æ¨¡å‹å¾®è°ƒ
       - æŒ‡ä»¤å¾®è°ƒ
       - å¯¹è¯æ¨¡å‹è®­ç»ƒ

    2. **å¼ºåŒ–å­¦ä¹  (RL)** ğŸ®
       - PPO (Proximal Policy Optimization)
       - RLOO (Reinforce Leave One Out)
       - GRPO (Group Relative Policy Optimization)

    3. **åå¥½ä¼˜åŒ–** ğŸ¯
       - DPO (Direct Preference Optimization)
       - KTO (Kahneman-Tversky Optimization)
       - ORPO (Odds Ratio Preference Optimization)
       - CPO (Contrastive Preference Optimization)

    4. **å¥–åŠ±å»ºæ¨¡** ğŸ†
       - å¥–åŠ±æ¨¡å‹è®­ç»ƒ
       - è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ (PRM)
       - è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

    5. **é«˜çº§åŠŸèƒ½** âš¡
       - åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
       - å†…å­˜ä¼˜åŒ–
       - vLLMé›†æˆ
       - PEFTé›†æˆ

    ### TRL vs å…¶ä»–è®­ç»ƒæ¡†æ¶

    | ç‰¹æ€§ | TRL | Axolotl | LLaMA-Factory | DeepSpeed-Chat |
    |------|-----|---------|---------------|----------------|
    | SFTæ”¯æŒ | âœ… | âœ… | âœ… | âœ… |
    | RLHFæ”¯æŒ | âœ… å®Œæ•´ | â­â­ åŸºç¡€ | â­â­ åŸºç¡€ | âœ… å®Œæ•´ |
    | DPOæ”¯æŒ | âœ… | âœ… | âœ… | âŒ |
    | æ˜“ç”¨æ€§ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­ |
    | çµæ´»æ€§ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
    | å®˜æ–¹æ”¯æŒ | âœ… HFå®˜æ–¹ | ç¤¾åŒº | ç¤¾åŒº | âœ… MSå®˜æ–¹ |
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“¦ å®‰è£…å’Œç¯å¢ƒé…ç½®

    ### æ–¹å¼1: åŸºç¡€å®‰è£…

    ```bash
    # åŸºç¡€å®‰è£…
    pip install trl

    # æˆ–ä½¿ç”¨uv
    uv pip install trl
    ```

    ### æ–¹å¼2: å®Œæ•´å®‰è£…ï¼ˆåŒ…å«æ‰€æœ‰ä¾èµ–ï¼‰

    ```bash
    # å®‰è£…æ‰€æœ‰å¯é€‰ä¾èµ–
    pip install trl[all]

    # æˆ–åˆ†åˆ«å®‰è£…ç‰¹å®šåŠŸèƒ½
    pip install trl[peft]      # PEFTé›†æˆ
    pip install trl[deepspeed] # DeepSpeedæ”¯æŒ
    pip install trl[diffusers] # å›¾åƒç”Ÿæˆæ¨¡å‹æ”¯æŒ
    ```

    ### æ–¹å¼3: ä»æºç å®‰è£…ï¼ˆå¼€å‘ç‰ˆæœ¬ï¼‰

    ```bash
    git clone https://github.com/huggingface/trl.git
    cd trl
    pip install -e .
    ```

    ### ä¾èµ–è¦æ±‚

    - Python >= 3.8
    - PyTorch >= 2.0
    - Transformers >= 4.36
    - Accelerate >= 0.20
    - Datasets >= 2.0
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 1ï¸âƒ£ ç›‘ç£å¾®è°ƒ (SFT)

    ### ä»€ä¹ˆæ˜¯SFTï¼Ÿ

    ç›‘ç£å¾®è°ƒæ˜¯ä½¿ç”¨æ ‡æ³¨æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è¿‡ç¨‹ã€‚è¿™æ˜¯è®­ç»ƒæŒ‡ä»¤æ¨¡å‹çš„ç¬¬ä¸€æ­¥ã€‚

    ### SFTçš„åº”ç”¨åœºæ™¯

    - æŒ‡ä»¤è·Ÿéšæ¨¡å‹
    - å¯¹è¯æ¨¡å‹
    - ç‰¹å®šä»»åŠ¡å¾®è°ƒ
    - é¢†åŸŸé€‚åº”

    ### æ•°æ®æ ¼å¼

    TRLæ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼š

    | æ ¼å¼ | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | æ ‡å‡†æ ¼å¼ | `{"text": "..."}` | å•è½®å¯¹è¯ |
    | å¯¹è¯æ ¼å¼ | `{"messages": [...]}` | å¤šè½®å¯¹è¯ |
    | æŒ‡ä»¤æ ¼å¼ | `{"prompt": "...", "completion": "..."}` | æŒ‡ä»¤-å“åº”å¯¹ |
    """
    )
    return


@app.cell
def _():
    print("=" * 60)
    print("ğŸ“ SFTè®­ç»ƒç¤ºä¾‹")
    print("=" * 60)

    # å‡†å¤‡ç¤ºä¾‹æ•°æ®
    sft_example_data = [
        {
            "text": "<|user|>\nä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚<|assistant|>\nä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ã€‚"
        },
        {
            "text": "<|user|>\nä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ<|assistant|>\næœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›æ€§èƒ½ã€‚"
        },
        {
            "text": "<|user|>\nå¦‚ä½•å­¦ä¹ Pythonï¼Ÿ<|assistant|>\nå­¦ä¹ Pythonå¯ä»¥ä»åŸºç¡€è¯­æ³•å¼€å§‹ï¼Œç„¶åé€šè¿‡å®è·µé¡¹ç›®æ¥å·©å›ºçŸ¥è¯†ã€‚"
        }
    ]

    print(f"\nâœ… å‡†å¤‡äº† {len(sft_example_data)} æ¡è®­ç»ƒæ•°æ®")
    print("\nç¤ºä¾‹æ•°æ®:")
    for sft_idx, sft_item in enumerate(sft_example_data[:2], 1):
        print(f"\n{sft_idx}. {sft_item['text'][:80]}...")

    # SFTè®­ç»ƒé…ç½®ç¤ºä¾‹
    sft_config_example = """
from trl import SFTConfig, SFTTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")

# 2. é…ç½®è®­ç»ƒå‚æ•°
config = SFTConfig(
    output_dir="./sft_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    max_seq_length=512,
    logging_steps=10,
    save_steps=100,
)

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,
    args=config,
    train_dataset=dataset,
    tokenizer=tokenizer,
)

# 4. å¼€å§‹è®­ç»ƒ
trainer.train()
"""

    print("\n" + "=" * 60)
    print("ğŸ“‹ SFTè®­ç»ƒé…ç½®ç¤ºä¾‹:")
    print("=" * 60)
    print(sft_config_example)

    return sft_config_example, sft_example_data


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 2ï¸âƒ£ ç›´æ¥åå¥½ä¼˜åŒ– (DPO)

    ### ä»€ä¹ˆæ˜¯DPOï¼Ÿ

    DPOæ˜¯ä¸€ç§æ— éœ€å¥–åŠ±æ¨¡å‹çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œç›´æ¥ä»åå¥½æ•°æ®ä¸­å­¦ä¹ ã€‚ç›¸æ¯”ä¼ ç»ŸRLHFï¼ŒDPOæ›´ç®€å•ã€æ›´ç¨³å®šã€‚

    ### DPOçš„ä¼˜åŠ¿

    - âœ… æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹
    - âœ… è®­ç»ƒæ›´ç¨³å®š
    - âœ… å®ç°æ›´ç®€å•
    - âœ… è®¡ç®—æ•ˆç‡æ›´é«˜

    ### æ•°æ®æ ¼å¼

    DPOéœ€è¦åå¥½å¯¹æ•°æ®ï¼š

    ```python
    {
        "prompt": "ç”¨æˆ·é—®é¢˜",
        "chosen": "æ›´å¥½çš„å›ç­”",
        "rejected": "è¾ƒå·®çš„å›ç­”"
    }
    ```
    """
    )
    return


@app.cell
def _():
    print("=" * 60)
    print("ğŸ¯ DPOè®­ç»ƒç¤ºä¾‹")
    print("=" * 60)

    # å‡†å¤‡DPOç¤ºä¾‹æ•°æ®
    dpo_example_data = [
        {
            "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "chosen": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            "rejected": "äººå·¥æ™ºèƒ½å°±æ˜¯æœºå™¨äººã€‚"
        },
        {
            "prompt": "å¦‚ä½•ä¿æŒå¥åº·ï¼Ÿ",
            "chosen": "ä¿æŒå¥åº·éœ€è¦å‡è¡¡é¥®é£Ÿã€è§„å¾‹è¿åŠ¨ã€å……è¶³ç¡çœ å’Œè‰¯å¥½çš„å¿ƒç†çŠ¶æ€ã€‚",
            "rejected": "å¤šåƒå°±è¡Œäº†ã€‚"
        }
    ]

    print(f"\nâœ… å‡†å¤‡äº† {len(dpo_example_data)} æ¡åå¥½å¯¹æ•°æ®")
    print("\nç¤ºä¾‹æ•°æ®:")
    for dpo_idx, dpo_item in enumerate(dpo_example_data, 1):
        print(f"\n{dpo_idx}. é—®é¢˜: {dpo_item['prompt']}")
        print(f"   âœ… å¥½å›ç­”: {dpo_item['chosen'][:50]}...")
        print(f"   âŒ å·®å›ç­”: {dpo_item['rejected'][:50]}...")

    # DPOè®­ç»ƒé…ç½®ç¤ºä¾‹
    dpo_config_example = """
from trl import DPOConfig, DPOTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("your-sft-model")
tokenizer = AutoTokenizer.from_pretrained("your-sft-model")

# 2. é…ç½®DPOå‚æ•°
config = DPOConfig(
    output_dir="./dpo_output",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    learning_rate=5e-7,
    beta=0.1,  # DPOæ¸©åº¦å‚æ•°
    max_length=512,
    max_prompt_length=256,
)

# 3. åˆ›å»ºDPOè®­ç»ƒå™¨
trainer = DPOTrainer(
    model=model,
    args=config,
    train_dataset=preference_dataset,
    tokenizer=tokenizer,
)

# 4. å¼€å§‹è®­ç»ƒ
trainer.train()
"""

    print("\n" + "=" * 60)
    print("ğŸ“‹ DPOè®­ç»ƒé…ç½®ç¤ºä¾‹:")
    print("=" * 60)
    print(dpo_config_example)

    return dpo_config_example, dpo_example_data


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 3ï¸âƒ£ å¼ºåŒ–å­¦ä¹  (PPO/RLOO)

    ### ä»€ä¹ˆæ˜¯PPOï¼Ÿ

    PPO (Proximal Policy Optimization) æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºä»å¥–åŠ±ä¿¡å·ä¸­ä¼˜åŒ–æ¨¡å‹ã€‚è¿™æ˜¯ç»å…¸RLHFçš„æ ¸å¿ƒç®—æ³•ã€‚

    ### PPO vs RLOO

    | ç‰¹æ€§ | PPO | RLOO |
    |------|-----|------|
    | å¤æ‚åº¦ | é«˜ | ä½ |
    | ç¨³å®šæ€§ | å¥½ | å¾ˆå¥½ |
    | è®¡ç®—æˆæœ¬ | é«˜ | ä¸­ç­‰ |
    | é€‚ç”¨åœºæ™¯ | å¤æ‚ä»»åŠ¡ | ç®€å•ä»»åŠ¡ |

    ### RLHFæµç¨‹

    ```
    1. SFTè®­ç»ƒ â†’ 2. å¥–åŠ±æ¨¡å‹è®­ç»ƒ â†’ 3. PPOä¼˜åŒ– â†’ 4. è¯„ä¼°
    ```

    ### å¥–åŠ±å‡½æ•°ç±»å‹

    - **æ¨¡å‹å¥–åŠ±**: ä½¿ç”¨è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹
    - **è§„åˆ™å¥–åŠ±**: åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°
    - **æ··åˆå¥–åŠ±**: ç»“åˆå¤šç§å¥–åŠ±ä¿¡å·
    """
    )
    return


@app.cell
def _():
    print("=" * 60)
    print("ğŸ® PPOè®­ç»ƒç¤ºä¾‹")
    print("=" * 60)

    # PPOè®­ç»ƒé…ç½®ç¤ºä¾‹
    ppo_config_example = """
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from transformers import AutoTokenizer

# 1. åŠ è½½æ¨¡å‹ï¼ˆå¸¦ä»·å€¼å¤´ï¼‰
model = AutoModelForCausalLMWithValueHead.from_pretrained("your-sft-model")
tokenizer = AutoTokenizer.from_pretrained("your-sft-model")

# 2. åŠ è½½å¥–åŠ±æ¨¡å‹
reward_model = AutoModelForSequenceClassification.from_pretrained("reward-model")

# 3. é…ç½®PPOå‚æ•°
config = PPOConfig(
    model_name="ppo_model",
    learning_rate=1.41e-5,
    batch_size=16,
    mini_batch_size=4,
    gradient_accumulation_steps=1,
    ppo_epochs=4,
    max_grad_norm=0.5,
)

# 4. åˆ›å»ºPPOè®­ç»ƒå™¨
ppo_trainer = PPOTrainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
)

# 5. è®­ç»ƒå¾ªç¯
for batch in dataloader:
    # ç”Ÿæˆå“åº”
    response_tensors = ppo_trainer.generate(batch["query"])

    # è®¡ç®—å¥–åŠ±
    rewards = compute_rewards(response_tensors, reward_model)

    # PPOæ›´æ–°
    stats = ppo_trainer.step(batch["query"], response_tensors, rewards)
"""

    print("\nğŸ“‹ PPOè®­ç»ƒé…ç½®ç¤ºä¾‹:")
    print("=" * 60)
    print(ppo_config_example)

    print("\n" + "=" * 60)
    print("ğŸ’¡ PPOè®­ç»ƒè¦ç‚¹:")
    print("=" * 60)
    print("1. éœ€è¦å…ˆè®­ç»ƒSFTæ¨¡å‹")
    print("2. éœ€è¦è®­ç»ƒå¥–åŠ±æ¨¡å‹")
    print("3. è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼ˆéœ€è¦å¤šæ¬¡å‰å‘ä¼ æ’­ï¼‰")
    print("4. è¶…å‚æ•°è°ƒä¼˜å¾ˆé‡è¦")
    print("5. å»ºè®®ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")

    return (ppo_config_example,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 4ï¸âƒ£ å¥–åŠ±æ¨¡å‹è®­ç»ƒ

    ### ä»€ä¹ˆæ˜¯å¥–åŠ±æ¨¡å‹ï¼Ÿ

    å¥–åŠ±æ¨¡å‹æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚å®ƒæ˜¯RLHFæµç¨‹ä¸­çš„å…³é”®ç»„ä»¶ã€‚

    ### å¥–åŠ±æ¨¡å‹çš„ä½œç”¨

    - è¯„ä¼°å“åº”è´¨é‡
    - æä¾›è®­ç»ƒä¿¡å·
    - å¼•å¯¼æ¨¡å‹ä¼˜åŒ–æ–¹å‘

    ### è®­ç»ƒæ•°æ®æ ¼å¼

    ```python
    {
        "prompt": "ç”¨æˆ·é—®é¢˜",
        "chosen": "é«˜è´¨é‡å›ç­”",
        "rejected": "ä½è´¨é‡å›ç­”"
    }
    ```
    """
    )
    return


@app.cell
def _():
    print("=" * 60)
    print("ğŸ† å¥–åŠ±æ¨¡å‹è®­ç»ƒç¤ºä¾‹")
    print("=" * 60)

    # å¥–åŠ±æ¨¡å‹è®­ç»ƒé…ç½®
    reward_config_example = """
from trl import RewardConfig, RewardTrainer
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    "your-base-model",
    num_labels=1  # å¥–åŠ±æ¨¡å‹è¾“å‡ºå•ä¸ªåˆ†æ•°
)
tokenizer = AutoTokenizer.from_pretrained("your-base-model")

# 2. é…ç½®è®­ç»ƒå‚æ•°
config = RewardConfig(
    output_dir="./reward_model",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    learning_rate=1e-5,
    max_length=512,
)

# 3. åˆ›å»ºå¥–åŠ±è®­ç»ƒå™¨
trainer = RewardTrainer(
    model=model,
    args=config,
    train_dataset=preference_dataset,
    tokenizer=tokenizer,
)

# 4. å¼€å§‹è®­ç»ƒ
trainer.train()

# 5. ä½¿ç”¨å¥–åŠ±æ¨¡å‹
def get_reward(text):
    inputs = tokenizer(text, return_tensors="pt")
    reward = model(**inputs).logits[0, 0].item()
    return reward
"""

    print("\nğŸ“‹ å¥–åŠ±æ¨¡å‹è®­ç»ƒé…ç½®:")
    print("=" * 60)
    print(reward_config_example)

    print("\n" + "=" * 60)
    print("ğŸ’¡ å¥–åŠ±æ¨¡å‹è®­ç»ƒè¦ç‚¹:")
    print("=" * 60)
    print("1. éœ€è¦é«˜è´¨é‡çš„åå¥½æ•°æ®")
    print("2. æ•°æ®é‡è¦è¶³å¤Ÿï¼ˆå»ºè®®>10Kå¯¹ï¼‰")
    print("3. æ³¨æ„è¿‡æ‹Ÿåˆé—®é¢˜")
    print("4. å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–")
    print("5. è¯„ä¼°æ—¶å…³æ³¨å‡†ç¡®ç‡å’Œæ ¡å‡†åº¦")

    return (reward_config_example,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 5ï¸âƒ£ å…¶ä»–ä¼˜åŒ–æ–¹æ³•

    ### KTO (Kahneman-Tversky Optimization)

    åŸºäºå‰æ™¯ç†è®ºçš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œä¸éœ€è¦æˆå¯¹çš„åå¥½æ•°æ®ã€‚

    ```python
    from trl import KTOConfig, KTOTrainer

    config = KTOConfig(
        output_dir="./kto_output",
        beta=0.1,
        desirable_weight=1.0,
        undesirable_weight=1.0,
    )
    ```

    ### ORPO (Odds Ratio Preference Optimization)

    ç»“åˆSFTå’Œåå¥½ä¼˜åŒ–çš„å•é˜¶æ®µæ–¹æ³•ã€‚

    ```python
    from trl import ORPOConfig, ORPOTrainer

    config = ORPOConfig(
        output_dir="./orpo_output",
        beta=0.1,
        max_length=512,
    )
    ```

    ### CPO (Contrastive Preference Optimization)

    ä½¿ç”¨å¯¹æ¯”å­¦ä¹ çš„åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚

    ```python
    from trl import CPOConfig, CPOTrainer

    config = CPOConfig(
        output_dir="./cpo_output",
        beta=0.1,
        label_smoothing=0.0,
    )
    ```

    ### GRPO (Group Relative Policy Optimization)

    ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œé€‚ç”¨äºæ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ã€‚

    ```python
    from trl import GRPOConfig, GRPOTrainer

    config = GRPOConfig(
        output_dir="./grpo_output",
        num_generations=4,
        temperature=0.7,
    )
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 6ï¸âƒ£ PEFTé›†æˆ

    ### ä»€ä¹ˆæ˜¯PEFTï¼Ÿ

    PEFT (Parameter-Efficient Fine-Tuning) æ˜¯ä¸€ç§é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°ã€‚

    ### æ”¯æŒçš„PEFTæ–¹æ³•

    | æ–¹æ³• | è¯´æ˜ | å‚æ•°é‡ |
    |------|------|--------|
    | LoRA | ä½ç§©é€‚åº” | ~0.1% |
    | QLoRA | é‡åŒ–LoRA | ~0.1% |
    | Prefix Tuning | å‰ç¼€å¾®è°ƒ | ~0.1% |
    | P-Tuning | æç¤ºå¾®è°ƒ | ~0.01% |

    ### TRL + PEFTç¤ºä¾‹

    ```python
    from peft import LoraConfig, get_peft_model
    from trl import SFTTrainer, SFTConfig

    # 1. é…ç½®LoRA
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],
        task_type="CAUSAL_LM",
    )

    # 2. åˆ›å»ºè®­ç»ƒå™¨ï¼ˆè‡ªåŠ¨åº”ç”¨PEFTï¼‰
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        peft_config=peft_config,
    )
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 7ï¸âƒ£ åˆ†å¸ƒå¼è®­ç»ƒ

    ### DeepSpeedé›†æˆ

    TRLå®Œå…¨æ”¯æŒDeepSpeedçš„ZeROä¼˜åŒ–ã€‚

    ```bash
    # ä½¿ç”¨DeepSpeedå¯åŠ¨è®­ç»ƒ
    accelerate launch --config_file deepspeed_config.yaml train.py
    ```

    **DeepSpeedé…ç½®ç¤ºä¾‹ (ds_config.json):**

    ```json
    {
        "train_batch_size": "auto",
        "gradient_accumulation_steps": "auto",
        "gradient_clipping": 1.0,
        "zero_optimization": {
            "stage": 2,
            "offload_optimizer": {
                "device": "cpu"
            }
        },
        "fp16": {
            "enabled": true
        }
    }
    ```

    ### FSDP (Fully Sharded Data Parallel)

    ```python
    from trl import SFTConfig

    config = SFTConfig(
        output_dir="./output",
        fsdp="full_shard auto_wrap",
        fsdp_config={
            "fsdp_transformer_layer_cls_to_wrap": "LlamaDecoderLayer"
        }
    )
    ```

    ### å¤šGPUè®­ç»ƒ

    ```bash
    # ä½¿ç”¨accelerate
    accelerate launch --num_processes 4 train.py

    # ä½¿ç”¨torchrun
    torchrun --nproc_per_node 4 train.py
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 8ï¸âƒ£ å†…å­˜ä¼˜åŒ–æŠ€å·§

    ### 1. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)

    ```python
    from trl import SFTConfig

    config = SFTConfig(
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False}
    )
    ```

    ### 2. æ··åˆç²¾åº¦è®­ç»ƒ

    ```python
    config = SFTConfig(
        fp16=True,  # æˆ– bf16=True
        optim="adamw_torch_fused",
    )
    ```

    ### 3. Flash Attention

    ```python
    from transformers import AutoModelForCausalLM

    model = AutoModelForCausalLM.from_pretrained(
        "model_name",
        attn_implementation="flash_attention_2",
        torch_dtype=torch.bfloat16,
    )
    ```

    ### 4. é‡åŒ–è®­ç»ƒ (QLoRA)

    ```python
    from transformers import BitsAndBytesConfig

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        "model_name",
        quantization_config=bnb_config,
    )
    ```

    ### å†…å­˜ä½¿ç”¨å¯¹æ¯”

    | æ–¹æ³• | 7Bæ¨¡å‹æ˜¾å­˜ | 13Bæ¨¡å‹æ˜¾å­˜ |
    |------|-----------|------------|
    | å…¨ç²¾åº¦ | ~28GB | ~52GB |
    | FP16 | ~14GB | ~26GB |
    | QLoRA | ~6GB | ~10GB |
    | QLoRA + GC | ~4GB | ~7GB |
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## 9ï¸âƒ£ å®æˆ˜æ¡ˆä¾‹ï¼šå®Œæ•´çš„RLHFæµç¨‹

    ### æµç¨‹æ¦‚è§ˆ

    ```
    åŸå§‹æ¨¡å‹ â†’ SFT â†’ å¥–åŠ±æ¨¡å‹ â†’ PPO/DPO â†’ å¯¹é½æ¨¡å‹
    ```

    ### æ­¥éª¤è¯¦è§£

    #### ç¬¬1æ­¥ï¼šç›‘ç£å¾®è°ƒ (SFT)

    ä½¿ç”¨æŒ‡ä»¤æ•°æ®å¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚

    #### ç¬¬2æ­¥ï¼šæ”¶é›†åå¥½æ•°æ®

    - äººå·¥æ ‡æ³¨
    - AIè¾…åŠ©æ ‡æ³¨
    - è‡ªåŠ¨ç”Ÿæˆ

    #### ç¬¬3æ­¥ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹

    ä½¿ç”¨åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚

    #### ç¬¬4æ­¥ï¼šå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–

    ä½¿ç”¨PPOæˆ–DPOä¼˜åŒ–æ¨¡å‹ã€‚

    #### ç¬¬5æ­¥ï¼šè¯„ä¼°å’Œè¿­ä»£

    - è‡ªåŠ¨è¯„ä¼°ï¼ˆå›°æƒ‘åº¦ã€å¥–åŠ±åˆ†æ•°ï¼‰
    - äººå·¥è¯„ä¼°ï¼ˆè´¨é‡ã€å®‰å…¨æ€§ï¼‰
    - è¿­ä»£ä¼˜åŒ–
    """
    )
    return


@app.cell
def _():
    print("=" * 60)
    print("ğŸš€ å®Œæ•´RLHFæµç¨‹ç¤ºä¾‹")
    print("=" * 60)

    rlhf_pipeline = """
# ========== ç¬¬1æ­¥ï¼šSFTè®­ç»ƒ ==========
from trl import SFTConfig, SFTTrainer

sft_config = SFTConfig(
    output_dir="./sft_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-5,
)

sft_trainer = SFTTrainer(
    model=base_model,
    args=sft_config,
    train_dataset=instruction_dataset,
)
sft_trainer.train()

# ========== ç¬¬2æ­¥ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹ ==========
from trl import RewardConfig, RewardTrainer

reward_config = RewardConfig(
    output_dir="./reward_model",
    num_train_epochs=1,
    per_device_train_batch_size=4,
)

reward_trainer = RewardTrainer(
    model=reward_base_model,
    args=reward_config,
    train_dataset=preference_dataset,
)
reward_trainer.train()

# ========== ç¬¬3æ­¥ï¼šDPOä¼˜åŒ–ï¼ˆæ¨èï¼‰ ==========
from trl import DPOConfig, DPOTrainer

dpo_config = DPOConfig(
    output_dir="./dpo_model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    beta=0.1,
)

dpo_trainer = DPOTrainer(
    model=sft_model,
    args=dpo_config,
    train_dataset=preference_dataset,
)
dpo_trainer.train()

# ========== æˆ–ä½¿ç”¨PPOä¼˜åŒ– ==========
from trl import PPOConfig, PPOTrainer

ppo_config = PPOConfig(
    model_name="ppo_model",
    learning_rate=1.41e-5,
)

ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=sft_model_with_value_head,
)

# PPOè®­ç»ƒå¾ªç¯
for batch in dataloader:
    responses = ppo_trainer.generate(batch["query"])
    rewards = reward_model(responses)
    ppo_trainer.step(batch["query"], responses, rewards)
"""

    print("\nğŸ“‹ å®Œæ•´RLHFæµç¨‹ä»£ç :")
    print("=" * 60)
    print(rlhf_pipeline)

    print("\n" + "=" * 60)
    print("ğŸ’¡ æµç¨‹è¦ç‚¹:")
    print("=" * 60)
    print("1. SFTæ˜¯åŸºç¡€ï¼Œè´¨é‡å†³å®šä¸Šé™")
    print("2. åå¥½æ•°æ®è´¨é‡å¾ˆå…³é”®")
    print("3. DPOæ¯”PPOæ›´ç®€å•ç¨³å®šï¼ˆæ¨èï¼‰")
    print("4. éœ€è¦å……åˆ†çš„è¯„ä¼°å’Œæµ‹è¯•")
    print("5. å¯ä»¥å¤šæ¬¡è¿­ä»£ä¼˜åŒ–")

    return (rlhf_pipeline,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ”Ÿ CLIå·¥å…·ä½¿ç”¨

    ### TRL CLI

    TRLæä¾›äº†å‘½ä»¤è¡Œå·¥å…·ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹ã€‚

    #### SFTè®­ç»ƒ

    ```bash
    trl sft \\
        --model_name_or_path Qwen/Qwen2.5-0.5B \\
        --dataset_name timdettmers/openassistant-guanaco \\
        --output_dir ./sft_output \\
        --num_train_epochs 3 \\
        --per_device_train_batch_size 4 \\
        --learning_rate 2e-5
    ```

    #### DPOè®­ç»ƒ

    ```bash
    trl dpo \\
        --model_name_or_path ./sft_model \\
        --dataset_name Anthropic/hh-rlhf \\
        --output_dir ./dpo_output \\
        --num_train_epochs 1 \\
        --beta 0.1
    ```

    #### èŠå¤©ç•Œé¢

    ```bash
    trl chat --model_name_or_path ./trained_model
    ```

    ### é…ç½®æ–‡ä»¶æ–¹å¼

    ```yaml
    # config.yaml
    model_name_or_path: Qwen/Qwen2.5-0.5B
    dataset_name: timdettmers/openassistant-guanaco
    output_dir: ./output
    num_train_epochs: 3
    per_device_train_batch_size: 4
    learning_rate: 2.0e-5
    gradient_checkpointing: true
    fp16: true
    ```

    ```bash
    trl sft --config config.yaml
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“š APIé€ŸæŸ¥è¡¨

    ### è®­ç»ƒå™¨ (Trainers)

    | è®­ç»ƒå™¨ | ç”¨é€” | å…³é”®å‚æ•° |
    |--------|------|----------|
    | `SFTTrainer` | ç›‘ç£å¾®è°ƒ | `max_seq_length`, `packing` |
    | `DPOTrainer` | ç›´æ¥åå¥½ä¼˜åŒ– | `beta`, `max_length` |
    | `PPOTrainer` | å¼ºåŒ–å­¦ä¹  | `ppo_epochs`, `mini_batch_size` |
    | `RewardTrainer` | å¥–åŠ±æ¨¡å‹ | `max_length` |
    | `KTOTrainer` | KTä¼˜åŒ– | `desirable_weight` |
    | `ORPOTrainer` | ORä¼˜åŒ– | `beta` |
    | `GRPOTrainer` | ç»„ç›¸å¯¹ä¼˜åŒ– | `num_generations` |

    ### é…ç½®ç±» (Configs)

    | é…ç½® | è¯´æ˜ | ç¤ºä¾‹ |
    |------|------|------|
    | `SFTConfig` | SFTé…ç½® | `SFTConfig(output_dir="./output")` |
    | `DPOConfig` | DPOé…ç½® | `DPOConfig(beta=0.1)` |
    | `PPOConfig` | PPOé…ç½® | `PPOConfig(learning_rate=1e-5)` |
    | `RewardConfig` | å¥–åŠ±æ¨¡å‹é…ç½® | `RewardConfig(num_train_epochs=1)` |

    ### æ¨¡å‹ç±»

    | ç±» | è¯´æ˜ |
    |-----|------|
    | `AutoModelForCausalLMWithValueHead` | å¸¦ä»·å€¼å¤´çš„å› æœè¯­è¨€æ¨¡å‹ |
    | `AutoModelForSeq2SeqLMWithValueHead` | å¸¦ä»·å€¼å¤´çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ |

    ### å·¥å…·å‡½æ•°

    | å‡½æ•° | è¯´æ˜ |
    |------|------|
    | `create_reference_model()` | åˆ›å»ºå‚è€ƒæ¨¡å‹ |
    | `prepare_model_for_kbit_training()` | å‡†å¤‡é‡åŒ–è®­ç»ƒ |
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ’¡ æœ€ä½³å®è·µ

    ### 1. é€‰æ‹©åˆé€‚çš„è®­ç»ƒæ–¹æ³•

    - **ç®€å•ä»»åŠ¡** â†’ SFTå³å¯
    - **éœ€è¦å¯¹é½** â†’ SFT + DPO
    - **å¤æ‚å¯¹é½** â†’ SFT + å¥–åŠ±æ¨¡å‹ + PPO
    - **èµ„æºå—é™** â†’ ä½¿ç”¨PEFT (LoRA/QLoRA)

    ### 2. æ•°æ®å‡†å¤‡

    - ç¡®ä¿æ•°æ®è´¨é‡é«˜äºæ•°é‡
    - SFTæ•°æ®ï¼šå¤šæ ·æ€§å¾ˆé‡è¦
    - åå¥½æ•°æ®ï¼šå·®å¼‚è¦æ˜æ˜¾
    - æ•°æ®æ¸…æ´—å’Œå»é‡

    ### 3. è¶…å‚æ•°è°ƒä¼˜

    **SFTå…³é”®å‚æ•°:**
    - `learning_rate`: 2e-5 åˆ° 5e-5
    - `num_train_epochs`: 1-3
    - `max_seq_length`: æ ¹æ®ä»»åŠ¡è°ƒæ•´

    **DPOå…³é”®å‚æ•°:**
    - `beta`: 0.1 åˆ° 0.5
    - `learning_rate`: 5e-7 åˆ° 5e-6
    - `num_train_epochs`: 1

    **PPOå…³é”®å‚æ•°:**
    - `learning_rate`: 1e-5 åˆ° 5e-5
    - `ppo_epochs`: 4
    - `mini_batch_size`: æ ¹æ®æ˜¾å­˜è°ƒæ•´

    ### 4. è¯„ä¼°ç­–ç•¥

    - ä½¿ç”¨éªŒè¯é›†ç›‘æ§è¿‡æ‹Ÿåˆ
    - å®šæœŸç”Ÿæˆæ ·æœ¬æ£€æŸ¥è´¨é‡
    - ä½¿ç”¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ˆBLEUã€ROUGEç­‰ï¼‰
    - äººå·¥è¯„ä¼°å…³é”®æ ·æœ¬

    ### 5. å¸¸è§é—®é¢˜

    **æ˜¾å­˜ä¸è¶³:**
    - ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    - å‡å°batch size
    - ä½¿ç”¨QLoRA
    - ä½¿ç”¨DeepSpeed ZeRO

    **è®­ç»ƒä¸ç¨³å®š:**
    - é™ä½å­¦ä¹ ç‡
    - ä½¿ç”¨æ¢¯åº¦è£å‰ª
    - æ£€æŸ¥æ•°æ®è´¨é‡
    - ä½¿ç”¨warmup

    **æ•ˆæœä¸å¥½:**
    - å¢åŠ è®­ç»ƒæ•°æ®
    - è°ƒæ•´è¶…å‚æ•°
    - æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    - å°è¯•ä¸åŒçš„åŸºç¡€æ¨¡å‹
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ”— é›†æˆç¤ºä¾‹

    ### ä¸Hugging Face Hubé›†æˆ

    ```python
    from trl import SFTTrainer, SFTConfig

    config = SFTConfig(
        output_dir="./output",
        push_to_hub=True,
        hub_model_id="username/model-name",
        hub_strategy="every_save",
    )

    trainer = SFTTrainer(
        model=model,
        args=config,
        train_dataset=dataset,
    )

    trainer.train()
    trainer.push_to_hub()
    ```

    ### ä¸Weights & Biasesé›†æˆ

    ```python
    import wandb

    wandb.init(project="trl-training", name="sft-run-1")

    config = SFTConfig(
        output_dir="./output",
        report_to="wandb",
        logging_steps=10,
    )
    ```

    ### ä¸vLLMé›†æˆï¼ˆæ¨ç†åŠ é€Ÿï¼‰

    ```python
    from trl import SFTTrainer
    from trl.trainer.utils import get_vllm_model

    # è®­ç»ƒåä½¿ç”¨vLLMåŠ é€Ÿæ¨ç†
    vllm_model = get_vllm_model(
        model_name="./trained_model",
        tensor_parallel_size=2,
    )

    outputs = vllm_model.generate(
        prompts=["ä½ å¥½"],
        max_tokens=100,
    )
    ```

    ### ä¸Unslothé›†æˆï¼ˆè®­ç»ƒåŠ é€Ÿï¼‰

    ```python
    from unsloth import FastLanguageModel

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/llama-3-8b-bnb-4bit",
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    # ä½¿ç”¨TRLè®­ç»ƒ
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
    )
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“Š æ€§èƒ½å¯¹æ¯”

    ### è®­ç»ƒæ–¹æ³•å¯¹æ¯”

    | æ–¹æ³• | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜å ç”¨ | æ•ˆæœ | éš¾åº¦ |
    |------|---------|---------|------|------|
    | SFT | åŸºå‡† | åŸºå‡† | â­â­â­ | â­ |
    | SFT + LoRA | 0.8x | 0.3x | â­â­â­ | â­ |
    | SFT + QLoRA | 0.9x | 0.2x | â­â­â­ | â­â­ |
    | DPO | 1.2x | 1.5x | â­â­â­â­ | â­â­ |
    | PPO | 3-5x | 2-3x | â­â­â­â­â­ | â­â­â­â­ |

    ### æ¨¡å‹è§„æ¨¡ä¸èµ„æºéœ€æ±‚

    | æ¨¡å‹å¤§å° | å…¨ç²¾åº¦SFT | QLoRA | æ¨èGPU |
    |---------|----------|-------|---------|
    | 1B | 4GB | 2GB | RTX 3060 |
    | 3B | 12GB | 4GB | RTX 3090 |
    | 7B | 28GB | 6GB | A100 40GB |
    | 13B | 52GB | 10GB | A100 80GB |
    | 70B | 280GB | 40GB | 8x A100 |

    ### ä¼˜åŒ–æŠ€æœ¯æ•ˆæœ

    | æŠ€æœ¯ | æ˜¾å­˜èŠ‚çœ | é€Ÿåº¦å½±å“ | ç²¾åº¦å½±å“ |
    |------|---------|---------|---------|
    | Gradient Checkpointing | 30-50% | -20% | æ—  |
    | Flash Attention 2 | 10-20% | +30% | æ—  |
    | 4-bité‡åŒ– | 75% | -10% | è½»å¾® |
    | DeepSpeed ZeRO-3 | 60-80% | -5% | æ—  |
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“ æ€»ç»“

    ### TRLçš„ä¼˜åŠ¿

    - âœ… **å…¨æ ˆè§£å†³æ–¹æ¡ˆ** - ä»SFTåˆ°RLHFçš„å®Œæ•´å·¥å…·é“¾
    - âœ… **æ˜“äºä½¿ç”¨** - ç®€æ´çš„APIå’Œä¸°å¯Œçš„æ–‡æ¡£
    - âœ… **çµæ´»æ€§é«˜** - æ”¯æŒå¤šç§è®­ç»ƒæ–¹æ³•å’Œä¼˜åŒ–æŠ€æœ¯
    - âœ… **ç”Ÿæ€å®Œå–„** - ä¸Transformersã€PEFTã€DeepSpeedç­‰æ— ç¼é›†æˆ
    - âœ… **æŒç»­æ›´æ–°** - Hugging Faceå®˜æ–¹ç»´æŠ¤ï¼Œç´§è·Ÿæœ€æ–°ç ”ç©¶

    ### é€‚ç”¨åœºæ™¯

    - ğŸ¯ **æŒ‡ä»¤å¾®è°ƒ** - è®­ç»ƒéµå¾ªæŒ‡ä»¤çš„æ¨¡å‹
    - ğŸ’¬ **å¯¹è¯æ¨¡å‹** - æ„å»ºèŠå¤©æœºå™¨äºº
    - ğŸ” **åå¥½å¯¹é½** - ä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½
    - ğŸ“š **é¢†åŸŸé€‚åº”** - ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹å¾®è°ƒ
    - ğŸ§ª **ç ”ç©¶å®éªŒ** - å¿«é€ŸéªŒè¯æ–°æƒ³æ³•

    ### å­¦ä¹ è·¯å¾„

    1. **å…¥é—¨é˜¶æ®µ**
       - å­¦ä¹ SFTåŸºç¡€
       - ç†è§£æ•°æ®æ ¼å¼
       - è¿è¡Œç®€å•ç¤ºä¾‹

    2. **è¿›é˜¶é˜¶æ®µ**
       - æŒæ¡DPOè®­ç»ƒ
       - å­¦ä¹ PEFTé›†æˆ
       - ä¼˜åŒ–è®­ç»ƒæ€§èƒ½

    3. **é«˜çº§é˜¶æ®µ**
       - å®ç°å®Œæ•´RLHF
       - è‡ªå®šä¹‰è®­ç»ƒæµç¨‹
       - åˆ†å¸ƒå¼è®­ç»ƒéƒ¨ç½²

    ### ä½•æ—¶ä½¿ç”¨TRL

    - âœ… **éœ€è¦å¾®è°ƒè¯­è¨€æ¨¡å‹** â†’ ä½¿ç”¨TRL
    - âœ… **éœ€è¦åå¥½å¯¹é½** â†’ ä½¿ç”¨TRLçš„DPO/PPO
    - âœ… **èµ„æºå—é™** â†’ ä½¿ç”¨TRL + PEFT
    - âœ… **å¿«é€ŸåŸå‹** â†’ ä½¿ç”¨TRL CLI

    ### ä½•æ—¶ä¸ä½¿ç”¨TRL

    - âŒ **åªéœ€è¦æ¨ç†** â†’ ä½¿ç”¨vLLMæˆ–TGI
    - âŒ **é¢„è®­ç»ƒå¤§æ¨¡å‹** â†’ ä½¿ç”¨Megatron-LM
    - âŒ **éè¯­è¨€æ¨¡å‹** â†’ ä½¿ç”¨å…¶ä»–æ¡†æ¶

    ### å­¦ä¹ èµ„æº

    - ğŸ“– [å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/trl)
    - ğŸ’» [GitHubä»“åº“](https://github.com/huggingface/trl)
    - ğŸ“ [ç¤ºä¾‹ä»£ç ](https://github.com/huggingface/trl/tree/main/examples)
    - ğŸ¥ [è§†é¢‘æ•™ç¨‹](https://huggingface.co/learn)
    - ğŸ’¬ [Discordç¤¾åŒº](https://discord.gg/hugging-face)
    - ğŸ“š [Smol Course](https://huggingface.co/learn/smol-course)

    ### ç›¸å…³å·¥å…·

    - **Transformers** - åŸºç¡€æ¨¡å‹åº“
    - **PEFT** - å‚æ•°é«˜æ•ˆå¾®è°ƒ
    - **Accelerate** - åˆ†å¸ƒå¼è®­ç»ƒ
    - **DeepSpeed** - å¤§è§„æ¨¡è®­ç»ƒä¼˜åŒ–
    - **vLLM** - é«˜æ€§èƒ½æ¨ç†
    - **Unsloth** - è®­ç»ƒåŠ é€Ÿ

    ---

    **æ­å–œï¼** ğŸ‰ ä½ å·²ç»æŒæ¡äº†TRLçš„æ ¸å¿ƒæ¦‚å¿µå’Œä½¿ç”¨æ–¹æ³•ã€‚

    ç°åœ¨ä½ å¯ä»¥å¼€å§‹è®­ç»ƒè‡ªå·±çš„è¯­è¨€æ¨¡å‹äº†ï¼

    ### ä¸‹ä¸€æ­¥å»ºè®®

    1. é€‰æ‹©ä¸€ä¸ªå°æ¨¡å‹ï¼ˆå¦‚Qwen2.5-0.5Bï¼‰è¿›è¡ŒSFTå®éªŒ
    2. å‡†å¤‡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®
    3. ä½¿ç”¨LoRAå‡å°‘èµ„æºéœ€æ±‚
    4. é€æ­¥å°è¯•DPOç­‰é«˜çº§æ–¹æ³•
    5. åŠ å…¥ç¤¾åŒºï¼Œåˆ†äº«ç»éªŒå’Œé—®é¢˜
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ## ğŸ“– é™„å½•ï¼šå¸¸ç”¨ä»£ç ç‰‡æ®µ

    ### å¿«é€Ÿå¼€å§‹æ¨¡æ¿

    ```python
    # æœ€å°åŒ–SFTè®­ç»ƒç¤ºä¾‹
    from trl import SFTConfig, SFTTrainer
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B")
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")
    dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")

    # é…ç½®å’Œè®­ç»ƒ
    config = SFTConfig(output_dir="./output", num_train_epochs=1)
    trainer = SFTTrainer(model=model, args=config, train_dataset=dataset, tokenizer=tokenizer)
    trainer.train()
    ```

    ### æ•°æ®å¤„ç†æ¨¡æ¿

    ```python
    # æ ¼å¼åŒ–å¯¹è¯æ•°æ®
    def format_chat_template(example):
        messages = example["messages"]
        text = tokenizer.apply_chat_template(messages, tokenize=False)
        return {"text": text}

    dataset = dataset.map(format_chat_template)
    ```

    ### æ¨ç†æ¨¡æ¿

    ```python
    # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
    from transformers import pipeline

    generator = pipeline("text-generation", model="./output")
    output = generator("ä½ å¥½", max_length=100)
    print(output[0]["generated_text"])
    ```

    ### è¯„ä¼°æ¨¡æ¿

    ```python
    # è®¡ç®—å›°æƒ‘åº¦
    from trl import SFTTrainer

    eval_results = trainer.evaluate()
    print(f"Perplexity: {eval_results['eval_loss']:.2f}")
    ```
    """
    )
    return


if __name__ == "__main__":
    app.run()


