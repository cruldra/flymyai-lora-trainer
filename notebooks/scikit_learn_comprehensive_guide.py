import marimo

__generated_with = "0.16.5"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # Scikit-learn å®Œå…¨æŒ‡å—

    ## ğŸ¯ ä»€ä¹ˆæ˜¯Scikit-learnï¼Ÿ

    **Scikit-learn** æ˜¯Pythonä¸­æœ€æµè¡Œçš„æœºå™¨å­¦ä¹ åº“ï¼Œæä¾›äº†ç®€å•é«˜æ•ˆçš„æ•°æ®æŒ–æ˜å’Œæ•°æ®åˆ†æå·¥å…·ã€‚å®ƒå»ºç«‹åœ¨NumPyã€SciPyå’Œmatplotlibä¹‹ä¸Šã€‚

    ### æ ¸å¿ƒç‰¹ç‚¹

    - **ç®€å•ä¸€è‡´çš„API**ï¼šæ‰€æœ‰ç®—æ³•éƒ½éµå¾ªç›¸åŒçš„æ¥å£æ¨¡å¼
    - **ä¸°å¯Œçš„ç®—æ³•åº“**ï¼šæ¶µç›–åˆ†ç±»ã€å›å½’ã€èšç±»ã€é™ç»´ç­‰
    - **ä¼˜ç§€çš„æ–‡æ¡£**ï¼šè¯¦ç»†çš„ç”¨æˆ·æŒ‡å—å’ŒAPIæ–‡æ¡£
    - **æ´»è·ƒçš„ç¤¾åŒº**ï¼šæŒç»­æ›´æ–°å’Œç»´æŠ¤
    - **ç”Ÿäº§å°±ç»ª**ï¼šç»è¿‡å……åˆ†æµ‹è¯•ï¼Œå¯ç”¨äºç”Ÿäº§ç¯å¢ƒ

    ### ä¸»è¦åŠŸèƒ½æ¨¡å—

    1. **ç›‘ç£å­¦ä¹ **ï¼šåˆ†ç±»å’Œå›å½’ç®—æ³•
    2. **æ— ç›‘ç£å­¦ä¹ **ï¼šèšç±»ã€é™ç»´ã€å¼‚å¸¸æ£€æµ‹
    3. **æ¨¡å‹é€‰æ‹©**ï¼šäº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢ã€è¯„ä¼°æŒ‡æ ‡
    4. **æ•°æ®é¢„å¤„ç†**ï¼šç‰¹å¾ç¼©æ”¾ã€ç¼–ç ã€è½¬æ¢
    5. **ç‰¹å¾å·¥ç¨‹**ï¼šç‰¹å¾é€‰æ‹©ã€ç‰¹å¾æå–
    6. **é›†æˆæ–¹æ³•**ï¼šBaggingã€Boostingã€Stacking
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“¦ å®‰è£…å’Œå¯¼å…¥

    é¦–å…ˆè®©æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åº“ï¼š
    """
    )
    return


@app.cell
def _():
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn import __version__ as sklearn_version

    print(f"âœ… Scikit-learn ç‰ˆæœ¬: {sklearn_version}")
    print(f"âœ… NumPy ç‰ˆæœ¬: {np.__version__}")
    print(f"âœ… Pandas ç‰ˆæœ¬: {pd.__version__}")

    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
    np.random.seed(42)

    # è®¾ç½®ç»˜å›¾æ ·å¼
    sns.set_style("whitegrid")
    plt.rcParams['figure.figsize'] = (10, 6)
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    return np, pd, plt, seaborn, sklearn_version, sns


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ—ï¸ Scikit-learnçš„æ ¸å¿ƒAPIè®¾è®¡

    Scikit-learnçš„æ‰€æœ‰ç®—æ³•éƒ½éµå¾ªç»Ÿä¸€çš„APIæ¨¡å¼ï¼š

    ### 1. Estimatorï¼ˆä¼°è®¡å™¨ï¼‰
    æ‰€æœ‰æœºå™¨å­¦ä¹ ç®—æ³•éƒ½å®ç°äº†`fit()`æ–¹æ³•ï¼š
    ```python
    estimator.fit(X_train, y_train)
    ```

    ### 2. Predictorï¼ˆé¢„æµ‹å™¨ï¼‰
    ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°äº†`predict()`æ–¹æ³•ï¼š
    ```python
    predictions = estimator.predict(X_test)
    ```

    ### 3. Transformerï¼ˆè½¬æ¢å™¨ï¼‰
    æ•°æ®é¢„å¤„ç†ç±»å®ç°äº†`transform()`æ–¹æ³•ï¼š
    ```python
    X_transformed = transformer.transform(X)
    ```

    ### 4. Pipelineï¼ˆç®¡é“ï¼‰
    å¯ä»¥å°†å¤šä¸ªæ­¥éª¤ä¸²è”èµ·æ¥ï¼š
    ```python
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression())
    ])
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“Š æ•°æ®é›†å‡†å¤‡

    è®©æˆ‘ä»¬ä½¿ç”¨scikit-learnå†…ç½®çš„æ•°æ®é›†æ¥æ¼”ç¤ºå„ç§åŠŸèƒ½ï¼š
    """
    )
    return


@app.cell
def _(np, pd):
    from sklearn.datasets import load_iris, load_diabetes, make_classification, make_regression

    # 1. åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
    iris = load_iris()
    iris_df = pd.DataFrame(
        data=iris.data,
        columns=iris.feature_names
    )
    iris_df['target'] = iris.target
    iris_df['species'] = iris_df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

    print("ğŸŒ¸ é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆåˆ†ç±»ï¼‰:")
    print(f"  æ ·æœ¬æ•°: {len(iris_df)}")
    print(f"  ç‰¹å¾æ•°: {len(iris.feature_names)}")
    print(f"  ç±»åˆ«æ•°: {len(iris.target_names)}")
    print(f"  ç±»åˆ«: {iris.target_names}")

    # 2. åŠ è½½ç³–å°¿ç—…æ•°æ®é›†ï¼ˆå›å½’ä»»åŠ¡ï¼‰
    diabetes = load_diabetes()
    diabetes_df = pd.DataFrame(
        data=diabetes.data,
        columns=diabetes.feature_names
    )
    diabetes_df['target'] = diabetes.target

    print("\nğŸ’‰ ç³–å°¿ç—…æ•°æ®é›†ï¼ˆå›å½’ï¼‰:")
    print(f"  æ ·æœ¬æ•°: {len(diabetes_df)}")
    print(f"  ç‰¹å¾æ•°: {len(diabetes.feature_names)}")

    # 3. ç”Ÿæˆè‡ªå®šä¹‰åˆ†ç±»æ•°æ®é›†
    X_class, y_class = make_classification(
        n_samples=1000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        n_classes=2,
        random_state=42
    )

    print("\nğŸ² è‡ªå®šä¹‰åˆ†ç±»æ•°æ®é›†:")
    print(f"  æ ·æœ¬æ•°: {len(X_class)}")
    print(f"  ç‰¹å¾æ•°: {X_class.shape[1]}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_class)}")

    return X_class, diabetes, diabetes_df, iris, iris_df, load_diabetes, load_iris, make_classification, make_regression, y_class


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ”„ æ•°æ®é¢„å¤„ç†

    æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ æµç¨‹ä¸­çš„å…³é”®æ­¥éª¤ã€‚Scikit-learnæä¾›äº†ä¸°å¯Œçš„é¢„å¤„ç†å·¥å…·ï¼š
    """
    )
    return


@app.cell
def _(X_class, np, y_class):
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
    from sklearn.impute import SimpleImputer

    print("ğŸ”§ æ•°æ®é¢„å¤„ç†æ¼”ç¤º:")
    print("=" * 50)

    # 1. æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_class, y_class, test_size=0.2, random_state=42, stratify=y_class
    )
    print(f"\n1ï¸âƒ£ æ•°æ®åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {X_train.shape}")
    print(f"  æµ‹è¯•é›†: {X_test.shape}")

    # 2. ç‰¹å¾ç¼©æ”¾ - StandardScalerï¼ˆæ ‡å‡†åŒ–ï¼‰
    scaler_standard = StandardScaler()
    X_train_scaled = scaler_standard.fit_transform(X_train)
    X_test_scaled = scaler_standard.transform(X_test)

    print(f"\n2ï¸âƒ£ æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰:")
    print(f"  åŸå§‹æ•°æ®å‡å€¼: {X_train[:, 0].mean():.4f}")
    print(f"  åŸå§‹æ•°æ®æ ‡å‡†å·®: {X_train[:, 0].std():.4f}")
    print(f"  æ ‡å‡†åŒ–åå‡å€¼: {X_train_scaled[:, 0].mean():.4f}")
    print(f"  æ ‡å‡†åŒ–åæ ‡å‡†å·®: {X_train_scaled[:, 0].std():.4f}")

    # 3. ç‰¹å¾ç¼©æ”¾ - MinMaxScalerï¼ˆå½’ä¸€åŒ–ï¼‰
    scaler_minmax = MinMaxScaler()
    X_train_normalized = scaler_minmax.fit_transform(X_train)

    print(f"\n3ï¸âƒ£ å½’ä¸€åŒ–ï¼ˆMinMaxScalerï¼‰:")
    print(f"  åŸå§‹æ•°æ®èŒƒå›´: [{X_train[:, 0].min():.4f}, {X_train[:, 0].max():.4f}]")
    print(f"  å½’ä¸€åŒ–åèŒƒå›´: [{X_train_normalized[:, 0].min():.4f}, {X_train_normalized[:, 0].max():.4f}]")

    # 4. ç¼ºå¤±å€¼å¤„ç†
    X_with_missing = X_train.copy()
    X_with_missing[0:10, 0] = np.nan

    imputer = SimpleImputer(strategy='mean')
    X_imputed = imputer.fit_transform(X_with_missing)

    print(f"\n4ï¸âƒ£ ç¼ºå¤±å€¼å¤„ç†:")
    print(f"  ç¼ºå¤±å€¼æ•°é‡: {np.isnan(X_with_missing).sum()}")
    print(f"  å¡«å……åç¼ºå¤±å€¼: {np.isnan(X_imputed).sum()}")

    return LabelEncoder, MinMaxScaler, OneHotEncoder, SimpleImputer, StandardScaler, X_imputed, X_test, X_test_scaled, X_train, X_train_normalized, X_train_scaled, X_with_missing, imputer, scaler_minmax, scaler_standard, train_test_split, y_test, y_train


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ¯ ç›‘ç£å­¦ä¹ ï¼šåˆ†ç±»ç®—æ³•

    è®©æˆ‘ä»¬æ¢ç´¢scikit-learnä¸­æœ€å¸¸ç”¨çš„åˆ†ç±»ç®—æ³•ï¼š
    """
    )
    return


@app.cell
def _(X_test_scaled, X_train_scaled, y_test, y_train):
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

    print("ğŸ¯ åˆ†ç±»ç®—æ³•å¯¹æ¯”:")
    print("=" * 70)

    classifiers = {
        "é€»è¾‘å›å½’": LogisticRegression(random_state=42, max_iter=1000),
        "å†³ç­–æ ‘": DecisionTreeClassifier(random_state=42, max_depth=5),
        "éšæœºæ£®æ—": RandomForestClassifier(n_estimators=100, random_state=42),
        "æ¢¯åº¦æå‡": GradientBoostingClassifier(n_estimators=100, random_state=42),
        "æ”¯æŒå‘é‡æœº": SVC(kernel='rbf', random_state=42),
        "Kè¿‘é‚»": KNeighborsClassifier(n_neighbors=5),
        "æœ´ç´ è´å¶æ–¯": GaussianNB()
    }

    results = []

    for name, clf in classifiers.items():
        # è®­ç»ƒæ¨¡å‹
        clf.fit(X_train_scaled, y_train)

        # é¢„æµ‹
        y_pred = clf.predict(X_test_scaled)

        # è¯„ä¼°
        accuracy = accuracy_score(y_test, y_pred)

        results.append({
            "ç®—æ³•": name,
            "å‡†ç¡®ç‡": f"{accuracy:.4f}",
            "æ¨¡å‹å¯¹è±¡": clf
        })

        print(f"{name:12s} - å‡†ç¡®ç‡: {accuracy:.4f}")

    print("\nâœ… æ‰€æœ‰åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")

    return DecisionTreeClassifier, GaussianNB, GradientBoostingClassifier, KNeighborsClassifier, LogisticRegression, RandomForestClassifier, SVC, accuracy, accuracy_score, classification_report, clf, classifiers, confusion_matrix, name, results, y_pred


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“ˆ ç›‘ç£å­¦ä¹ ï¼šå›å½’ç®—æ³•

    å›å½’ç®—æ³•ç”¨äºé¢„æµ‹è¿ç»­å€¼ï¼š
    """
    )
    return


@app.cell
def _(diabetes):
    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.svm import SVR
    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

    # å‡†å¤‡å›å½’æ•°æ®
    X_reg = diabetes.data
    y_reg = diabetes.target

    from sklearn.model_selection import train_test_split as tts
    X_reg_train, X_reg_test, y_reg_train, y_reg_test = tts(
        X_reg, y_reg, test_size=0.2, random_state=42
    )

    print("ğŸ“ˆ å›å½’ç®—æ³•å¯¹æ¯”:")
    print("=" * 80)

    regressors = {
        "çº¿æ€§å›å½’": LinearRegression(),
        "å²­å›å½’": Ridge(alpha=1.0),
        "Lassoå›å½’": Lasso(alpha=1.0),
        "å¼¹æ€§ç½‘ç»œ": ElasticNet(alpha=1.0, l1_ratio=0.5),
        "å†³ç­–æ ‘å›å½’": DecisionTreeRegressor(random_state=42, max_depth=5),
        "éšæœºæ£®æ—å›å½’": RandomForestRegressor(n_estimators=100, random_state=42),
        "æ¢¯åº¦æå‡å›å½’": GradientBoostingRegressor(n_estimators=100, random_state=42),
    }

    regression_results = []

    for reg_name, regressor in regressors.items():
        # è®­ç»ƒ
        regressor.fit(X_reg_train, y_reg_train)

        # é¢„æµ‹
        y_reg_pred = regressor.predict(X_reg_test)

        # è¯„ä¼°
        mse = mean_squared_error(y_reg_test, y_reg_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_reg_test, y_reg_pred)
        r2 = r2_score(y_reg_test, y_reg_pred)

        regression_results.append({
            "ç®—æ³•": reg_name,
            "RMSE": f"{rmse:.2f}",
            "MAE": f"{mae:.2f}",
            "RÂ²": f"{r2:.4f}"
        })

        print(f"{reg_name:15s} - RMSE: {rmse:6.2f}, MAE: {mae:6.2f}, RÂ²: {r2:.4f}")

    print("\nâœ… æ‰€æœ‰å›å½’å™¨è®­ç»ƒå®Œæˆ")

    return DecisionTreeRegressor, ElasticNet, GradientBoostingRegressor, Lasso, LinearRegression, RandomForestRegressor, Ridge, SVR, X_reg, X_reg_test, X_reg_train, mae, mean_absolute_error, mean_squared_error, mse, r2, r2_score, reg_name, regressor, regressors, regression_results, rmse, tts, y_reg, y_reg_pred, y_reg_test, y_reg_train


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ” æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©

    Scikit-learnæä¾›äº†ä¸°å¯Œçš„æ¨¡å‹è¯„ä¼°å·¥å…·ï¼š
    """
    )
    return


@app.cell
def _(X_train_scaled, y_train):
    from sklearn.model_selection import cross_val_score, cross_validate, KFold, StratifiedKFold
    from sklearn.linear_model import LogisticRegression as LR

    print("ğŸ” äº¤å‰éªŒè¯æ¼”ç¤º:")
    print("=" * 50)

    model_cv = LR(random_state=42, max_iter=1000)

    # 1. ç®€å•äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model_cv, X_train_scaled, y_train, cv=5)

    print(f"\n1ï¸âƒ£ 5æŠ˜äº¤å‰éªŒè¯:")
    print(f"  å„æŠ˜å¾—åˆ†: {cv_scores}")
    print(f"  å¹³å‡å¾—åˆ†: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

    # 2. è¯¦ç»†äº¤å‰éªŒè¯
    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
    cv_results = cross_validate(
        model_cv, X_train_scaled, y_train,
        cv=5,
        scoring=scoring,
        return_train_score=True
    )

    print(f"\n2ï¸âƒ£ å¤šæŒ‡æ ‡äº¤å‰éªŒè¯:")
    for metric in scoring:
        test_scores = cv_results[f'test_{metric}']
        print(f"  {metric:20s}: {test_scores.mean():.4f} (+/- {test_scores.std() * 2:.4f})")

    # 3. åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    stratified_scores = cross_val_score(model_cv, X_train_scaled, y_train, cv=skf)

    print(f"\n3ï¸âƒ£ åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯:")
    print(f"  å¹³å‡å¾—åˆ†: {stratified_scores.mean():.4f} (+/- {stratified_scores.std() * 2:.4f})")

    return KFold, LR, StratifiedKFold, cross_val_score, cross_validate, cv_results, cv_scores, metric, model_cv, scoring, skf, stratified_scores, test_scores


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ›ï¸ è¶…å‚æ•°è°ƒä¼˜

    ä½¿ç”¨ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢æ¥æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°ï¼š
    """
    )
    return


@app.cell
def _(X_train_scaled, y_train):
    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
    from sklearn.ensemble import RandomForestClassifier as RFC

    print("ğŸ›ï¸ è¶…å‚æ•°è°ƒä¼˜æ¼”ç¤º:")
    print("=" * 50)

    # 1. ç½‘æ ¼æœç´¢
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    rf_model = RFC(random_state=42)

    grid_search = GridSearchCV(
        rf_model,
        param_grid,
        cv=3,
        scoring='accuracy',
        n_jobs=-1,
        verbose=0
    )

    print("\n1ï¸âƒ£ ç½‘æ ¼æœç´¢ï¼ˆGridSearchCVï¼‰:")
    print(f"  å‚æ•°ç»„åˆæ•°: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}")
    print("  æœç´¢ä¸­...")

    grid_search.fit(X_train_scaled, y_train)

    print(f"  æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"  æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")

    # 2. éšæœºæœç´¢
    from scipy.stats import randint

    param_distributions = {
        'n_estimators': randint(50, 300),
        'max_depth': [3, 5, 7, 10, None],
        'min_samples_split': randint(2, 20),
        'min_samples_leaf': randint(1, 10)
    }

    random_search = RandomizedSearchCV(
        rf_model,
        param_distributions,
        n_iter=20,
        cv=3,
        scoring='accuracy',
        random_state=42,
        n_jobs=-1,
        verbose=0
    )

    print("\n2ï¸âƒ£ éšæœºæœç´¢ï¼ˆRandomizedSearchCVï¼‰:")
    print(f"  éšæœºå°è¯•æ¬¡æ•°: 20")
    print("  æœç´¢ä¸­...")

    random_search.fit(X_train_scaled, y_train)

    print(f"  æœ€ä½³å‚æ•°: {random_search.best_params_}")
    print(f"  æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}")

    return GridSearchCV, RFC, RandomizedSearchCV, grid_search, param_distributions, param_grid, randint, random_search, rf_model


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ”— Pipelineï¼ˆç®¡é“ï¼‰

    Pipelineå¯ä»¥å°†å¤šä¸ªå¤„ç†æ­¥éª¤ä¸²è”èµ·æ¥ï¼Œç®€åŒ–å·¥ä½œæµç¨‹ï¼š
    """
    )
    return


@app.cell
def _(X_class, y_class):
    from sklearn.pipeline import Pipeline, make_pipeline
    from sklearn.preprocessing import StandardScaler as SS
    from sklearn.decomposition import PCA
    from sklearn.linear_model import LogisticRegression as LReg

    print("ğŸ”— Pipelineæ¼”ç¤º:")
    print("=" * 50)

    # 1. ä½¿ç”¨Pipelineç±»
    pipeline1 = Pipeline([
        ('scaler', SS()),
        ('pca', PCA(n_components=10)),
        ('classifier', LReg(random_state=42, max_iter=1000))
    ])

    print("\n1ï¸âƒ£ Pipelineæ„å»º:")
    print(f"  æ­¥éª¤: {[name for name, _ in pipeline1.steps]}")

    # åˆ†å‰²æ•°æ®
    from sklearn.model_selection import train_test_split as tts2
    X_pipe_train, X_pipe_test, y_pipe_train, y_pipe_test = tts2(
        X_class, y_class, test_size=0.2, random_state=42
    )

    # è®­ç»ƒpipeline
    pipeline1.fit(X_pipe_train, y_pipe_train)

    # é¢„æµ‹
    pipe_score = pipeline1.score(X_pipe_test, y_pipe_test)

    print(f"  è®­ç»ƒå®Œæˆ")
    print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {pipe_score:.4f}")

    # 2. ä½¿ç”¨make_pipelineï¼ˆè‡ªåŠ¨å‘½åï¼‰
    pipeline2 = make_pipeline(
        SS(),
        PCA(n_components=10),
        LReg(random_state=42, max_iter=1000)
    )

    print("\n2ï¸âƒ£ make_pipelineï¼ˆè‡ªåŠ¨å‘½åï¼‰:")
    print(f"  æ­¥éª¤: {[name for name, _ in pipeline2.steps]}")

    pipeline2.fit(X_pipe_train, y_pipe_train)
    pipe2_score = pipeline2.score(X_pipe_test, y_pipe_test)

    print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {pipe2_score:.4f}")

    return LReg, PCA, Pipeline, SS, X_pipe_test, X_pipe_train, make_pipeline, pipe2_score, pipe_score, pipeline1, pipeline2, tts2, y_pipe_test, y_pipe_train


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ¨ ç‰¹å¾å·¥ç¨‹

    ç‰¹å¾é€‰æ‹©å’Œç‰¹å¾æå–æ˜¯æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ï¼š
    """
    )
    return


@app.cell
def _(X_class, y_class):
    from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel
    from sklearn.ensemble import RandomForestClassifier as RFCls
    from sklearn.decomposition import PCA as PCAComp

    print("ğŸ¨ ç‰¹å¾å·¥ç¨‹æ¼”ç¤º:")
    print("=" * 50)

    # 1. å•å˜é‡ç‰¹å¾é€‰æ‹©
    selector_univariate = SelectKBest(score_func=f_classif, k=10)
    X_selected_univariate = selector_univariate.fit_transform(X_class, y_class)

    print(f"\n1ï¸âƒ£ å•å˜é‡ç‰¹å¾é€‰æ‹©ï¼ˆSelectKBestï¼‰:")
    print(f"  åŸå§‹ç‰¹å¾æ•°: {X_class.shape[1]}")
    print(f"  é€‰æ‹©ç‰¹å¾æ•°: {X_selected_univariate.shape[1]}")
    print(f"  ç‰¹å¾å¾—åˆ†å‰5: {sorted(selector_univariate.scores_, reverse=True)[:5]}")

    # 2. é€’å½’ç‰¹å¾æ¶ˆé™¤
    estimator_rfe = RFCls(n_estimators=50, random_state=42)
    selector_rfe = RFE(estimator=estimator_rfe, n_features_to_select=10, step=1)
    X_selected_rfe = selector_rfe.fit_transform(X_class, y_class)

    print(f"\n2ï¸âƒ£ é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰:")
    print(f"  åŸå§‹ç‰¹å¾æ•°: {X_class.shape[1]}")
    print(f"  é€‰æ‹©ç‰¹å¾æ•°: {X_selected_rfe.shape[1]}")
    print(f"  é€‰ä¸­ç‰¹å¾ç´¢å¼•: {np.where(selector_rfe.support_)[0][:10]}")

    # 3. åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
    estimator_model = RFCls(n_estimators=100, random_state=42)
    estimator_model.fit(X_class, y_class)

    selector_model = SelectFromModel(estimator_model, prefit=True, threshold='median')
    X_selected_model = selector_model.transform(X_class)

    print(f"\n3ï¸âƒ£ åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©ï¼ˆSelectFromModelï¼‰:")
    print(f"  åŸå§‹ç‰¹å¾æ•°: {X_class.shape[1]}")
    print(f"  é€‰æ‹©ç‰¹å¾æ•°: {X_selected_model.shape[1]}")

    # 4. PCAé™ç»´
    pca_reducer = PCAComp(n_components=10)
    X_pca = pca_reducer.fit_transform(X_class)

    print(f"\n4ï¸âƒ£ PCAé™ç»´:")
    print(f"  åŸå§‹ç‰¹å¾æ•°: {X_class.shape[1]}")
    print(f"  é™ç»´åç‰¹å¾æ•°: {X_pca.shape[1]}")
    print(f"  è§£é‡Šæ–¹å·®æ¯”: {pca_reducer.explained_variance_ratio_[:5]}")
    print(f"  ç´¯è®¡è§£é‡Šæ–¹å·®: {pca_reducer.explained_variance_ratio_.sum():.4f}")

    return PCAComp, RFCls, RFE, SelectFromModel, SelectKBest, X_pca, X_selected_model, X_selected_rfe, X_selected_univariate, estimator_model, estimator_rfe, f_classif, pca_reducer, selector_model, selector_rfe, selector_univariate


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ¤ æ— ç›‘ç£å­¦ä¹ ï¼šèšç±»

    èšç±»ç®—æ³•ç”¨äºå‘ç°æ•°æ®ä¸­çš„è‡ªç„¶åˆ†ç»„ï¼š
    """
    )
    return


@app.cell
def _(iris):
    from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
    from sklearn.metrics import silhouette_score, davies_bouldin_score

    print("ğŸ¤ èšç±»ç®—æ³•æ¼”ç¤º:")
    print("=" * 50)

    X_cluster = iris.data

    # 1. K-Meansèšç±»
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    kmeans_labels = kmeans.fit_predict(X_cluster)

    kmeans_silhouette = silhouette_score(X_cluster, kmeans_labels)
    kmeans_db = davies_bouldin_score(X_cluster, kmeans_labels)

    print(f"\n1ï¸âƒ£ K-Meansèšç±»:")
    print(f"  èšç±»æ•°: 3")
    print(f"  è½®å»“ç³»æ•°: {kmeans_silhouette:.4f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
    print(f"  Davies-BouldinæŒ‡æ•°: {kmeans_db:.4f} (è¶Šå°è¶Šå¥½)")
    print(f"  å„ç°‡æ ·æœ¬æ•°: {np.bincount(kmeans_labels)}")

    # 2. DBSCANèšç±»
    dbscan = DBSCAN(eps=0.5, min_samples=5)
    dbscan_labels = dbscan.fit_predict(X_cluster)

    n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
    n_noise = list(dbscan_labels).count(-1)

    print(f"\n2ï¸âƒ£ DBSCANèšç±»:")
    print(f"  å‘ç°ç°‡æ•°: {n_clusters_dbscan}")
    print(f"  å™ªå£°ç‚¹æ•°: {n_noise}")

    if n_clusters_dbscan > 1:
        dbscan_silhouette = silhouette_score(X_cluster[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])
        print(f"  è½®å»“ç³»æ•°: {dbscan_silhouette:.4f}")

    # 3. å±‚æ¬¡èšç±»
    hierarchical = AgglomerativeClustering(n_clusters=3)
    hierarchical_labels = hierarchical.fit_predict(X_cluster)

    hierarchical_silhouette = silhouette_score(X_cluster, hierarchical_labels)
    hierarchical_db = davies_bouldin_score(X_cluster, hierarchical_labels)

    print(f"\n3ï¸âƒ£ å±‚æ¬¡èšç±»ï¼ˆAgglomerativeClusteringï¼‰:")
    print(f"  èšç±»æ•°: 3")
    print(f"  è½®å»“ç³»æ•°: {hierarchical_silhouette:.4f}")
    print(f"  Davies-BouldinæŒ‡æ•°: {hierarchical_db:.4f}")
    print(f"  å„ç°‡æ ·æœ¬æ•°: {np.bincount(hierarchical_labels)}")

    return AgglomerativeClustering, DBSCAN, KMeans, X_cluster, davies_bouldin_score, dbscan, dbscan_labels, dbscan_silhouette, hierarchical, hierarchical_db, hierarchical_labels, hierarchical_silhouette, kmeans, kmeans_db, kmeans_labels, kmeans_silhouette, n_clusters_dbscan, n_noise, silhouette_score


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ­ é›†æˆå­¦ä¹ 

    é›†æˆæ–¹æ³•é€šè¿‡ç»„åˆå¤šä¸ªæ¨¡å‹æ¥æå‡æ€§èƒ½ï¼š
    """
    )
    return


@app.cell
def _(X_train_scaled, y_train):
    from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, StackingClassifier
    from sklearn.linear_model import LogisticRegression as LogReg
    from sklearn.tree import DecisionTreeClassifier as DTC
    from sklearn.svm import SVC as SVCls

    print("ğŸ­ é›†æˆå­¦ä¹ æ¼”ç¤º:")
    print("=" * 50)

    # 1. æŠ•ç¥¨åˆ†ç±»å™¨
    voting_clf = VotingClassifier(
        estimators=[
            ('lr', LogReg(random_state=42, max_iter=1000)),
            ('dt', DTC(random_state=42, max_depth=5)),
            ('svc', SVCls(random_state=42, probability=True))
        ],
        voting='soft'
    )

    voting_clf.fit(X_train_scaled, y_train)
    voting_score = voting_clf.score(X_train_scaled, y_train)

    print(f"\n1ï¸âƒ£ æŠ•ç¥¨åˆ†ç±»å™¨ï¼ˆVotingClassifierï¼‰:")
    print(f"  åŸºå­¦ä¹ å™¨æ•°é‡: {len(voting_clf.estimators_)}")
    print(f"  è®­ç»ƒé›†å‡†ç¡®ç‡: {voting_score:.4f}")

    # 2. Bagging
    bagging_clf = BaggingClassifier(
        estimator=DTC(random_state=42),
        n_estimators=50,
        random_state=42,
        max_samples=0.8,
        max_features=0.8
    )

    bagging_clf.fit(X_train_scaled, y_train)
    bagging_score = bagging_clf.score(X_train_scaled, y_train)

    print(f"\n2ï¸âƒ£ Baggingåˆ†ç±»å™¨:")
    print(f"  åŸºå­¦ä¹ å™¨æ•°é‡: {bagging_clf.n_estimators}")
    print(f"  è®­ç»ƒé›†å‡†ç¡®ç‡: {bagging_score:.4f}")

    # 3. AdaBoost
    adaboost_clf = AdaBoostClassifier(
        estimator=DTC(max_depth=1, random_state=42),
        n_estimators=50,
        random_state=42
    )

    adaboost_clf.fit(X_train_scaled, y_train)
    adaboost_score = adaboost_clf.score(X_train_scaled, y_train)

    print(f"\n3ï¸âƒ£ AdaBooståˆ†ç±»å™¨:")
    print(f"  åŸºå­¦ä¹ å™¨æ•°é‡: {adaboost_clf.n_estimators}")
    print(f"  è®­ç»ƒé›†å‡†ç¡®ç‡: {adaboost_score:.4f}")

    # 4. Stacking
    stacking_clf = StackingClassifier(
        estimators=[
            ('lr', LogReg(random_state=42, max_iter=1000)),
            ('dt', DTC(random_state=42, max_depth=5)),
        ],
        final_estimator=LogReg(random_state=42, max_iter=1000),
        cv=3
    )

    stacking_clf.fit(X_train_scaled, y_train)
    stacking_score = stacking_clf.score(X_train_scaled, y_train)

    print(f"\n4ï¸âƒ£ Stackingåˆ†ç±»å™¨:")
    print(f"  åŸºå­¦ä¹ å™¨æ•°é‡: {len(stacking_clf.estimators_)}")
    print(f"  å…ƒå­¦ä¹ å™¨: {type(stacking_clf.final_estimator_).__name__}")
    print(f"  è®­ç»ƒé›†å‡†ç¡®ç‡: {stacking_score:.4f}")

    return AdaBoostClassifier, BaggingClassifier, DTC, LogReg, SVCls, StackingClassifier, VotingClassifier, adaboost_clf, adaboost_score, bagging_clf, bagging_score, stacking_clf, stacking_score, voting_clf, voting_score


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“Š æ¨¡å‹æŒä¹…åŒ–

    è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ä¿å­˜å’ŒåŠ è½½ï¼š
    """
    )
    return


@app.cell
def _():
    import joblib
    import pickle

    print("ğŸ“Š æ¨¡å‹æŒä¹…åŒ–æ¼”ç¤º:")
    print("=" * 50)

    # åˆ›å»ºå¹¶è®­ç»ƒä¸€ä¸ªç®€å•æ¨¡å‹
    from sklearn.linear_model import LogisticRegression as LRModel
    from sklearn.datasets import make_classification as mc

    X_persist, y_persist = mc(n_samples=100, n_features=10, random_state=42)
    model_persist = LRModel(random_state=42, max_iter=1000)
    model_persist.fit(X_persist, y_persist)

    # 1. ä½¿ç”¨joblibä¿å­˜ï¼ˆæ¨èï¼‰
    joblib.dump(model_persist, 'model_joblib.pkl')
    print("\n1ï¸âƒ£ ä½¿ç”¨joblibä¿å­˜:")
    print("  âœ… æ¨¡å‹å·²ä¿å­˜ä¸º model_joblib.pkl")

    # åŠ è½½æ¨¡å‹
    loaded_model_joblib = joblib.load('model_joblib.pkl')
    joblib_score = loaded_model_joblib.score(X_persist, y_persist)
    print(f"  âœ… æ¨¡å‹å·²åŠ è½½ï¼Œå‡†ç¡®ç‡: {joblib_score:.4f}")

    # 2. ä½¿ç”¨pickleä¿å­˜
    with open('model_pickle.pkl', 'wb') as f:
        pickle.dump(model_persist, f)

    print("\n2ï¸âƒ£ ä½¿ç”¨pickleä¿å­˜:")
    print("  âœ… æ¨¡å‹å·²ä¿å­˜ä¸º model_pickle.pkl")

    # åŠ è½½æ¨¡å‹
    with open('model_pickle.pkl', 'rb') as f:
        loaded_model_pickle = pickle.load(f)

    pickle_score = loaded_model_pickle.score(X_persist, y_persist)
    print(f"  âœ… æ¨¡å‹å·²åŠ è½½ï¼Œå‡†ç¡®ç‡: {pickle_score:.4f}")

    print("\nğŸ’¡ æ¨èä½¿ç”¨joblibï¼Œå› ä¸ºå®ƒå¯¹å¤§å‹numpyæ•°ç»„æ›´é«˜æ•ˆ")

    return LRModel, X_persist, joblib, joblib_score, loaded_model_joblib, loaded_model_pickle, mc, model_persist, pickle, pickle_score, y_persist


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

    ä½¿ç”¨scikit-learnæ—¶çš„å…³é”®è¦ç‚¹ï¼š
    """
    )
    return


@app.cell
def _(mo, pd):
    best_practices = {
        "æ•°æ®å‡†å¤‡": [
            "âœ… å§‹ç»ˆåˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†",
            "âœ… ä½¿ç”¨stratifyå‚æ•°ä¿æŒç±»åˆ«åˆ†å¸ƒ",
            "âœ… åœ¨è®­ç»ƒé›†ä¸Šfitï¼Œåœ¨æµ‹è¯•é›†ä¸Štransform",
            "âœ… å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼",
            "âœ… å¯¹ç‰¹å¾è¿›è¡Œé€‚å½“çš„ç¼©æ”¾"
        ],

        "æ¨¡å‹è®­ç»ƒ": [
            "âœ… ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰",
            "âœ… ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹",
            "âœ… è®¾ç½®random_stateä¿è¯å¯å¤ç°æ€§",
            "âœ… ä½¿ç”¨Pipelineç»„ç»‡å·¥ä½œæµ",
            "âœ… ç›‘æ§è®­ç»ƒæ—¶é—´å’Œèµ„æºæ¶ˆè€—"
        ],

        "è¶…å‚æ•°è°ƒä¼˜": [
            "âœ… å…ˆç”¨RandomizedSearchCVå¿«é€Ÿæ¢ç´¢",
            "âœ… å†ç”¨GridSearchCVç²¾ç»†è°ƒä¼˜",
            "âœ… ä½¿ç”¨åˆé€‚çš„è¯„åˆ†æŒ‡æ ‡",
            "âœ… æ³¨æ„è¿‡æ‹Ÿåˆé£é™©",
            "âœ… è€ƒè™‘è®¡ç®—æˆæœ¬å’Œæ—¶é—´"
        ],

        "æ¨¡å‹è¯„ä¼°": [
            "âœ… ä½¿ç”¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡",
            "âœ… ç»˜åˆ¶æ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿",
            "âœ… åˆ†æç‰¹å¾é‡è¦æ€§",
            "âœ… æ£€æŸ¥æ¨¡å‹åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šçš„è¡¨ç°",
            "âœ… è¿›è¡Œé”™è¯¯åˆ†æ"
        ],

        "ç”Ÿäº§éƒ¨ç½²": [
            "âœ… ä½¿ç”¨joblibä¿å­˜æ¨¡å‹",
            "âœ… ä¿å­˜é¢„å¤„ç†å™¨å’Œæ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯",
            "âœ… ç›‘æ§æ¨¡å‹æ€§èƒ½",
            "âœ… å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹",
            "âœ… å®æ–½A/Bæµ‹è¯•"
        ]
    }

    practices_df = pd.DataFrame([
        {"ç±»åˆ«": category, "å®è·µ": practice}
        for category, practices in best_practices.items()
        for practice in practices
    ])

    mo.md(f"""
    ### ğŸ“‹ Scikit-learnæœ€ä½³å®è·µ

    {practices_df.to_markdown(index=False)}

    ### ğŸ”‘ æ ¸å¿ƒåŸåˆ™

    1. **ä¸€è‡´çš„API**: æ‰€æœ‰ä¼°è®¡å™¨éƒ½éµå¾ªfit/predict/transformæ¨¡å¼
    2. **å¯ç»„åˆæ€§**: ä½¿ç”¨Pipelineå’ŒFeatureUnionç»„åˆå¤šä¸ªæ­¥éª¤
    3. **å¯æ£€æŸ¥æ€§**: æ¨¡å‹å‚æ•°å’Œå±æ€§éƒ½å¯ä»¥è®¿é—®
    4. **åˆç†çš„é»˜è®¤å€¼**: å¤§å¤šæ•°å‚æ•°éƒ½æœ‰åˆç†çš„é»˜è®¤è®¾ç½®
    5. **å¯å¤ç°æ€§**: è®¾ç½®random_stateç¡®ä¿ç»“æœå¯å¤ç°

    ### ğŸ“š å¸¸ç”¨æ¨¡å—é€ŸæŸ¥

    | æ¨¡å— | åŠŸèƒ½ | å¸¸ç”¨ç±» |
    |------|------|--------|
    | `sklearn.preprocessing` | æ•°æ®é¢„å¤„ç† | StandardScaler, MinMaxScaler, LabelEncoder |
    | `sklearn.model_selection` | æ¨¡å‹é€‰æ‹© | train_test_split, cross_val_score, GridSearchCV |
    | `sklearn.linear_model` | çº¿æ€§æ¨¡å‹ | LogisticRegression, LinearRegression, Ridge, Lasso |
    | `sklearn.tree` | å†³ç­–æ ‘ | DecisionTreeClassifier, DecisionTreeRegressor |
    | `sklearn.ensemble` | é›†æˆæ–¹æ³• | RandomForest, GradientBoosting, AdaBoost |
    | `sklearn.svm` | æ”¯æŒå‘é‡æœº | SVC, SVR |
    | `sklearn.neighbors` | è¿‘é‚»ç®—æ³• | KNeighborsClassifier, KNeighborsRegressor |
    | `sklearn.cluster` | èšç±» | KMeans, DBSCAN, AgglomerativeClustering |
    | `sklearn.decomposition` | é™ç»´ | PCA, TruncatedSVD, NMF |
    | `sklearn.metrics` | è¯„ä¼°æŒ‡æ ‡ | accuracy_score, f1_score, roc_auc_score |
    | `sklearn.pipeline` | ç®¡é“ | Pipeline, make_pipeline, FeatureUnion |
    | `sklearn.feature_selection` | ç‰¹å¾é€‰æ‹© | SelectKBest, RFE, SelectFromModel |
    """)

    return best_practices, practices, practices_df


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸš€ å®æˆ˜æ¡ˆä¾‹ï¼šå®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹

    è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„æ¡ˆä¾‹æ¥æ¼”ç¤ºscikit-learnçš„å…¸å‹ä½¿ç”¨æµç¨‹ï¼š
    """
    )
    return


@app.cell
def _(np):
    from sklearn.datasets import load_breast_cancer
    from sklearn.model_selection import train_test_split as final_split
    from sklearn.preprocessing import StandardScaler as FinalScaler
    from sklearn.decomposition import PCA as FinalPCA
    from sklearn.ensemble import RandomForestClassifier as FinalRF
    from sklearn.model_selection import cross_val_score as final_cv
    from sklearn.metrics import classification_report as final_report, confusion_matrix as final_cm
    from sklearn.pipeline import Pipeline as FinalPipeline

    print("ğŸš€ å®Œæ•´æœºå™¨å­¦ä¹ æµç¨‹æ¼”ç¤ºï¼šä¹³è…ºç™Œè¯Šæ–­")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®
    cancer_data = load_breast_cancer()
    X_cancer = cancer_data.data
    y_cancer = cancer_data.target

    print(f"\nğŸ“Š æ­¥éª¤1: æ•°æ®åŠ è½½")
    print(f"  æ ·æœ¬æ•°: {X_cancer.shape[0]}")
    print(f"  ç‰¹å¾æ•°: {X_cancer.shape[1]}")
    print(f"  ç±»åˆ«: {cancer_data.target_names}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_cancer)}")

    # 2. æ•°æ®åˆ†å‰²
    X_cancer_train, X_cancer_test, y_cancer_train, y_cancer_test = final_split(
        X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer
    )

    print(f"\nâœ‚ï¸ æ­¥éª¤2: æ•°æ®åˆ†å‰²")
    print(f"  è®­ç»ƒé›†: {X_cancer_train.shape}")
    print(f"  æµ‹è¯•é›†: {X_cancer_test.shape}")

    # 3. æ„å»ºPipeline
    cancer_pipeline = FinalPipeline([
        ('scaler', FinalScaler()),
        ('pca', FinalPCA(n_components=0.95)),
        ('classifier', FinalRF(n_estimators=100, random_state=42))
    ])

    print(f"\nğŸ”— æ­¥éª¤3: æ„å»ºPipeline")
    print(f"  æ­¥éª¤: {[name for name, _ in cancer_pipeline.steps]}")

    # 4. äº¤å‰éªŒè¯
    cv_scores_cancer = final_cv(cancer_pipeline, X_cancer_train, y_cancer_train, cv=5)

    print(f"\nğŸ” æ­¥éª¤4: äº¤å‰éªŒè¯")
    print(f"  5æŠ˜äº¤å‰éªŒè¯å¾—åˆ†: {cv_scores_cancer}")
    print(f"  å¹³å‡å¾—åˆ†: {cv_scores_cancer.mean():.4f} (+/- {cv_scores_cancer.std() * 2:.4f})")

    # 5. è®­ç»ƒæ¨¡å‹
    cancer_pipeline.fit(X_cancer_train, y_cancer_train)

    print(f"\nğŸ“ æ­¥éª¤5: æ¨¡å‹è®­ç»ƒ")
    print(f"  âœ… è®­ç»ƒå®Œæˆ")
    print(f"  PCAä¿ç•™æˆåˆ†æ•°: {cancer_pipeline.named_steps['pca'].n_components_}")
    print(f"  è§£é‡Šæ–¹å·®æ¯”: {cancer_pipeline.named_steps['pca'].explained_variance_ratio_.sum():.4f}")

    # 6. æ¨¡å‹è¯„ä¼°
    y_cancer_pred = cancer_pipeline.predict(X_cancer_test)
    test_score_cancer = cancer_pipeline.score(X_cancer_test, y_cancer_test)

    print(f"\nğŸ“ˆ æ­¥éª¤6: æ¨¡å‹è¯„ä¼°")
    print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score_cancer:.4f}")

    print(f"\n  åˆ†ç±»æŠ¥å‘Š:")
    print(final_report(y_cancer_test, y_cancer_pred, target_names=cancer_data.target_names))

    print(f"  æ··æ·†çŸ©é˜µ:")
    print(final_cm(y_cancer_test, y_cancer_pred))

    # 7. ç‰¹å¾é‡è¦æ€§
    feature_importance_cancer = cancer_pipeline.named_steps['classifier'].feature_importances_
    print(f"\nğŸ¯ æ­¥éª¤7: ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print(f"  å‰5ä¸ªæœ€é‡è¦çš„PCAæˆåˆ†: {sorted(feature_importance_cancer, reverse=True)[:5]}")

    # 8. ä¿å­˜æ¨¡å‹
    import joblib as jl
    jl.dump(cancer_pipeline, 'cancer_diagnosis_model.pkl')

    print(f"\nğŸ’¾ æ­¥éª¤8: æ¨¡å‹ä¿å­˜")
    print(f"  âœ… æ¨¡å‹å·²ä¿å­˜ä¸º cancer_diagnosis_model.pkl")

    print(f"\nğŸ‰ å®Œæ•´æµç¨‹æ¼”ç¤ºå®Œæˆï¼")

    return FinalPCA, FinalPipeline, FinalRF, FinalScaler, X_cancer, X_cancer_test, X_cancer_train, cancer_data, cancer_pipeline, cv_scores_cancer, feature_importance_cancer, final_cm, final_cv, final_report, final_split, jl, load_breast_cancer, test_score_cancer, y_cancer, y_cancer_pred, y_cancer_test, y_cancer_train


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ğŸ“ æ€»ç»“

    é€šè¿‡è¿™ä¸ªäº¤äº’å¼æŒ‡å—ï¼Œæˆ‘ä»¬å…¨é¢æ¢ç´¢äº†scikit-learnçš„æ ¸å¿ƒåŠŸèƒ½ï¼š

    ### ğŸ¯ å…³é”®å­¦ä¹ æˆæœ

    1. **ç†è§£äº†scikit-learnçš„è®¾è®¡å“²å­¦**ï¼šç»Ÿä¸€çš„APIã€å¯ç»„åˆæ€§ã€å¯æ£€æŸ¥æ€§
    2. **æŒæ¡äº†æ•°æ®é¢„å¤„ç†æŠ€æœ¯**ï¼šç¼©æ”¾ã€ç¼–ç ã€ç¼ºå¤±å€¼å¤„ç†
    3. **å­¦ä¹ äº†ç›‘ç£å­¦ä¹ ç®—æ³•**ï¼šåˆ†ç±»å’Œå›å½’çš„å„ç§æ–¹æ³•
    4. **å®è·µäº†æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©**ï¼šäº¤å‰éªŒè¯ã€è¶…å‚æ•°è°ƒä¼˜
    5. **åº”ç”¨äº†Pipelineå·¥ä½œæµ**ï¼šç®€åŒ–å’Œæ ‡å‡†åŒ–æœºå™¨å­¦ä¹ æµç¨‹
    6. **æ¢ç´¢äº†ç‰¹å¾å·¥ç¨‹**ï¼šç‰¹å¾é€‰æ‹©å’Œé™ç»´æŠ€æœ¯
    7. **äº†è§£äº†æ— ç›‘ç£å­¦ä¹ **ï¼šèšç±»ç®—æ³•
    8. **æŒæ¡äº†é›†æˆæ–¹æ³•**ï¼šæå‡æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯
    9. **å­¦ä¼šäº†æ¨¡å‹æŒä¹…åŒ–**ï¼šä¿å­˜å’ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    10. **å®Œæˆäº†ç«¯åˆ°ç«¯æ¡ˆä¾‹**ï¼šä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æµç¨‹

    ### ğŸš€ Scikit-learnçš„æ ¸å¿ƒä¼˜åŠ¿

    - **ç®€å•æ˜“ç”¨**ï¼šä¸€è‡´çš„APIè®¾è®¡ï¼Œæ˜“äºå­¦ä¹ å’Œä½¿ç”¨
    - **åŠŸèƒ½å…¨é¢**ï¼šæ¶µç›–æœºå™¨å­¦ä¹ çš„å„ä¸ªæ–¹é¢
    - **æ€§èƒ½ä¼˜ç§€**ï¼šåº•å±‚ä½¿ç”¨Cythonä¼˜åŒ–ï¼Œè¿è¡Œé«˜æ•ˆ
    - **æ–‡æ¡£å®Œå–„**ï¼šè¯¦ç»†çš„æ–‡æ¡£å’Œä¸°å¯Œçš„ç¤ºä¾‹
    - **ç¤¾åŒºæ´»è·ƒ**ï¼šæŒç»­æ›´æ–°å’Œç»´æŠ¤

    ### ğŸ“š è¿›ä¸€æ­¥å­¦ä¹ èµ„æº

    1. **å®˜æ–¹æ–‡æ¡£**: https://scikit-learn.org/stable/
    2. **ç”¨æˆ·æŒ‡å—**: https://scikit-learn.org/stable/user_guide.html
    3. **APIå‚è€ƒ**: https://scikit-learn.org/stable/modules/classes.html
    4. **ç¤ºä¾‹åº“**: https://scikit-learn.org/stable/auto_examples/index.html
    5. **æ•™ç¨‹**: https://scikit-learn.org/stable/tutorial/index.html

    ### ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

    1. åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨scikit-learn
    2. å°è¯•ä¸åŒçš„ç®—æ³•å’Œå‚æ•°ç»„åˆ
    3. å‚ä¸Kaggleç«èµ›å®è·µ
    4. é˜…è¯»scikit-learnæºç æ·±å…¥ç†è§£
    5. ä¸ºå¼€æºç¤¾åŒºåšè´¡çŒ®

    Scikit-learnæ˜¯Pythonæœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿçš„åŸºçŸ³ï¼ŒæŒæ¡å®ƒå°†ä¸ºä½ çš„æ•°æ®ç§‘å­¦ä¹‹æ—…æ‰“ä¸‹åšå®çš„åŸºç¡€ï¼ğŸ‰
    """
    )
    return


@app.cell
def _():
    from datetime import timedelta
    return (timedelta,)


if __name__ == "__main__":
    app.run()

