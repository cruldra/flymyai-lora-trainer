import marimo

__generated_with = "0.16.2"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # å®Œæ•´çš„MLOpsè“å›¾ï¼šæ•°æ®å’Œç®¡é“å·¥ç¨‹â€”ç¬¬Aéƒ¨åˆ†ï¼ˆå«å®ç°ï¼‰

    MLOpså’ŒLLMOpsé€Ÿæˆè¯¾ç¨‹â€”ç¬¬5éƒ¨åˆ†

    ## å›é¡¾

    åœ¨æˆ‘ä»¬æ·±å…¥è¿™ä¸ªMLOpså’ŒLLMOpsé€Ÿæˆè¯¾ç¨‹çš„ç¬¬5éƒ¨åˆ†ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹ä¸Šä¸€éƒ¨åˆ†æ¶µç›–çš„å†…å®¹ã€‚

    åœ¨ç¬¬4éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å…³äºå¯é‡ç°æ€§å’Œç‰ˆæœ¬æ§åˆ¶çš„è®¨è®ºæ‰©å±•åˆ°äº†ä½¿ç”¨Weights & Biases (W&B)çš„å®é™…æ¢ç´¢ã€‚

    ![W&Bæ¦‚è§ˆ](https://www.dailydoseofds.com/content/images/2025/08/image-111.png)

    æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†W&Bã€å…¶æ ¸å¿ƒç†å¿µï¼Œä»¥åŠå®ƒä¸MLflowçš„å¹¶æ’æ¯”è¾ƒã€‚

    å…³é”®è¦ç‚¹å¾ˆæ˜ç¡®ï¼šMLflow vs W&Bä¸æ˜¯å…³äºå“ªä¸ªæ›´å¥½ï¼Œè€Œæ˜¯ä¸ºä½ çš„ç”¨ä¾‹é€‰æ‹©æ­£ç¡®çš„å·¥å…·ã€‚

    ![å·¥å…·å¯¹æ¯”](https://www.dailydoseofds.com/content/images/2025/08/image-removebg-preview.png)

    ä»é‚£é‡Œï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®é™…æ“ä½œã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªæ¼”ç¤ºæ¢ç´¢äº†ä½¿ç”¨W&Bçš„å®éªŒè·Ÿè¸ªå’Œç‰ˆæœ¬æ§åˆ¶ï¼š

    - **ä½¿ç”¨scikit-learnè¿›è¡Œé¢„æµ‹å»ºæ¨¡**ï¼Œæˆ‘ä»¬ï¼š
        - è®°å½•æŒ‡æ ‡
        - è·Ÿè¸ªå®éªŒ
        - ç®¡ç†å·¥ä»¶
        - æ³¨å†Œæ¨¡å‹

    ![scikit-learnæ¼”ç¤º](https://www.dailydoseofds.com/content/images/2025/08/image-114.png)

    - **ä½¿ç”¨PyTorchè¿›è¡Œæ—¶é—´åºåˆ—é”€å”®é¢„æµ‹**ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š
        - æ„å»ºå¤šæ­¥éª¤ç®¡é“
        - W&Bçš„æ·±åº¦å­¦ä¹ é›†æˆ
        - è®°å½•å·¥ä»¶
        - æ¨¡å‹æ£€æŸ¥ç‚¹

    ![PyTorchæ¼”ç¤º](https://www.dailydoseofds.com/content/images/2025/08/image-113.png)

    å¦‚æœä½ è¿˜æ²¡æœ‰æŸ¥çœ‹ç¬¬4éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®å…ˆé˜…è¯»å®ƒï¼Œå› ä¸ºå®ƒä¸ºå³å°†åˆ°æ¥çš„å†…å®¹å¥ å®šäº†åŸºç¡€å’Œæµç¨‹ã€‚

    åœ¨æœ¬ç« ä»¥åŠæ¥ä¸‹æ¥çš„å‡ ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»ç³»ç»Ÿè§’åº¦æ¢ç´¢æ•°æ®å’Œç®¡é“å·¥ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µã€‚è¿™ä¸ªé˜¶æ®µå½¢æˆäº†æ”¯æŒMLOpsç”Ÿå‘½å‘¨æœŸä¸­æ‰€æœ‰åç»­é˜¶æ®µå®ç°çš„ç»“æ„æ€§éª¨å¹²ã€‚

    ![MLOpsç”Ÿå‘½å‘¨æœŸ](https://www.dailydoseofds.com/content/images/2025/08/image-115.png)

    æˆ‘ä»¬å°†è®¨è®ºï¼š

    - **æ•°æ®æºå’Œæ ¼å¼**
    - **ETLç®¡é“**
    - **å®é™…å®ç°**

    ä¸€å¦‚æ—¢å¾€ï¼Œæ¯ä¸ªæƒ³æ³•å’Œæ¦‚å¿µéƒ½å°†å¾—åˆ°å…·ä½“ç¤ºä¾‹ã€æ¼”ç»ƒå’Œå®ç”¨æŠ€å·§çš„æ”¯æŒï¼Œå¸®åŠ©ä½ æŒæ¡æƒ³æ³•å’Œå®ç°ã€‚

    è®©æˆ‘ä»¬å¼€å§‹å§ï¼

    ---

    ## å¼•è¨€ï¼šç†è§£æ•°æ®æ™¯è§‚

    åœ¨æœºå™¨å­¦ä¹ è¿è¥(MLOps)ä¸­ï¼ŒæˆåŠŸä¸ä»…å–å†³äºæ¨¡å‹ï¼Œè¿˜å–å†³äºä¸ºè¿™äº›æ¨¡å‹æä¾›æ•°æ®çš„æ•°æ®ç®¡é“ã€‚

    ![æ•°æ®ç®¡é“é‡è¦æ€§](https://www.dailydoseofds.com/content/images/2025/08/image-117-1.png)

    ç”Ÿäº§æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªå®Œå…¨ä¸åŒçš„é‡å…½ã€‚

    åœ¨è¿™é‡Œï¼Œå¦‚æœæœ€èªæ˜çš„æ¨¡å‹æ¶æ„è¢«æä¾›ä¸å¯é çš„æ•°æ®ï¼Œæˆ–è€…å¦‚æœå…¶é¢„æµ‹æ— æ³•é‡ç°ï¼ˆæ­£å¦‚æˆ‘ä»¬åœ¨æ—©æœŸéƒ¨åˆ†çœ‹åˆ°çš„ï¼‰ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯æ¯«æ— ä»·å€¼çš„ã€‚

    å› æ­¤ï¼Œè‡³å…³é‡è¦çš„æ˜¯è¦ç†è§£ï¼Œåœ¨MLä¸–ç•Œä¸­ï¼ŒåŸææ–™æ˜¯æ•°æ®ï¼Œæˆ‘ä»¬ä¸ºæ•°æ®åšå‡ºçš„é€‰æ‹©å¯¹æˆ‘ä»¬æ•´ä¸ªMLç³»ç»Ÿçš„æ€§èƒ½ã€å¯æ‰©å±•æ€§å’Œå¯é æ€§æœ‰æ·±è¿œçš„ä¸‹æ¸¸åæœã€‚

    ![æ•°æ®å†³ç­–å½±å“](https://www.dailydoseofds.com/content/images/2025/08/image-116-1.png)

    ç”±äºä¸Šè¿°äº‹å®ï¼Œåœ¨ä¼ä¸šMLOpsç¯å¢ƒä¸­ï¼Œå·¥ç¨‹å¸ˆåœ¨ä¸€ä¸ªåŸºæœ¬çœŸç†ä¸‹è¿ä½œï¼š**æ¨¡å‹é€šå¸¸æ˜¯å•†å“ï¼Œä½†æ•°æ®å’Œå¤„ç†å®ƒçš„ç®¡é“æ˜¯é©±åŠ¨ä¸šåŠ¡ä»·å€¼çš„æŒä¹…ã€å¯é˜²å¾¡çš„èµ„äº§ã€‚**

    ---

    ## ä»åŸå§‹ä¿¡å·è·å–ç»“æ„åŒ–ä¿¡æ¯

    ç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®ä¸æ˜¯ä¸€ä¸ªå¹²å‡€ã€é™æ€çš„CSVæ–‡ä»¶ã€‚å®ƒæ˜¯æ¥è‡ªå¤šä¸ªæºçš„åŠ¨æ€ã€æ··ä¹±å’Œè¿ç»­çš„ä¿¡å·æµï¼Œæ¯ä¸ªæºéƒ½æœ‰è‡ªå·±çš„ç‰¹å¾å’Œè¦æ±‚ã€‚

    ### æ•°æ®æº

    ç”Ÿäº§MLç³»ç»Ÿä¸æ¥è‡ªå¤šä¸ªæ¥æºçš„æ•°æ®äº¤äº’ï¼Œä¾‹å¦‚ï¼š

    #### ç”¨æˆ·è¾“å…¥æ•°æ®

    è¿™æ˜¯ç”¨æˆ·æ˜ç¡®æä¾›çš„æ•°æ®ï¼Œä¾‹å¦‚æœç´¢æ ä¸­çš„æ–‡æœ¬ã€ä¸Šä¼ çš„å›¾åƒæˆ–è¡¨å•æäº¤ã€‚

    è¿™ä¸ªæ•°æ®æºæ˜¯å‡ºäº†åçš„ä¸å¯é ï¼Œå› ä¸ºç”¨æˆ·é€šå¸¸å¾ˆæ‡’ï¼Œå¦‚æœç”¨æˆ·å¯èƒ½è¾“å…¥æœªæ ¼å¼åŒ–å’ŒåŸå§‹æ•°æ®ï¼Œä»–ä»¬å°±ä¼šè¿™æ ·åšã€‚å› æ­¤ï¼Œè¿™äº›æ•°æ®éœ€è¦é‡å‹éªŒè¯å’Œå¼ºå¤§çš„é”™è¯¯å¤„ç†ã€‚

    #### ç³»ç»Ÿç”Ÿæˆçš„æ•°æ®ï¼ˆæ—¥å¿—ï¼‰

    ![ç³»ç»Ÿæ—¥å¿—](https://www.dailydoseofds.com/content/images/2025/08/image-118-1.png)

    åº”ç”¨ç¨‹åºå’ŒåŸºç¡€è®¾æ–½ç”Ÿæˆå¤§é‡æ—¥å¿—ã€‚

    è¿™äº›æ—¥å¿—è®°å½•é‡è¦äº‹ä»¶ã€ç³»ç»ŸçŠ¶æ€ï¼ˆå¦‚å†…å­˜ä½¿ç”¨ï¼‰ã€æœåŠ¡è°ƒç”¨å’Œæ¨¡å‹é¢„æµ‹ã€‚

    è™½ç„¶é€šå¸¸å¾ˆå˜ˆæ‚ï¼Œä½†æ—¥å¿—å¯¹äºè°ƒè¯•ã€ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶å†µä»¥åŠå¯¹æˆ‘ä»¬æ¥è¯´è‡³å…³é‡è¦çš„æ˜¯ï¼Œä¸ºæˆ‘ä»¬çš„MLç³»ç»Ÿæä¾›å¯è§æ€§æ˜¯æ— ä»·çš„ã€‚

    å¯¹äºè®¸å¤šç”¨ä¾‹ï¼Œæ—¥å¿—å¯ä»¥æ‰¹é‡å¤„ç†ï¼ˆå¦‚æ¯æ—¥æˆ–æ¯å‘¨ï¼‰ï¼Œä½†å¯¹äºå®æ—¶ç›‘æ§å’Œè­¦æŠ¥ï¼Œéœ€è¦æ›´å¿«çš„å¤„ç†ã€‚

    #### å†…éƒ¨æ•°æ®åº“

    è¿™æ˜¯ä¼ä¸šé€šå¸¸ä»ä¸­è·å¾—æœ€å¤§ä»·å€¼çš„åœ°æ–¹ã€‚

    ç®¡ç†åº“å­˜ã€å®¢æˆ·å…³ç³»(CRM)ã€ç”¨æˆ·è´¦æˆ·å’Œé‡‘èäº¤æ˜“çš„æ•°æ®åº“é€šå¸¸æ˜¯ç‰¹å¾å·¥ç¨‹æœ€æœ‰ä»·å€¼çš„æ¥æºã€‚è¿™äº›æ•°æ®é€šå¸¸é«˜åº¦ç»“æ„åŒ–å¹¶éµå¾ªå…³ç³»æ¨¡å‹ã€‚

    ä¾‹å¦‚ï¼Œæ¨èæ¨¡å‹å¯èƒ½å¤„ç†ç”¨æˆ·çš„æŸ¥è¯¢ï¼Œä½†å®ƒå¿…é¡»æ£€æŸ¥å†…éƒ¨åº“å­˜æ•°æ®åº“ä»¥ç¡®ä¿æ¨èçš„äº§å“å®é™…ä¸Šæœ‰åº“å­˜ï¼Œç„¶åæ‰èƒ½æ˜¾ç¤ºå®ƒä»¬ã€‚

    #### ç¬¬ä¸‰æ–¹æ•°æ®

    ![ç¬¬ä¸‰æ–¹æ•°æ®](https://www.dailydoseofds.com/content/images/2025/08/image-119.png)

    è¿™æ˜¯ä»å¤–éƒ¨ä¾›åº”å•†è·å¾—çš„æ•°æ®ã€‚å®ƒå¯ä»¥ä»äººå£ç»Ÿè®¡ä¿¡æ¯å’Œç¤¾äº¤åª’ä½“æ´»åŠ¨åˆ°è´­ä¹°ä¹ æƒ¯ã€‚è™½ç„¶å®ƒå¯¹äºå¼•å¯¼æ¨èç³»ç»Ÿç­‰æ¨¡å‹å¯èƒ½å¾ˆå¼ºå¤§ï¼Œä½†å…¶å¯ç”¨æ€§è¶Šæ¥è¶Šå—åˆ°éšç§æ³•è§„çš„é™åˆ¶ã€‚

    ç°åœ¨æˆ‘ä»¬å¹¿æ³›äº†è§£äº†MLç³»ç»Ÿçš„æ•°æ®æ¥æºï¼Œè®©æˆ‘ä»¬ä¹Ÿç»§ç»­äº†è§£MLç®¡é“ä¸Šä¸‹æ–‡ä¸­ä¸€äº›é‡è¦çš„æ•°æ®æ ¼å¼ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### æ•°æ®æ ¼å¼

    ä½ ä¸ºå­˜å‚¨é€‰æ‹©çš„æ ¼å¼æ˜¯ä¸€ä¸ªå…³é”®çš„æ¶æ„å†³ç­–ï¼Œç›´æ¥å½±å“å­˜å‚¨æˆæœ¬ã€è®¿é—®é€Ÿåº¦å’Œæ˜“ç”¨æ€§ã€‚éœ€è¦ç†è§£çš„ä¸¤ä¸ªæœ€é‡è¦çš„äºŒåˆ†æ³•æ˜¯æ–‡æœ¬ä¸äºŒè¿›åˆ¶ä»¥åŠè¡Œä¸»åºä¸åˆ—ä¸»åºæ ¼å¼ã€‚

    #### æ–‡æœ¬ vs. äºŒè¿›åˆ¶

    åƒJSONå’ŒCSVè¿™æ ·çš„æ–‡æœ¬æ ¼å¼æ˜¯äººç±»å¯è¯»çš„ã€‚ä½ å¯ä»¥åœ¨æ–‡æœ¬ç¼–è¾‘å™¨ä¸­æ‰“å¼€JSONæˆ–CSVæ–‡ä»¶å¹¶ç«‹å³ç†è§£å…¶å†…å®¹ã€‚

    ![æ–‡æœ¬æ ¼å¼](https://www.dailydoseofds.com/content/images/2025/08/image-121-1.png)

    è¿™ä½¿å®ƒä»¬éå¸¸é€‚åˆè°ƒè¯•ã€é…ç½®å’Œç³»ç»Ÿé—´çš„æ•°æ®äº¤æ¢ã€‚ç‰¹åˆ«æ˜¯JSONï¼Œç”±äºå…¶ç®€å•æ€§å’Œçµæ´»æ€§ï¼Œæ— å¤„ä¸åœ¨ï¼Œèƒ½å¤Ÿè¡¨ç¤ºç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®ã€‚

    ç„¶è€Œï¼Œè¿™ç§å¯è¯»æ€§æ˜¯æœ‰ä»£ä»·çš„ï¼šæ–‡æœ¬æ–‡ä»¶å†—é•¿ä¸”æ¶ˆè€—æ˜¾è‘—æ›´å¤šçš„å­˜å‚¨ç©ºé—´ã€‚å°†æ•°å­—`1000000`å­˜å‚¨ä¸ºæ–‡æœ¬éœ€è¦7ä¸ªå­—ç¬¦ï¼ˆå› æ­¤åœ¨ASCIIä¸­æ˜¯7å­—èŠ‚ï¼‰ï¼Œè€Œåœ¨äºŒè¿›åˆ¶æ ¼å¼ä¸­å°†å…¶å­˜å‚¨ä¸º32ä½æ•´æ•°åªéœ€è¦4å­—èŠ‚ã€‚

    ç°åœ¨ï¼Œè°ˆåˆ°åƒParquetè¿™æ ·çš„äºŒè¿›åˆ¶æ ¼å¼ï¼Œè¿™äº›æ ¼å¼ä¸æ˜¯äººç±»å¯è¯»çš„ï¼Œæ˜¯ä¸ºæœºå™¨æ¶ˆè´¹è€Œè®¾è®¡çš„ã€‚

    å®ƒä»¬æ›´ç´§å‡‘ä¸”å¤„ç†æ•ˆç‡æ›´é«˜ã€‚ç¨‹åºå¿…é¡»çŸ¥é“äºŒè¿›åˆ¶æ–‡ä»¶çš„ç¡®åˆ‡æ¨¡å¼å’Œå¸ƒå±€æ‰èƒ½è§£é‡Šå…¶å­—èŠ‚ã€‚

    ![äºŒè¿›åˆ¶æ ¼å¼](https://www.dailydoseofds.com/content/images/2025/08/image-122.png)

    ç©ºé—´èŠ‚çœå¯èƒ½æ˜¯æˆå‰§æ€§çš„ï¼›ä¾‹å¦‚ï¼Œ14 MBçš„CSVæ–‡ä»¶åœ¨è½¬æ¢ä¸ºäºŒè¿›åˆ¶Parquetæ ¼å¼æ—¶å¯ä»¥å‡å°‘åˆ°6 MBã€‚å¯¹äºå¤§è§„æ¨¡åˆ†æå·¥ä½œè´Ÿè½½ï¼ŒåƒParquetè¿™æ ·çš„äºŒè¿›åˆ¶æ ¼å¼æ˜¯è¡Œä¸šæ ‡å‡†ã€‚

    #### è¡Œä¸»åº vs. åˆ—ä¸»åº

    è¿™ç§åŒºåˆ«å¯¹äºMLå·¥ç¨‹å¸ˆæ¥è¯´å¯èƒ½æ˜¯æœ€å…³é”®çš„ï¼Œå› ä¸ºå®ƒç›´æ¥å…³ç³»åˆ°æˆ‘ä»¬é€šå¸¸å¦‚ä½•è®¿é—®æ•°æ®è¿›è¡Œè®­ç»ƒå’Œåˆ†æã€‚å®ƒæè¿°äº†æ•°æ®åœ¨å†…å­˜ä¸­çš„å¸ƒå±€æ–¹å¼ã€‚

    ![è¡Œåˆ—å­˜å‚¨å¯¹æ¯”](https://www.dailydoseofds.com/content/images/2025/08/image-123.png)

    åœ¨**è¡Œä¸»åºæ ¼å¼**ï¼ˆå¦‚CSVï¼‰ä¸­ï¼Œè¡Œçš„è¿ç»­å…ƒç´ å­˜å‚¨åœ¨å½¼æ­¤æ—è¾¹ã€‚æŠŠå®ƒæƒ³è±¡æˆé€è¡Œè¯»å–è¡¨æ ¼ã€‚è¿™ç§å¸ƒå±€é’ˆå¯¹å†™å…¥å¯†é›†å‹å·¥ä½œè´Ÿè½½è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨è¿™äº›å·¥ä½œè´Ÿè½½ä¸­ä½ ç»å¸¸æ·»åŠ æ–°çš„ã€å®Œæ•´çš„è®°å½•ï¼ˆè¡Œï¼‰ã€‚

    å¦‚æœä½ çš„ä¸»è¦è®¿é—®æ¨¡å¼æ˜¯ä¸€æ¬¡æ£€ç´¢æ•´ä¸ªæ ·æœ¬ï¼Œä¾‹å¦‚è·å–ç‰¹å®šç”¨æˆ·IDçš„æ‰€æœ‰æ•°æ®ï¼Œå®ƒä¹Ÿå¾ˆé«˜æ•ˆã€‚

    åœ¨**åˆ—ä¸»åºæ ¼å¼**ï¼ˆå¦‚Parquetï¼‰ä¸­ï¼Œåˆ—çš„è¿ç»­å…ƒç´ å­˜å‚¨åœ¨å½¼æ­¤æ—è¾¹ã€‚è¿™é’ˆå¯¹åˆ†ææŸ¥è¯¢è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¿™åœ¨æœºå™¨å­¦ä¹ ä¸­å¾ˆå¸¸è§ã€‚è€ƒè™‘è®¡ç®—æ•°ç™¾ä¸‡æ ·æœ¬ä¸­å•ä¸ªç‰¹å¾çš„å¹³å‡å€¼çš„ä»»åŠ¡ã€‚

    åœ¨åˆ—ä¸»åºæ ¼å¼ä¸­ï¼Œç³»ç»Ÿå¯ä»¥å°†è¯¥åˆ—ä½œä¸ºå•ä¸ªè¿ç»­çš„å†…å­˜å—è¯»å–ï¼Œè¿™éå¸¸é«˜æ•ˆä¸”å¯¹ç¼“å­˜å‹å¥½ã€‚åœ¨è¡Œä¸»åºæ ¼å¼ä¸­ï¼Œå®ƒå¿…é¡»åœ¨å†…å­˜ä¸­è·³è·ƒï¼Œä»æ¯è¡Œè¯»å–ä¸€å°å—æ•°æ®ï¼Œè¿™æ˜¾è‘—æ›´æ…¢ã€‚

    ![æ€§èƒ½å¯¹æ¯”](https://www.dailydoseofds.com/content/images/2025/08/image-124.png)

    æ€§èƒ½å½±å“å¹¶ä¸å¾®å¦™ã€‚ä¾‹å¦‚ï¼Œæµè¡Œçš„`pandas`åº“æ˜¯å›´ç»•åˆ—ä¸»åº`DataFrame`æ„å»ºçš„ã€‚

    ä¸€ä¸ªå¸¸è§åœºæ™¯æ˜¯é€è¡Œè¿­ä»£`DataFrame`ã€‚è¿™å¯èƒ½æ¯”é€åˆ—è¿­ä»£æ…¢å‡ ä¸ªæ•°é‡çº§ã€‚

    è¿™ä¸ªä»£ç ç‰‡æ®µæ˜¾ç¤ºï¼ŒæŒ‰åˆ—è¿­ä»£32M+è¡Œçš„`DataFrame`åªéœ€ä¸åˆ°2å¾®ç§’ï¼Œè€ŒæŒ‰è¡Œè¿­ä»£ç›¸åŒçš„`DataFrame`éœ€è¦38å¾®ç§’ï¼Œè¿™æ˜¯çº¦20å€çš„å·®å¼‚ã€‚

    ![Pandasè¿­ä»£æ€§èƒ½](https://www.dailydoseofds.com/content/images/2025/08/pandas-iteration.jpeg)

    è¿™æ˜¯å› ä¸ºï¼Œå¦‚ä¸Šæ‰€è¿°ï¼ŒPandas DataFrameæ˜¯ä¸€ä¸ªåˆ—ä¸»åºæ•°æ®ç»“æ„ï¼Œè¿™æ„å‘³ç€åˆ—ä¸­çš„è¿ç»­å…ƒç´ åœ¨å†…å­˜ä¸­å­˜å‚¨åœ¨å½¼æ­¤æ—è¾¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

    ![å†…å­˜å¸ƒå±€](https://substackcdn.com/image/fetch/$s_!4ZSx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67161b70-c600-4467-a407-0cbb2c0667fa_2640x1144.png)
    *åˆ—åœ¨å†…å­˜ä¸­å­˜å‚¨æ–¹å¼çš„ç®€åŒ–ç‰ˆæœ¬*

    å„ä¸ªåˆ—å¯èƒ½åˆ†å¸ƒåœ¨å†…å­˜ä¸­çš„ä¸åŒä½ç½®ã€‚ç„¶è€Œï¼Œæ¯åˆ—çš„å…ƒç´ æ€»æ˜¯åœ¨ä¸€èµ·ã€‚

    ç”±äºå¤„ç†å™¨å¯¹è¿ç»­å†…å­˜å—æ›´é«˜æ•ˆï¼Œæ£€ç´¢åˆ—æ¯”è·å–è¡Œå¿«å¾—å¤šã€‚

    ![è¡Œè®¿é—®æ¨¡å¼](https://substackcdn.com/image/fetch/$s_!B69T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97dc86f0-e5db-41fd-bb2a-425ed2d0a69d_3274x1353.png)
    *è¡Œè®¿é—®æ—¶çš„å†…å­˜è®¿é—®æ¨¡å¼*

    æ¢å¥è¯è¯´ï¼Œåœ¨è¿­ä»£æ—¶ï¼Œæ¯è¡Œéƒ½æ˜¯é€šè¿‡è®¿é—®éè¿ç»­çš„å†…å­˜å—æ¥æ£€ç´¢çš„ã€‚å¤„ç†å™¨å¿…é¡»ä¸æ–­ä»ä¸€ä¸ªå†…å­˜ä½ç½®ç§»åŠ¨åˆ°å¦ä¸€ä¸ªå†…å­˜ä½ç½®æ¥è·å–æ‰€æœ‰è¡Œå…ƒç´ ã€‚

    ç»“æœï¼Œè¿è¡Œæ—¶é—´æ€¥å‰§å¢åŠ ã€‚

    è¿™ä¸æ˜¯pandasçš„ç¼ºé™·ï¼›è¿™æ˜¯å…¶åº•å±‚åˆ—ä¸»åºæ•°æ®æ¨¡å‹çš„ç›´æ¥åæœã€‚

    æœ‰äº†å¯¹æ•°æ®æ¥æºå’Œæ ¼å¼çš„ç†è§£ï¼Œè®©æˆ‘ä»¬ç°åœ¨æ¢ç´¢æ•°æ®å·¥ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µä¹‹ä¸€ï¼šETLç®¡é“ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## æ•°æ®å·¥ç¨‹åŸºç¡€ï¼šETL

    åœ¨æ„å»ºç®¡é“ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åœ¨MLä¸Šä¸‹æ–‡ä¸­å¯¹æ•°æ®å·¥ç¨‹åŸºç¡€æœ‰æ‰å®çš„æŒæ¡ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åŸå§‹æ•°æ®å¦‚ä½•è¢«æå–ã€å¤„ç†å¹¶ç»„ç»‡ç”¨äºæœºå™¨å­¦ä¹ å·¥ä½œæµã€‚

    ### MLå·¥ä½œæµä¸­çš„ETL

    ETLä»£è¡¨æå–(Extract)ã€è½¬æ¢(Transform)ã€åŠ è½½(Load)ã€‚å®ƒæè¿°äº†ä»æºè·å–æ•°æ®ã€å°†å…¶å¤„ç†æˆå¯ç”¨å½¢å¼ï¼Œå¹¶å°†å…¶åŠ è½½åˆ°å­˜å‚¨æˆ–ç³»ç»Ÿä¸­ä¾›ä½¿ç”¨çš„ç®¡é“ã€‚ETLé€šå¸¸æ˜¯ä¸ºæ¨¡å‹è®­ç»ƒæˆ–æ¨ç†å‡†å¤‡æ•°æ®çš„ç¬¬ä¸€é˜¶æ®µã€‚

    ![ETLæµç¨‹](https://www.dailydoseofds.com/content/images/2025/08/image-125.png)

    è®©æˆ‘ä»¬ç®€è¦åœ°ä»ç†è®ºä¸Šè®¨è®ºæ¯ä¸ªé˜¶æ®µï¼š

    #### æå–(Extract)

    è¿™æ¶‰åŠä»å„ç§æ•°æ®æºæ”¶é›†åŸå§‹æ•°æ®ã€‚åœ¨MLä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™å¯èƒ½æ„å‘³ç€ï¼š

    - **ä»æ•°æ®åº“æŸ¥è¯¢**ï¼šä½¿ç”¨SQLä»å…³ç³»æ•°æ®åº“æå–è®°å½•
    - **APIè°ƒç”¨**ï¼šä»REST APIæˆ–GraphQLç«¯ç‚¹è·å–æ•°æ®
    - **æ–‡ä»¶è¯»å–**ï¼šå¤„ç†CSVã€JSONã€Parquetæˆ–å…¶ä»–æ ¼å¼çš„æ–‡ä»¶
    - **æµæ•°æ®**ï¼šä»Kafkaã€Kinesisæˆ–å…¶ä»–æµå¹³å°æ¶ˆè´¹å®æ—¶æ•°æ®
    - **æ—¥å¿—è§£æ**ï¼šä»åº”ç”¨ç¨‹åºæ—¥å¿—ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯

    **å…³é”®æŒ‘æˆ˜**ï¼š

    - å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼å’Œæ¨¡å¼
    - ç®¡ç†APIé€Ÿç‡é™åˆ¶å’Œè¶…æ—¶
    - å¤„ç†å¤§å‹æ•°æ®é›†çš„å†…å­˜é™åˆ¶
    - ç¡®ä¿æ•°æ®æå–çš„ä¸€è‡´æ€§å’Œå¯é æ€§

    #### è½¬æ¢(Transform)

    è¿™æ˜¯æ•°æ®è¢«æ¸…ç†ã€éªŒè¯ã€ä¸°å¯Œå’Œé‡æ„ä»¥æ»¡è¶³MLæ¨¡å‹è¦æ±‚çš„é˜¶æ®µã€‚å¸¸è§çš„è½¬æ¢åŒ…æ‹¬ï¼š

    **æ•°æ®æ¸…ç†**ï¼š

    - å¤„ç†ç¼ºå¤±å€¼ï¼ˆæ’è¡¥ã€åˆ é™¤æˆ–æ ‡è®°ï¼‰
    - å»é™¤é‡å¤è®°å½•
    - ä¿®æ­£æ•°æ®ç±»å‹ä¸ä¸€è‡´
    - å¤„ç†å¼‚å¸¸å€¼

    **ç‰¹å¾å·¥ç¨‹**ï¼š

    - åˆ›å»ºæ–°ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œä»æ—¥æœŸæå–æ˜ŸæœŸå‡ ï¼‰
    - ç‰¹å¾ç¼©æ”¾å’Œæ ‡å‡†åŒ–
    - åˆ†ç±»å˜é‡ç¼–ç ï¼ˆç‹¬çƒ­ç¼–ç ã€æ ‡ç­¾ç¼–ç ï¼‰
    - æ–‡æœ¬é¢„å¤„ç†ï¼ˆæ ‡è®°åŒ–ã€è¯å¹²æå–ï¼‰

    **æ•°æ®éªŒè¯**ï¼š

    - æ¨¡å¼éªŒè¯ï¼ˆç¡®ä¿åˆ—å­˜åœ¨ä¸”ç±»å‹æ­£ç¡®ï¼‰
    - æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆèŒƒå›´éªŒè¯ã€æ ¼å¼æ£€æŸ¥ï¼‰
    - ä¸šåŠ¡è§„åˆ™éªŒè¯

    **æ•°æ®èšåˆ**ï¼š

    - æŒ‰æ—¶é—´çª—å£æˆ–å…¶ä»–ç»´åº¦æ±‡æ€»æ•°æ®
    - è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€ç™¾åˆ†ä½æ•°ï¼‰
    - åˆ›å»ºç‰¹å¾äº¤å‰

    #### åŠ è½½(Load)

    æœ€åé˜¶æ®µæ¶‰åŠå°†è½¬æ¢åçš„æ•°æ®å­˜å‚¨åœ¨ç›®æ ‡ç³»ç»Ÿä¸­ï¼Œå‡†å¤‡ç”¨äºMLè®­ç»ƒæˆ–æ¨ç†ï¼š

    **å­˜å‚¨é€‰é¡¹**ï¼š

    - **æ•°æ®ä»“åº“**ï¼šå¦‚Snowflakeã€BigQueryã€Redshiftç”¨äºåˆ†æå·¥ä½œè´Ÿè½½
    - **æ•°æ®æ¹–**ï¼šå¦‚S3ã€HDFSç”¨äºåŸå§‹å’Œå¤„ç†è¿‡çš„æ•°æ®
    - **ç‰¹å¾å­˜å‚¨**ï¼šä¸“é—¨çš„ç³»ç»Ÿç”¨äºMLç‰¹å¾ç®¡ç†
    - **æ•°æ®åº“**ï¼šå…³ç³»å‹æˆ–NoSQLæ•°æ®åº“ç”¨äºæ“ä½œæ•°æ®
    - **ç¼“å­˜ç³»ç»Ÿ**ï¼šå¦‚Redisç”¨äºå¿«é€Ÿè®¿é—®é¢‘ç¹ä½¿ç”¨çš„æ•°æ®

    **åŠ è½½ç­–ç•¥**ï¼š

    - **æ‰¹é‡åŠ è½½**ï¼šå®šæœŸï¼ˆæ¯æ—¥ã€æ¯å°æ—¶ï¼‰å¤„ç†å¤§é‡æ•°æ®
    - **å¢é‡åŠ è½½**ï¼šåªå¤„ç†æ–°çš„æˆ–æ›´æ”¹çš„æ•°æ®
    - **å®æ—¶åŠ è½½**ï¼šè¿ç»­å¤„ç†æµæ•°æ®
    - **æ··åˆæ–¹æ³•**ï¼šç»“åˆæ‰¹é‡å’Œå®æ—¶å¤„ç†

    ### ETL vs ELTï¼šç°ä»£æ•°æ®å·¥ç¨‹çš„æ¼”è¿›

    ä¼ ç»Ÿçš„ETLæ–¹æ³•åœ¨æ•°æ®åŠ è½½åˆ°ç›®æ ‡ç³»ç»Ÿä¹‹å‰è¿›è¡Œè½¬æ¢ã€‚ç„¶è€Œï¼Œéšç€äº‘æ•°æ®ä»“åº“å’Œå¤§æ•°æ®å¤„ç†èƒ½åŠ›çš„å‡ºç°ï¼ŒELTï¼ˆæå–-åŠ è½½-è½¬æ¢ï¼‰æ¨¡å¼å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚

    #### ELTçš„ä¼˜åŠ¿

    **æ›´å¿«çš„æ•°æ®å¯ç”¨æ€§**ï¼š

    - åŸå§‹æ•°æ®ç«‹å³å¯ç”¨äºæ¢ç´¢
    - ä¸éœ€è¦ç­‰å¾…æ‰€æœ‰è½¬æ¢å®Œæˆ

    **çµæ´»æ€§**ï¼š

    - å¯ä»¥å¯¹ç›¸åŒçš„åŸå§‹æ•°æ®åº”ç”¨å¤šç§è½¬æ¢
    - æ›´å®¹æ˜“é€‚åº”ä¸æ–­å˜åŒ–çš„ä¸šåŠ¡éœ€æ±‚

    **å¯æ‰©å±•æ€§**ï¼š

    - åˆ©ç”¨ç°ä»£æ•°æ®ä»“åº“çš„è®¡ç®—èƒ½åŠ›
    - å¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªè½¬æ¢

    **æˆæœ¬æ•ˆç›Š**ï¼š

    - å‡å°‘ä¸“ç”¨ETLåŸºç¡€è®¾æ–½çš„éœ€æ±‚
    - æŒ‰éœ€ä»˜è´¹çš„äº‘è®¡ç®—æ¨¡å‹

    #### æ··åˆæ–¹æ³•

    åœ¨å®è·µä¸­ï¼Œè®¸å¤šMLç³»ç»Ÿä½¿ç”¨æ··åˆæ–¹æ³•ï¼š

    ```python
    # æ··åˆETL/ELTæµç¨‹ç¤ºä¾‹

    # 1. æå–åŸå§‹æ•°æ®
    raw_data = extract_from_sources()

    # 2. åŸºæœ¬æ¸…ç†å’ŒéªŒè¯ï¼ˆETLé£æ ¼ï¼‰
    cleaned_data = basic_cleaning(raw_data)

    # 3. åŠ è½½åˆ°æ•°æ®æ¹–ï¼ˆELTé£æ ¼ï¼‰
    load_to_data_lake(cleaned_data)

    # 4. åœ¨æ•°æ®ä»“åº“ä¸­è¿›è¡Œå¤æ‚è½¬æ¢ï¼ˆELTé£æ ¼ï¼‰
    transformed_data = complex_transformations_in_warehouse()

    # 5. ä¸ºMLå‡†å¤‡æœ€ç»ˆæ•°æ®é›†
    ml_ready_data = prepare_for_ml(transformed_data)
    ```

    ### MLç‰¹å®šçš„ETLè€ƒè™‘

    #### æ•°æ®æ¼‚ç§»æ£€æµ‹

    ```python
    def detect_data_drift(reference_data, current_data):
        \"\"\"æ£€æµ‹æ•°æ®åˆ†å¸ƒçš„å˜åŒ–\"\"\"
        from scipy import stats

        drift_scores = {}
        for column in reference_data.columns:
            if reference_data[column].dtype in ['int64', 'float64']:
                # ä½¿ç”¨KSæ£€éªŒæ£€æµ‹æ•°å€¼ç‰¹å¾çš„æ¼‚ç§»
                statistic, p_value = stats.ks_2samp(
                    reference_data[column],
                    current_data[column]
                )
                drift_scores[column] = {'statistic': statistic, 'p_value': p_value}

        return drift_scores
    ```

    #### ç‰¹å¾å­˜å‚¨é›†æˆ

    ```python
    def update_feature_store(features, feature_store_client):
        \"\"\"å°†å¤„ç†åçš„ç‰¹å¾æ›´æ–°åˆ°ç‰¹å¾å­˜å‚¨\"\"\"

        # éªŒè¯ç‰¹å¾æ¨¡å¼
        validate_feature_schema(features)

        # è®¡ç®—ç‰¹å¾ç»Ÿè®¡
        feature_stats = compute_feature_statistics(features)

        # æ›´æ–°ç‰¹å¾å­˜å‚¨
        feature_store_client.write_features(
            features=features,
            metadata=feature_stats,
            timestamp=datetime.now()
        )
    ```

    #### æ•°æ®è¡€ç¼˜è·Ÿè¸ª

    ```python
    def track_data_lineage(source_data, transformed_data, transformation_config):
        \"\"\"è·Ÿè¸ªæ•°æ®è½¬æ¢çš„è¡€ç¼˜\"\"\"

        lineage_info = {
            'source_hash': hash_dataframe(source_data),
            'target_hash': hash_dataframe(transformed_data),
            'transformation_config': transformation_config,
            'timestamp': datetime.now(),
            'row_count_change': len(transformed_data) - len(source_data),
            'column_changes': list(set(transformed_data.columns) - set(source_data.columns))
        }

        # è®°å½•åˆ°è¡€ç¼˜è·Ÿè¸ªç³»ç»Ÿ
        log_lineage(lineage_info)
    ```

    ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…çš„ä¾‹å­æ¥çœ‹çœ‹è¿™äº›æ¦‚å¿µæ˜¯å¦‚ä½•åœ¨å®è·µä¸­åº”ç”¨çš„ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## å®é™…å®ç°ï¼šå®¢æˆ·æµå¤±é¢„æµ‹ç®¡é“

    è®©æˆ‘ä»¬é€šè¿‡æ„å»ºä¸€ä¸ªå®Œæ•´çš„å®¢æˆ·æµå¤±é¢„æµ‹æ•°æ®ç®¡é“æ¥å°†ç†è®ºä»˜è¯¸å®è·µã€‚è¿™ä¸ªä¾‹å­å°†å±•ç¤ºETL/ELTçš„æ··åˆæ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®ç”Ÿæˆã€éªŒè¯ã€è½¬æ¢å’Œä¸ºMLå‡†å¤‡ã€‚

    ### é¡¹ç›®æ¦‚è¿°

    æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç®¡é“æ¥ï¼š

    1. **ç”Ÿæˆåˆæˆå®¢æˆ·æ•°æ®**ï¼ˆæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æ•°æ®æºï¼‰
    2. **å®æ–½æ•°æ®éªŒè¯**ï¼ˆç¡®ä¿æ•°æ®è´¨é‡ï¼‰
    3. **æ‰§è¡Œç‰¹å¾å·¥ç¨‹**ï¼ˆä¸ºMLå‡†å¤‡æ•°æ®ï¼‰
    4. **å­˜å‚¨å¤šç§æ ¼å¼**ï¼ˆå±•ç¤ºä¸åŒçš„å­˜å‚¨ç­–ç•¥ï¼‰
    5. **åˆ›å»ºè®­ç»ƒå°±ç»ªçš„æ•°æ®é›†**ï¼ˆæœ€ç»ˆçš„MLè¾“å‡ºï¼‰

    ### ç¬¬ä¸€æ­¥ï¼šæ•°æ®ç”Ÿæˆå’Œæå–

    é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåˆæˆæ•°æ®é›†æ¥æ¨¡æ‹Ÿå®¢æˆ·æ•°æ®ï¼š

    ```python
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    import random
    from typing import Dict, List, Tuple
    import warnings
    warnings.filterwarnings('ignore')

    def generate_customer_data(n_customers: int = 10000) -> pd.DataFrame:
        \"\"\"
        ç”Ÿæˆåˆæˆå®¢æˆ·æ•°æ®ç”¨äºæµå¤±é¢„æµ‹

        å‚æ•°:
            n_customers: è¦ç”Ÿæˆçš„å®¢æˆ·æ•°é‡

        è¿”å›:
            åŒ…å«å®¢æˆ·æ•°æ®çš„DataFrame
        \"\"\"
        np.random.seed(42)
        random.seed(42)

        # åŸºç¡€å®¢æˆ·ä¿¡æ¯
        customer_ids = [f"CUST_{i:06d}" for i in range(1, n_customers + 1)]

        # äººå£ç»Ÿè®¡æ•°æ®
        ages = np.random.normal(45, 15, n_customers).astype(int)
        ages = np.clip(ages, 18, 80)  # é™åˆ¶å¹´é¾„èŒƒå›´

        genders = np.random.choice(['M', 'F', 'Other'], n_customers, p=[0.48, 0.48, 0.04])

        # åœ°ç†æ•°æ®
        states = ['CA', 'NY', 'TX', 'FL', 'IL', 'PA', 'OH', 'GA', 'NC', 'MI']
        customer_states = np.random.choice(states, n_customers)

        # è´¦æˆ·ä¿¡æ¯
        account_lengths = np.random.exponential(24, n_customers)  # æœˆæ•°
        account_lengths = np.clip(account_lengths, 1, 120).astype(int)

        # æœåŠ¡ä½¿ç”¨æ•°æ®
        monthly_charges = np.random.normal(65, 20, n_customers)
        monthly_charges = np.clip(monthly_charges, 20, 150).round(2)

        total_charges = monthly_charges * account_lengths + np.random.normal(0, 50, n_customers)
        total_charges = np.clip(total_charges, 0, None).round(2)

        # æœåŠ¡ç‰¹å¾
        internet_service = np.random.choice(['DSL', 'Fiber', 'No'], n_customers, p=[0.4, 0.5, 0.1])
        online_security = np.random.choice(['Yes', 'No'], n_customers, p=[0.3, 0.7])
        tech_support = np.random.choice(['Yes', 'No'], n_customers, p=[0.25, 0.75])

        # åˆåŒä¿¡æ¯
        contract_types = np.random.choice(['Month-to-month', 'One year', 'Two year'],
                                        n_customers, p=[0.6, 0.25, 0.15])

        payment_methods = np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'],
                                         n_customers, p=[0.4, 0.2, 0.2, 0.2])

        # ç”Ÿæˆæµå¤±æ ‡ç­¾ï¼ˆåŸºäºä¸€äº›é€»è¾‘è§„åˆ™ï¼‰
        churn_probability = 0.1  # åŸºç¡€æµå¤±ç‡

        # å½±å“æµå¤±çš„å› ç´ 
        churn_prob_adjusted = churn_probability + np.where(
            (contract_types == 'Month-to-month') &
            (monthly_charges > 80) &
            (account_lengths < 12), 0.4, 0
        )

        churn_prob_adjusted += np.where(
            (internet_service == 'Fiber') &
            (online_security == 'No'), 0.2, 0
        )

        churn_prob_adjusted += np.where(ages > 65, 0.15, 0)
        churn_prob_adjusted = np.clip(churn_prob_adjusted, 0, 0.8)

        churn_labels = np.random.binomial(1, churn_prob_adjusted, n_customers)

        # åˆ›å»ºDataFrame
        data = {
            'customer_id': customer_ids,
            'age': ages,
            'gender': genders,
            'state': customer_states,
            'account_length_months': account_lengths,
            'monthly_charges': monthly_charges,
            'total_charges': total_charges,
            'internet_service': internet_service,
            'online_security': online_security,
            'tech_support': tech_support,
            'contract': contract_types,
            'payment_method': payment_methods,
            'label_churn': churn_labels
        }

        df = pd.DataFrame(data)

        # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æ•°æ®
        missing_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)
        df.loc[missing_indices, 'total_charges'] = np.nan

        # æ·»åŠ æ—¶é—´æˆ³
        df['created_at'] = datetime.now()
        df['updated_at'] = df['created_at'] + pd.to_timedelta(
            np.random.randint(0, 30, n_customers), unit='D'
        )

        return df

    # ç”Ÿæˆæ•°æ®
    print("ç”Ÿæˆå®¢æˆ·æ•°æ®...")
    customer_data = generate_customer_data(10000)
    print(f"ç”Ÿæˆäº† {len(customer_data)} æ¡å®¢æˆ·è®°å½•")
    print(f"æµå¤±ç‡: {customer_data['label_churn'].mean():.2%}")
    print("\\næ•°æ®æ ·æœ¬:")
    print(customer_data.head())
    ```

    ### ç¬¬äºŒæ­¥ï¼šæ•°æ®éªŒè¯

    åœ¨å¤„ç†æ•°æ®ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦éªŒè¯å…¶è´¨é‡å’Œå®Œæ•´æ€§ï¼š

    ```python
    def validate_customer_data(df: pd.DataFrame) -> Dict[str, any]:
        \"\"\"
        éªŒè¯å®¢æˆ·æ•°æ®çš„è´¨é‡å’Œå®Œæ•´æ€§

        å‚æ•°:
            df: è¦éªŒè¯çš„DataFrame

        è¿”å›:
            åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸
        \"\"\"
        validation_results = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'statistics': {}
        }

        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = [
            'customer_id', 'age', 'gender', 'monthly_charges',
            'total_charges', 'label_churn'
        ]

        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            validation_results['errors'].append(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
            validation_results['is_valid'] = False

        # æ£€æŸ¥æ•°æ®ç±»å‹
        if 'age' in df.columns and not pd.api.types.is_numeric_dtype(df['age']):
            validation_results['errors'].append("å¹´é¾„åˆ—å¿…é¡»æ˜¯æ•°å€¼ç±»å‹")
            validation_results['is_valid'] = False

        # æ£€æŸ¥æ•°æ®èŒƒå›´
        if 'age' in df.columns:
            invalid_ages = df[(df['age'] < 18) | (df['age'] > 120)]
            if len(invalid_ages) > 0:
                validation_results['warnings'].append(f"å‘ç° {len(invalid_ages)} ä¸ªå¼‚å¸¸å¹´é¾„å€¼")

        # æ£€æŸ¥é‡å¤çš„å®¢æˆ·ID
        if 'customer_id' in df.columns:
            duplicates = df['customer_id'].duplicated().sum()
            if duplicates > 0:
                validation_results['errors'].append(f"å‘ç° {duplicates} ä¸ªé‡å¤çš„å®¢æˆ·ID")
                validation_results['is_valid'] = False

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_stats = df.isnull().sum()
        validation_results['statistics']['missing_values'] = missing_stats.to_dict()

        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        if 'label_churn' in df.columns:
            churn_rate = df['label_churn'].mean()
            validation_results['statistics']['churn_rate'] = churn_rate

            if churn_rate > 0.5:
                validation_results['warnings'].append(f"æµå¤±ç‡å¼‚å¸¸é«˜: {churn_rate:.2%}")
            elif churn_rate < 0.01:
                validation_results['warnings'].append(f"æµå¤±ç‡å¼‚å¸¸ä½: {churn_rate:.2%}")

        # æ•°æ®è´¨é‡è¯„åˆ†
        error_count = len(validation_results['errors'])
        warning_count = len(validation_results['warnings'])

        if error_count == 0 and warning_count == 0:
            quality_score = 1.0
        elif error_count == 0:
            quality_score = max(0.7, 1.0 - warning_count * 0.1)
        else:
            quality_score = max(0.0, 0.5 - error_count * 0.1)

        validation_results['statistics']['quality_score'] = quality_score

        return validation_results

    # éªŒè¯æ•°æ®
    print("éªŒè¯æ•°æ®è´¨é‡...")
    validation_results = validate_customer_data(customer_data)

    print(f"æ•°æ®éªŒè¯ç»“æœ:")
    print(f"- æœ‰æ•ˆæ€§: {'âœ“' if validation_results['is_valid'] else 'âœ—'}")
    print(f"- è´¨é‡è¯„åˆ†: {validation_results['statistics']['quality_score']:.2f}")
    print(f"- é”™è¯¯æ•°é‡: {len(validation_results['errors'])}")
    print(f"- è­¦å‘Šæ•°é‡: {len(validation_results['warnings'])}")

    if validation_results['errors']:
        print("\\né”™è¯¯:")
        for error in validation_results['errors']:
            print(f"  - {error}")

    if validation_results['warnings']:
        print("\\nè­¦å‘Š:")
        for warning in validation_results['warnings']:
            print(f"  - {warning}")
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### ç¬¬ä¸‰æ­¥ï¼šæ•°æ®å­˜å‚¨ï¼ˆåŠ è½½é˜¶æ®µï¼‰

    ç°åœ¨è®©æˆ‘ä»¬å°†éªŒè¯åçš„æ•°æ®å­˜å‚¨ä¸ºå¤šç§æ ¼å¼ï¼Œå±•ç¤ºä¸åŒçš„å­˜å‚¨ç­–ç•¥ï¼š

    ```python
    import os
    import json
    from pathlib import Path

    def save_data_multiple_formats(df: pd.DataFrame, base_path: str = "data"):
        \"\"\"
        å°†æ•°æ®ä¿å­˜ä¸ºå¤šç§æ ¼å¼

        å‚æ•°:
            df: è¦ä¿å­˜çš„DataFrame
            base_path: åŸºç¡€å­˜å‚¨è·¯å¾„
        \"\"\"
        # åˆ›å»ºç›®å½•ç»“æ„
        Path(base_path).mkdir(exist_ok=True)
        Path(f"{base_path}/raw").mkdir(exist_ok=True)
        Path(f"{base_path}/processed").mkdir(exist_ok=True)

        # ä¿å­˜ä¸ºCSVï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œè¡Œä¸»åºï¼‰
        csv_path = f"{base_path}/raw/customer_data.csv"
        df.to_csv(csv_path, index=False)
        csv_size = os.path.getsize(csv_path) / (1024 * 1024)  # MB

        # ä¿å­˜ä¸ºParquetï¼ˆäºŒè¿›åˆ¶æ ¼å¼ï¼Œåˆ—ä¸»åºï¼‰
        parquet_path = f"{base_path}/raw/customer_data.parquet"
        df.to_parquet(parquet_path, index=False)
        parquet_size = os.path.getsize(parquet_path) / (1024 * 1024)  # MB

        # ä¿å­˜ä¸ºJSONï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œçµæ´»ç»“æ„ï¼‰
        json_path = f"{base_path}/raw/customer_data.json"
        df.to_json(json_path, orient='records', date_format='iso')
        json_size = os.path.getsize(json_path) / (1024 * 1024)  # MB

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'created_at': datetime.now().isoformat(),
            'record_count': len(df),
            'column_count': len(df.columns),
            'file_sizes_mb': {
                'csv': round(csv_size, 2),
                'parquet': round(parquet_size, 2),
                'json': round(json_size, 2)
            },
            'columns': list(df.columns),
            'dtypes': df.dtypes.astype(str).to_dict()
        }

        with open(f"{base_path}/raw/metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"æ•°æ®å·²ä¿å­˜åˆ° {base_path}/raw/")
        print(f"æ–‡ä»¶å¤§å°å¯¹æ¯”:")
        print(f"  - CSV: {csv_size:.2f} MB")
        print(f"  - Parquet: {parquet_size:.2f} MB ({parquet_size/csv_size:.1%} of CSV)")
        print(f"  - JSON: {json_size:.2f} MB ({json_size/csv_size:.1%} of CSV)")

        return metadata

    # ä¿å­˜æ•°æ®
    print("ä¿å­˜æ•°æ®ä¸ºå¤šç§æ ¼å¼...")
    metadata = save_data_multiple_formats(customer_data)
    ```

    ### ç¬¬å››æ­¥ï¼šç‰¹å¾å·¥ç¨‹å’Œè½¬æ¢

    ç°åœ¨è®©æˆ‘ä»¬å®æ–½ç‰¹å¾å·¥ç¨‹ç®¡é“ï¼š

    ```python
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.impute import SimpleImputer
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline

    def create_feature_engineering_pipeline(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        \"\"\"
        åˆ›å»ºç‰¹å¾å·¥ç¨‹ç®¡é“å¹¶å¤„ç†æ•°æ®

        å‚æ•°:
            df: è¾“å…¥DataFrame

        è¿”å›:
            è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†çš„å…ƒç»„
        \"\"\"
        # åˆ›å»ºæ•°æ®å‰¯æœ¬
        df_processed = df.copy()

        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        feature_columns = [col for col in df.columns if col not in ['label_churn', 'customer_id', 'created_at', 'updated_at']]
        X = df_processed[feature_columns]
        y = df_processed['label_churn']

        # åˆ†å‰²æ•°æ®ï¼ˆåœ¨é¢„å¤„ç†ä¹‹å‰åˆ†å‰²ä»¥é¿å…æ•°æ®æ³„æ¼ï¼‰
        X_train, X_valid, y_train, y_valid = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # è¯†åˆ«æ•°å€¼å’Œåˆ†ç±»åˆ—
        numeric_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()
        categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()

        print(f"æ•°å€¼åˆ— ({len(numeric_columns)}): {numeric_columns}")
        print(f"åˆ†ç±»åˆ— ({len(categorical_columns)}): {categorical_columns}")

        # åˆ›å»ºé¢„å¤„ç†ç®¡é“
        numeric_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ])

        categorical_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
        ])

        # ç»„åˆé¢„å¤„ç†å™¨
        preprocessor = ColumnTransformer([
            ('num', numeric_pipeline, numeric_columns),
            ('cat', categorical_pipeline, categorical_columns)
        ], remainder='drop')

        # æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®
        X_train_processed = preprocessor.fit_transform(X_train)
        X_valid_processed = preprocessor.transform(X_valid)

        # è·å–ç‰¹å¾åç§°
        numeric_feature_names = numeric_columns
        categorical_feature_names = []

        if categorical_columns:
            # è·å–ç‹¬çƒ­ç¼–ç åçš„ç‰¹å¾åç§°
            cat_encoder = preprocessor.named_transformers_['cat']['onehot']
            categorical_feature_names = cat_encoder.get_feature_names_out(categorical_columns).tolist()

        all_feature_names = numeric_feature_names + categorical_feature_names

        # è½¬æ¢ä¸ºDataFrame
        X_train_df = pd.DataFrame(
            X_train_processed,
            columns=all_feature_names,
            index=X_train.index.copy()
        ).reset_index(drop=True)

        X_valid_df = pd.DataFrame(
            X_valid_processed,
            columns=all_feature_names,
            index=X_valid.index.copy()
        ).reset_index(drop=True)

        # æ·»åŠ ç›®æ ‡å˜é‡
        train_final = pd.concat([
            X_train_df,
            y_train.reset_index(drop=True)
        ], axis=1)

        valid_final = pd.concat([
            X_valid_df,
            y_valid.reset_index(drop=True)
        ], axis=1)

        print(f"\\nç‰¹å¾å·¥ç¨‹å®Œæˆ:")
        print(f"  - è®­ç»ƒé›†: {train_final.shape}")
        print(f"  - éªŒè¯é›†: {valid_final.shape}")
        print(f"  - æ€»ç‰¹å¾æ•°: {len(all_feature_names)}")

        return train_final, valid_final, preprocessor

    # æ‰§è¡Œç‰¹å¾å·¥ç¨‹
    print("æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
    train_data, valid_data, feature_pipeline = create_feature_engineering_pipeline(customer_data)

    print("\\nè®­ç»ƒæ•°æ®æ ·æœ¬:")
    print(train_data.head())
    ```

    ### ç¬¬äº”æ­¥ï¼šæœ€ç»ˆæ•°æ®åŠ è½½å’Œç®¡é“å®Œæˆ

    ```python
    def save_processed_data(train_df: pd.DataFrame, valid_df: pd.DataFrame,
                          pipeline, base_path: str = "data"):
        \"\"\"
        ä¿å­˜å¤„ç†åçš„æ•°æ®å’Œç®¡é“

        å‚æ•°:
            train_df: è®­ç»ƒæ•°æ®
            valid_df: éªŒè¯æ•°æ®
            pipeline: ç‰¹å¾å·¥ç¨‹ç®¡é“
            base_path: åŸºç¡€è·¯å¾„
        \"\"\"
        import joblib

        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        train_df.to_csv(f"{base_path}/processed/train_data.csv", index=False)
        valid_df.to_csv(f"{base_path}/processed/valid_data.csv", index=False)

        # ä¿å­˜ä¸ºParquetæ ¼å¼ï¼ˆæ›´é«˜æ•ˆï¼‰
        train_df.to_parquet(f"{base_path}/processed/train_data.parquet", index=False)
        valid_df.to_parquet(f"{base_path}/processed/valid_data.parquet", index=False)

        # ä¿å­˜é¢„å¤„ç†ç®¡é“
        joblib.dump(pipeline, f"{base_path}/processed/feature_pipeline.joblib")

        # åˆ›å»ºæ•°æ®æ‘˜è¦
        summary = {
            'pipeline_created_at': datetime.now().isoformat(),
            'train_samples': len(train_df),
            'valid_samples': len(valid_df),
            'feature_count': len(train_df.columns) - 1,  # å‡å»ç›®æ ‡å˜é‡
            'target_distribution': {
                'train_churn_rate': train_df['label_churn'].mean(),
                'valid_churn_rate': valid_df['label_churn'].mean()
            },
            'feature_names': [col for col in train_df.columns if col != 'label_churn']
        }

        with open(f"{base_path}/processed/data_summary.json", 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ° {base_path}/processed/")
        print(f"æ•°æ®æ‘˜è¦:")
        print(f"  - è®­ç»ƒæ ·æœ¬: {summary['train_samples']:,}")
        print(f"  - éªŒè¯æ ·æœ¬: {summary['valid_samples']:,}")
        print(f"  - ç‰¹å¾æ•°é‡: {summary['feature_count']}")
        print(f"  - è®­ç»ƒé›†æµå¤±ç‡: {summary['target_distribution']['train_churn_rate']:.2%}")
        print(f"  - éªŒè¯é›†æµå¤±ç‡: {summary['target_distribution']['valid_churn_rate']:.2%}")

    # ä¿å­˜æœ€ç»ˆå¤„ç†çš„æ•°æ®
    print("ä¿å­˜å¤„ç†åçš„æ•°æ®...")
    save_processed_data(train_data, valid_data, feature_pipeline)
    ```

    ### ç®¡é“æ€§èƒ½åˆ†æ

    è®©æˆ‘ä»¬åˆ†ææˆ‘ä»¬æ„å»ºçš„ç®¡é“çš„æ€§èƒ½ç‰¹å¾ï¼š

    ```python
    def analyze_pipeline_performance():
        \"\"\"åˆ†ææ•°æ®ç®¡é“çš„æ€§èƒ½ç‰¹å¾\"\"\"

        print("=== æ•°æ®ç®¡é“æ€§èƒ½åˆ†æ ===\\n")

        # 1. å­˜å‚¨æ•ˆç‡åˆ†æ
        print("1. å­˜å‚¨æ ¼å¼æ•ˆç‡å¯¹æ¯”:")
        csv_size = os.path.getsize("data/raw/customer_data.csv") / 1024
        parquet_size = os.path.getsize("data/raw/customer_data.parquet") / 1024
        json_size = os.path.getsize("data/raw/customer_data.json") / 1024

        print(f"   - CSV: {csv_size:.1f} KB")
        print(f"   - Parquet: {parquet_size:.1f} KB (èŠ‚çœ {(1-parquet_size/csv_size)*100:.1f}%)")
        print(f"   - JSON: {json_size:.1f} KB (å¢åŠ  {(json_size/csv_size-1)*100:.1f}%)")

        # 2. æ•°æ®è´¨é‡æŒ‡æ ‡
        print("\\n2. æ•°æ®è´¨é‡æŒ‡æ ‡:")
        print(f"   - åŸå§‹æ•°æ®è®°å½•æ•°: {len(customer_data):,}")
        print(f"   - å¤„ç†åè®­ç»ƒè®°å½•æ•°: {len(train_data):,}")
        print(f"   - å¤„ç†åéªŒè¯è®°å½•æ•°: {len(valid_data):,}")
        print(f"   - æ•°æ®ä¿ç•™ç‡: {(len(train_data) + len(valid_data))/len(customer_data)*100:.1f}%")

        # 3. ç‰¹å¾å·¥ç¨‹æ•ˆæœ
        original_features = len([col for col in customer_data.columns
                               if col not in ['label_churn', 'customer_id', 'created_at', 'updated_at']])
        processed_features = len(train_data.columns) - 1  # å‡å»ç›®æ ‡å˜é‡

        print("\\n3. ç‰¹å¾å·¥ç¨‹æ•ˆæœ:")
        print(f"   - åŸå§‹ç‰¹å¾æ•°: {original_features}")
        print(f"   - å¤„ç†åç‰¹å¾æ•°: {processed_features}")
        print(f"   - ç‰¹å¾æ‰©å±•å€æ•°: {processed_features/original_features:.1f}x")

        # 4. ç±»åˆ«å¹³è¡¡
        print("\\n4. ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        train_churn_rate = train_data['label_churn'].mean()
        valid_churn_rate = valid_data['label_churn'].mean()
        print(f"   - è®­ç»ƒé›†æµå¤±ç‡: {train_churn_rate:.2%}")
        print(f"   - éªŒè¯é›†æµå¤±ç‡: {valid_churn_rate:.2%}")
        print(f"   - åˆ†å¸ƒå·®å¼‚: {abs(train_churn_rate - valid_churn_rate)*100:.2f} ç™¾åˆ†ç‚¹")

    # æ‰§è¡Œæ€§èƒ½åˆ†æ
    analyze_pipeline_performance()
    ```

    è¿™ä¸ªå®ç°å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„æ··åˆETL/ELTç®¡é“ï¼ŒåŒ…æ‹¬ï¼š

    #### âœ… **ETLç»„ä»¶**
    - **æå–**ï¼šä»åˆæˆæ•°æ®ç”Ÿæˆå™¨è·å–æ•°æ®
    - **è½¬æ¢**ï¼šæ•°æ®éªŒè¯ã€æ¸…ç†å’Œç‰¹å¾å·¥ç¨‹
    - **åŠ è½½**ï¼šå­˜å‚¨ä¸ºå¤šç§æ ¼å¼ä¾›ä¸‹æ¸¸ä½¿ç”¨

    #### âœ… **ELTç»„ä»¶**
    - **æå–**ï¼šåŸå§‹æ•°æ®ç›´æ¥åŠ è½½åˆ°å­˜å‚¨
    - **åŠ è½½**ï¼šåŸå§‹æ•°æ®å­˜å‚¨åœ¨æ•°æ®æ¹–ä¸­
    - **è½¬æ¢**ï¼šåœ¨å­˜å‚¨åè¿›è¡Œå¤æ‚çš„ç‰¹å¾å·¥ç¨‹

    #### âœ… **ç”Ÿäº§å°±ç»ªç‰¹æ€§**
    - æ•°æ®éªŒè¯å’Œè´¨é‡æ£€æŸ¥
    - å¤šæ ¼å¼å­˜å‚¨ï¼ˆCSVã€Parquetã€JSONï¼‰
    - ç®¡é“åºåˆ—åŒ–å’Œç‰ˆæœ¬æ§åˆ¶
    - æ€§èƒ½ç›‘æ§å’Œåˆ†æ
    - é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

    è¿™ç§æ–¹æ³•åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­æä¾›äº†çµæ´»æ€§ã€å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## å·¥å…·å’Œè§„æ¨¡çš„å¿«é€Ÿè¯´æ˜

    ä½ ä½¿ç”¨çš„å®é™…å·¥å…·æ ˆå°†æ ¹æ®æ•°æ®è§„æ¨¡å’Œç”Ÿäº§è¦æ±‚è€Œæœ‰æ‰€ä¸åŒã€‚åœ¨æ›´å¤§è§„æ¨¡ä¸‹ï¼Œä¾‹å¦‚ï¼Œä½ å¯èƒ½ä¼šä½¿ç”¨PySparkè¿›è¡Œåˆ†å¸ƒå¼æ•°æ®å¤„ç†ï¼Œä½¿ç”¨Airflowæˆ–Prefectè¿›è¡Œå·¥ä½œæµç¼–æ’ï¼Œä»¥åŠå…¶ä»–é«˜çº§æ¡†æ¶æ¥ç¡®ä¿å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚

    ### è§„æ¨¡åŒ–å·¥å…·æ ˆå¯¹æ¯”

    #### ğŸ  **å°è§„æ¨¡/åŸå‹é˜¶æ®µ**
    ```python
    # æˆ‘ä»¬æ¼”ç¤ºä¸­ä½¿ç”¨çš„å·¥å…·æ ˆ
    æŠ€æœ¯æ ˆï¼š
    - æ•°æ®å¤„ç†ï¼šPandas + NumPy
    - ç‰¹å¾å·¥ç¨‹ï¼šscikit-learn
    - å­˜å‚¨ï¼šæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
    - ç¼–æ’ï¼šPythonè„šæœ¬

    é€‚ç”¨åœºæ™¯ï¼š
    - æ•°æ®é‡ < 1GB
    - å•æœºå¤„ç†
    - å¿«é€ŸåŸå‹å¼€å‘
    - æ¦‚å¿µéªŒè¯
    ```

    #### ğŸ¢ **ä¸­ç­‰è§„æ¨¡/ç”Ÿäº§ç¯å¢ƒ**
    ```python
    # ä¸­ç­‰è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ
    æŠ€æœ¯æ ˆï¼š
    - æ•°æ®å¤„ç†ï¼šDask / Polars
    - ç‰¹å¾å·¥ç¨‹ï¼šscikit-learn + è‡ªå®šä¹‰ç®¡é“
    - å­˜å‚¨ï¼šäº‘å­˜å‚¨ (S3, GCS, Azure Blob)
    - ç¼–æ’ï¼šAirflow / Prefect
    - ç›‘æ§ï¼šPrometheus + Grafana

    é€‚ç”¨åœºæ™¯ï¼š
    - æ•°æ®é‡ 1GB - 100GB
    - å¤šæ ¸/å¤šèŠ‚ç‚¹å¤„ç†
    - å®šæœŸæ‰¹å¤„ç†ä½œä¸š
    - ä¸­ç­‰å¤æ‚åº¦çš„ç‰¹å¾å·¥ç¨‹
    ```

    #### ğŸ­ **å¤§è§„æ¨¡/ä¼ä¸šçº§**
    ```python
    # å¤§è§„æ¨¡ä¼ä¸šç¯å¢ƒ
    æŠ€æœ¯æ ˆï¼š
    - æ•°æ®å¤„ç†ï¼šApache Spark (PySpark)
    - æµå¤„ç†ï¼šApache Kafka + Spark Streaming
    - ç‰¹å¾å­˜å‚¨ï¼šFeast / Tecton / AWS SageMaker Feature Store
    - ç¼–æ’ï¼šApache Airflow / Kubeflow Pipelines
    - å­˜å‚¨ï¼šæ•°æ®æ¹– (Delta Lake, Apache Iceberg)
    - ç›‘æ§ï¼šDataDog / New Relic + è‡ªå®šä¹‰ä»ªè¡¨æ¿

    é€‚ç”¨åœºæ™¯ï¼š
    - æ•°æ®é‡ > 100GB
    - åˆ†å¸ƒå¼é›†ç¾¤å¤„ç†
    - å®æ—¶å’Œæ‰¹å¤„ç†æ··åˆ
    - å¤æ‚çš„ç‰¹å¾å·¥ç¨‹å’ŒMLç®¡é“
    ```

    ### å·¥å…·é€‰æ‹©å†³ç­–çŸ©é˜µ

    | è€ƒè™‘å› ç´  | å°è§„æ¨¡ | ä¸­ç­‰è§„æ¨¡ | å¤§è§„æ¨¡ |
    |---------|--------|----------|--------|
    | **æ•°æ®é‡** | < 1GB | 1-100GB | > 100GB |
    | **å¤„ç†é¢‘ç‡** | æŒ‰éœ€ | æ¯æ—¥/æ¯å°æ—¶ | å®æ—¶ + æ‰¹å¤„ç† |
    | **å›¢é˜Ÿè§„æ¨¡** | 1-3äºº | 3-10äºº | 10+äºº |
    | **é¢„ç®—** | ä½ | ä¸­ç­‰ | é«˜ |
    | **å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ | å¤æ‚ |
    | **å¯é æ€§è¦æ±‚** | åŸºç¡€ | é«˜ | å…³é”®ä»»åŠ¡ |

    ### è¿ç§»è·¯å¾„

    ```python
    # å…¸å‹çš„æŠ€æœ¯æ ˆæ¼”è¿›è·¯å¾„

    é˜¶æ®µ1ï¼šåŸå‹å¼€å‘
    pandas â†’ éªŒè¯æ¦‚å¿µ â†’ å°è§„æ¨¡æµ‹è¯•

    é˜¶æ®µ2ï¼šæ‰©å±•
    pandas â†’ Dask/Polars â†’ ä¸­ç­‰è§„æ¨¡ç”Ÿäº§

    é˜¶æ®µ3ï¼šä¼ä¸šçº§
    Dask/Polars â†’ PySpark â†’ å¤§è§„æ¨¡åˆ†å¸ƒå¼å¤„ç†

    # å…³é”®åŸåˆ™ï¼šæ¸è¿›å¼è¿ç§»
    - ä¿æŒæ ¸å¿ƒé€»è¾‘ä¸å˜
    - é€æ­¥æ›¿æ¢åº•å±‚æŠ€æœ¯
    - ç»´æŠ¤å‘åå…¼å®¹æ€§
    - æŒç»­æ€§èƒ½ç›‘æ§
    ```

    ç„¶è€Œï¼Œåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Pandasã€NumPyå’Œscikit-learnå®ç°äº†æˆ‘ä»¬çš„æ¼”ç¤ºï¼Œå› ä¸ºè¿™é‡Œçš„ä¸»è¦é‡ç‚¹æ˜¯é¦–å…ˆå»ºç«‹åŸºç¡€ã€‚

    **å…³é”®è¦ç‚¹æ˜¯ï¼Œè™½ç„¶æŠ€æœ¯æ ˆå¯èƒ½ä¼šæ”¹å˜ï¼Œä½†åº•å±‚åŸåˆ™ä¿æŒä¸å˜ã€‚** éšç€æˆ‘ä»¬åœ¨è¿™ä¸ªç³»åˆ—ä¸­çš„å‰è¿›ï¼Œæˆ‘ä»¬ä¹Ÿå°†æ¢ç´¢ä½¿ç”¨æ›´é«˜çº§å·¥å…·çš„æ¨¡æ‹Ÿï¼Œä½†ç°åœ¨ï¼Œé‡ç‚¹æ˜¯æŒæ¡æ ¸å¿ƒæ¦‚å¿µã€‚

    ### å®é™…ç”Ÿäº§è€ƒè™‘

    #### ğŸ”„ **æ•°æ®ç®¡é“ç›‘æ§**
    ```python
    # ç”Ÿäº§ç¯å¢ƒä¸­çš„ç®¡é“ç›‘æ§ç¤ºä¾‹
    import logging
    from datetime import datetime

    def monitor_pipeline_health(stage_name: str, input_data, output_data):
        \"\"\"ç›‘æ§ç®¡é“å„é˜¶æ®µçš„å¥åº·çŠ¶å†µ\"\"\"

        # æ•°æ®é‡ç›‘æ§
        input_count = len(input_data) if hasattr(input_data, '__len__') else 0
        output_count = len(output_data) if hasattr(output_data, '__len__') else 0

        # æ•°æ®è´¨é‡ç›‘æ§
        if hasattr(output_data, 'isnull'):
            null_percentage = output_data.isnull().sum().sum() / output_data.size * 100
        else:
            null_percentage = 0

        # è®°å½•æŒ‡æ ‡
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'stage': stage_name,
            'input_records': input_count,
            'output_records': output_count,
            'data_loss_percentage': (input_count - output_count) / input_count * 100 if input_count > 0 else 0,
            'null_percentage': null_percentage
        }

        # å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
        logging.info(f"Pipeline metrics: {metrics}")

        # è­¦æŠ¥æ¡ä»¶
        if metrics['data_loss_percentage'] > 10:
            logging.warning(f"High data loss in {stage_name}: {metrics['data_loss_percentage']:.1f}%")

        if metrics['null_percentage'] > 5:
            logging.warning(f"High null percentage in {stage_name}: {metrics['null_percentage']:.1f}%")
    ```

    #### ğŸš¨ **é”™è¯¯å¤„ç†å’Œæ¢å¤**
    ```python
    def robust_pipeline_stage(stage_func, input_data, max_retries=3):
        \"\"\"ä¸ºç®¡é“é˜¶æ®µæ·»åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶\"\"\"

        for attempt in range(max_retries):
            try:
                result = stage_func(input_data)
                logging.info(f"Stage completed successfully on attempt {attempt + 1}")
                return result

            except Exception as e:
                logging.error(f"Stage failed on attempt {attempt + 1}: {str(e)}")

                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•å¹¶é‡æ–°æŠ›å‡ºå¼‚å¸¸
                    logging.critical(f"Stage failed after {max_retries} attempts")
                    raise

                # ç­‰å¾…åé‡è¯•
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    ```

    #### ğŸ“Š **æ•°æ®è¡€ç¼˜è·Ÿè¸ª**
    ```python
    class DataLineageTracker:
        \"\"\"è·Ÿè¸ªæ•°æ®åœ¨ç®¡é“ä¸­çš„è¡€ç¼˜å…³ç³»\"\"\"

        def __init__(self):
            self.lineage_graph = {}

        def track_transformation(self, input_id: str, output_id: str,
                               transformation: str, metadata: dict = None):
            \"\"\"è®°å½•æ•°æ®è½¬æ¢\"\"\"

            if output_id not in self.lineage_graph:
                self.lineage_graph[output_id] = {
                    'inputs': [],
                    'transformations': [],
                    'metadata': []
                }

            self.lineage_graph[output_id]['inputs'].append(input_id)
            self.lineage_graph[output_id]['transformations'].append(transformation)
            self.lineage_graph[output_id]['metadata'].append(metadata or {})

        def get_lineage(self, data_id: str) -> dict:
            \"\"\"è·å–æ•°æ®çš„å®Œæ•´è¡€ç¼˜\"\"\"
            return self.lineage_graph.get(data_id, {})

        def visualize_lineage(self, data_id: str):
            \"\"\"å¯è§†åŒ–æ•°æ®è¡€ç¼˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\"\"\"
            lineage = self.get_lineage(data_id)
            print(f"æ•°æ®è¡€ç¼˜ for {data_id}:")
            for i, (input_id, transform) in enumerate(zip(
                lineage.get('inputs', []),
                lineage.get('transformations', [])
            )):
                print(f"  {i+1}. {input_id} --[{transform}]--> {data_id}")
    ```

    è¿™äº›ç”Ÿäº§çº§è€ƒè™‘ç¡®ä¿äº†æ•°æ®ç®¡é“çš„å¯é æ€§ã€å¯è§‚å¯Ÿæ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œè¿™å¯¹äºä¼ä¸šçº§MLç³»ç»Ÿè‡³å…³é‡è¦ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ç»“è®º

    åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†æœºå™¨å­¦ä¹ è¿è¥ä¸­æ•°æ®ç®¡é“çš„å…³é”®ä½œç”¨ï¼Œå±•ç¤ºäº†ç”Ÿäº§MLä¸­çš„æˆåŠŸæ›´å¤šåœ°å–å†³äºæ•°æ®çš„å¯é æ€§ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å¤æ‚æ€§ã€‚

    ### ğŸ¯ **å…³é”®å­¦ä¹ è¦ç‚¹**

    #### ğŸ“Š **æ•°æ®æ™¯è§‚ç†è§£**
    æˆ‘ä»¬æ¢ç´¢äº†å„ç§æ•°æ®æºå’Œæ ¼å¼åŠå…¶æ€§èƒ½æƒè¡¡ï¼Œç†è§£äº†è¿™äº›æ¶æ„é€‰æ‹©å¦‚ä½•å‘ä¸‹æ¸¸å½±å“MLæ•ˆç‡ï¼š

    - **æ•°æ®æºå¤šæ ·æ€§**ï¼šç”¨æˆ·è¾“å…¥ã€ç³»ç»Ÿæ—¥å¿—ã€å†…éƒ¨æ•°æ®åº“ã€ç¬¬ä¸‰æ–¹æ•°æ®
    - **æ ¼å¼é€‰æ‹©å½±å“**ï¼šæ–‡æœ¬vsäºŒè¿›åˆ¶ã€è¡Œä¸»åºvsåˆ—ä¸»åºçš„æ€§èƒ½å·®å¼‚
    - **å­˜å‚¨ç­–ç•¥**ï¼šä¸åŒæ ¼å¼çš„ç©ºé—´æ•ˆç‡å’Œè®¿é—®æ¨¡å¼ä¼˜åŒ–

    #### ğŸ”„ **ETL/ELTèŒƒå¼æŒæ¡**
    åœ¨è¿™ä¸ªåŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç ”ç©¶äº†ETLå’ŒELTèŒƒå¼ï¼Œè®¤è¯†åˆ°çœŸå®ä¸–ç•ŒMLå·¥ä½œæµä¸­ä½¿ç”¨çš„æƒè¡¡å’Œæ··åˆç­–ç•¥ï¼š

    - **ä¼ ç»ŸETL**ï¼šæå–â†’è½¬æ¢â†’åŠ è½½çš„çº¿æ€§æµç¨‹
    - **ç°ä»£ELT**ï¼šæå–â†’åŠ è½½â†’è½¬æ¢çš„çµæ´»æ–¹æ³•
    - **æ··åˆç­–ç•¥**ï¼šç»“åˆä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿

    #### ğŸ› ï¸ **å®é™…å®ç°ç»éªŒ**
    æœ€åï¼Œé€šè¿‡å®é™…æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªæ··åˆETL/ELTç®¡é“ï¼Œå®Œæ•´åŒ…å«ï¼š

    - **æ•°æ®ç”Ÿæˆ**ï¼šåˆæˆå®¢æˆ·æµå¤±æ•°æ®åˆ›å»º
    - **æ•°æ®éªŒè¯**ï¼šè´¨é‡æ£€æŸ¥å’Œå®Œæ•´æ€§éªŒè¯
    - **ç‰¹å¾å·¥ç¨‹**ï¼šä½¿ç”¨scikit-learnç®¡é“çš„è½¬æ¢
    - **å¤šæ ¼å¼å­˜å‚¨**ï¼šCSVã€Parquetã€JSONçš„å¯¹æ¯”
    - **scikit-learnç®¡é“å¤„ç†**ï¼šç”Ÿäº§å°±ç»ªçš„é¢„å¤„ç†æµç¨‹

    ### ğŸ’¡ **æ ¸å¿ƒæ´å¯Ÿ**

    #### ğŸ—ï¸ **æ¶æ„å†³ç­–çš„é‡è¦æ€§**
    æˆ‘ä»¬å­¦åˆ°äº†æ•°æ®æ ¼å¼å’Œå­˜å‚¨é€‰æ‹©ä¸æ˜¯æŠ€æœ¯ç»†èŠ‚ï¼Œè€Œæ˜¯å½±å“æ•´ä¸ªç³»ç»Ÿæ€§èƒ½çš„æ¶æ„å†³ç­–ï¼š

    - **Parquet vs CSV**ï¼šåœ¨æˆ‘ä»¬çš„æ¼”ç¤ºä¸­ï¼ŒParquetæ ¼å¼æ¯”CSVèŠ‚çœäº†çº¦50%çš„å­˜å‚¨ç©ºé—´
    - **åˆ—ä¸»åºä¼˜åŠ¿**ï¼špandas DataFrameçš„åˆ—è¿­ä»£æ¯”è¡Œè¿­ä»£å¿«20å€
    - **è®¿é—®æ¨¡å¼åŒ¹é…**ï¼šé€‰æ‹©ä¸ä½ çš„ä¸»è¦æ•°æ®è®¿é—®æ¨¡å¼åŒ¹é…çš„å­˜å‚¨æ ¼å¼

    #### ğŸ” **æ•°æ®è´¨é‡è‡³ä¸Š**
    æ•°æ®éªŒè¯å’Œè´¨é‡æ£€æŸ¥ä¸æ˜¯å¯é€‰çš„ï¼Œè€Œæ˜¯ç”Ÿäº§MLç³»ç»Ÿçš„å¿…éœ€ç»„ä»¶ï¼š

    - **æ—©æœŸéªŒè¯**ï¼šåœ¨ç®¡é“æ—©æœŸæ•è·æ•°æ®è´¨é‡é—®é¢˜
    - **æŒç»­ç›‘æ§**ï¼šè·Ÿè¸ªæ•°æ®æ¼‚ç§»å’Œåˆ†å¸ƒå˜åŒ–
    - **è‡ªåŠ¨åŒ–æ£€æŸ¥**ï¼šå®æ–½è‡ªåŠ¨åŒ–çš„æ•°æ®è´¨é‡é—¨æ§

    #### âš–ï¸ **çµæ´»æ€§ä¸æ•ˆç‡çš„å¹³è¡¡**
    æ··åˆETL/ELTæ–¹æ³•æä¾›äº†çµæ´»æ€§å’Œæ•ˆç‡çš„æœ€ä½³å¹³è¡¡ï¼š

    - **å³æ—¶å¯ç”¨æ€§**ï¼šåŸå§‹æ•°æ®ç«‹å³å¯ç”¨äºæ¢ç´¢
    - **å¤„ç†çµæ´»æ€§**ï¼šå¯ä»¥å¯¹ç›¸åŒæ•°æ®åº”ç”¨å¤šç§è½¬æ¢
    - **è®¡ç®—ä¼˜åŒ–**ï¼šåˆ©ç”¨ç°ä»£æ•°æ®ä»“åº“çš„å¤„ç†èƒ½åŠ›

    ### ğŸª **å…³é”®è¦ç‚¹**

    **åœ¨ä¼ä¸šMLOpsä¸­ï¼Œæ¨¡å‹æ˜¯å•†å“ï¼Œä½†æ•°æ®ç®¡é“æ˜¯èµ„äº§ã€‚** æŒæ¡æ•°æ®ç®¡é“çš„è®¾è®¡ã€éªŒè¯å’Œç¼–æ’æ˜¯ä½¿MLç³»ç»Ÿå¥å£®ã€å¯æ‰©å±•å’Œä¸šåŠ¡å…³é”®çš„å…³é”®ã€‚

    è¿™ä¸ªåŸåˆ™ä½“ç°åœ¨å‡ ä¸ªæ–¹é¢ï¼š

    #### ğŸ”„ **å¯é‡ç°æ€§**
    - ç‰ˆæœ¬åŒ–çš„æ•°æ®ç®¡é“ç¡®ä¿å®éªŒå¯é‡ç°
    - ç®¡é“åºåˆ—åŒ–å…è®¸åœ¨ä¸åŒç¯å¢ƒä¸­éƒ¨ç½²ç›¸åŒçš„å¤„ç†é€»è¾‘
    - æ•°æ®è¡€ç¼˜è·Ÿè¸ªæä¾›å®Œæ•´çš„å¯è¿½æº¯æ€§

    #### ğŸ“ˆ **å¯æ‰©å±•æ€§**
    - æ¨¡å—åŒ–ç®¡é“è®¾è®¡æ”¯æŒç»„ä»¶çº§æ‰©å±•
    - æ ¼å¼é€‰æ‹©å½±å“å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„èƒ½åŠ›
    - å·¥å…·æ ˆå¯ä»¥éšç€æ•°æ®é‡å¢é•¿è€Œæ¼”è¿›

    #### ğŸ›¡ï¸ **å¯é æ€§**
    - æ•°æ®éªŒè¯é˜²æ­¢ä¸‹æ¸¸æ¨¡å‹é”™è¯¯
    - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ç¡®ä¿ç®¡é“ç¨³å®šæ€§
    - ç›‘æ§å’Œè­¦æŠ¥æä¾›è¿è¥å¯è§æ€§

    ### ğŸš€ **æœªæ¥å±•æœ›**

    è¿™åªæ˜¯MLOps/LLMOpsç³»åˆ—ä¸­å…³äºæ•°æ®ç®¡é“å’Œå·¥ç¨‹çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œè¿˜æœ‰æ›´å¤šéƒ¨åˆ†å°†è·Ÿè¿›ã€‚

    åœ¨å³å°†åˆ°æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ï¼š

    #### ğŸ“Š **é«˜çº§æ•°æ®æ¦‚å¿µ**
    - **ç‰¹å¾å­˜å‚¨**ï¼šé›†ä¸­åŒ–ç‰¹å¾ç®¡ç†å’ŒæœåŠ¡
    - **æ•°æ®ç‰ˆæœ¬æ§åˆ¶**ï¼šä½¿ç”¨DVCå’Œå…¶ä»–å·¥å…·è¿›è¡Œæ•°æ®ç‰ˆæœ¬ç®¡ç†
    - **æ•°æ®æ¼‚ç§»æ£€æµ‹**ï¼šè‡ªåŠ¨åŒ–ç›‘æ§å’Œè­¦æŠ¥ç³»ç»Ÿ
    - **å®æ—¶ç‰¹å¾å·¥ç¨‹**ï¼šæµå¤„ç†å’Œåœ¨çº¿ç‰¹å¾è®¡ç®—

    #### ğŸ”§ **å…³é”®å·¥å…·å’Œè½¯ä»¶**
    - **Apache Airflow**ï¼šå·¥ä½œæµç¼–æ’å’Œè°ƒåº¦
    - **Apache Spark**ï¼šå¤§è§„æ¨¡åˆ†å¸ƒå¼æ•°æ®å¤„ç†
    - **Kafka + Spark Streaming**ï¼šå®æ—¶æ•°æ®å¤„ç†
    - **ç‰¹å¾å­˜å‚¨è§£å†³æ–¹æ¡ˆ**ï¼šFeastã€Tectonã€SageMaker Feature Store

    #### ğŸ—ï¸ **ç¼–æ’å’Œè‡ªåŠ¨åŒ–**
    - **ç®¡é“ç¼–æ’**ï¼šå¤æ‚å·¥ä½œæµçš„è®¾è®¡å’Œç®¡ç†
    - **ä¾èµ–ç®¡ç†**ï¼šä»»åŠ¡é—´ä¾èµ–å…³ç³»çš„å¤„ç†
    - **é”™è¯¯æ¢å¤**ï¼šå¤±è´¥å¤„ç†å’Œè‡ªåŠ¨é‡è¯•ç­–ç•¥
    - **èµ„æºç®¡ç†**ï¼šè®¡ç®—èµ„æºçš„åŠ¨æ€åˆ†é…å’Œä¼˜åŒ–

    åœ¨æ•°æ®å¤„ç†å’Œåˆ†æä¹‹åï¼Œæˆ‘ä»¬å°†ç»§ç»­è¿™ä¸ªé€Ÿæˆè¯¾ç¨‹çš„æ—…ç¨‹ï¼š

    #### ğŸ”„ **CI/CDå·¥ä½œæµ**
    - ä¸ºMLç³»ç»Ÿé‡èº«å®šåˆ¶çš„æŒç»­é›†æˆå’Œéƒ¨ç½²
    - æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„è‡ªåŠ¨åŒ–æµç¨‹
    - æµ‹è¯•ç­–ç•¥å’Œè´¨é‡ä¿è¯

    #### ğŸ¢ **è¡Œä¸šæ¡ˆä¾‹ç ”ç©¶**
    - æ¥è‡ªè¡Œä¸šçš„çœŸå®ä¸–ç•Œæ¡ˆä¾‹ç ”ç©¶
    - ä¸åŒè§„æ¨¡å’Œé¢†åŸŸçš„æœ€ä½³å®è·µ
    - å¸¸è§æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ

    #### ğŸ¤– **æ¨¡å‹å¼€å‘å’Œå®è·µ**
    - æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯çš„æœ€ä½³å®è·µ
    - è¶…å‚æ•°ä¼˜åŒ–å’Œå®éªŒç®¡ç†
    - æ¨¡å‹é€‰æ‹©å’Œé›†æˆç­–ç•¥

    #### ğŸ“Š **ç”Ÿäº§ç›‘æ§å’Œè§‚å¯Ÿ**
    - æ¨¡å‹æ€§èƒ½ç›‘æ§
    - æ•°æ®å’Œæ¨¡å‹æ¼‚ç§»æ£€æµ‹
    - è­¦æŠ¥å’Œäº‹ä»¶å“åº”

    #### ğŸ§  **LLMOpsç‰¹æ®Šè€ƒè™‘**
    - å¤§è¯­è¨€æ¨¡å‹çš„ç‰¹æ®Šè¿è¥éœ€æ±‚
    - æç¤ºå·¥ç¨‹å’Œç‰ˆæœ¬æ§åˆ¶
    - æˆæœ¬ä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜

    #### ğŸ”— **ç«¯åˆ°ç«¯é›†æˆ**
    - ç»“åˆç”Ÿå‘½å‘¨æœŸæ‰€æœ‰å…ƒç´ çš„å®Œæ•´ç«¯åˆ°ç«¯ç¤ºä¾‹
    - ä»æ•°æ®åˆ°éƒ¨ç½²çš„å®Œæ•´å·¥ä½œæµ
    - ä¼ä¸šçº§MLOpså¹³å°æ¶æ„

    ### ğŸ¯ **æœ€ç»ˆç›®æ ‡**

    ç›®æ ‡ï¼Œä¸€å¦‚æ—¢å¾€ï¼Œæ˜¯å¸®åŠ©ä½ åŸ¹å…»æˆç†Ÿçš„ã€**ä»¥ç³»ç»Ÿä¸ºä¸­å¿ƒçš„æ€ç»´æ–¹å¼**ï¼Œå°†æœºå™¨å­¦ä¹ ä¸è§†ä¸ºç‹¬ç«‹çš„å·¥ä»¶ï¼Œè€Œæ˜¯æ›´å¹¿æ³›è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿçš„æ´»è·ƒéƒ¨åˆ†ã€‚

    è¿™ç§æ€ç»´æ–¹å¼çš„ç‰¹å¾åŒ…æ‹¬ï¼š

    - **æ•´ä½“è§†è§’**ï¼šç†è§£MLç³»ç»Ÿçš„æ‰€æœ‰ç»„ä»¶å¦‚ä½•ç›¸äº’ä½œç”¨
    - **è´¨é‡æ„è¯†**ï¼šä¼˜å…ˆè€ƒè™‘æ•°æ®è´¨é‡å’Œç®¡é“å¯é æ€§
    - **å¯æ‰©å±•æ€§æ€ç»´**ï¼šè®¾è®¡èƒ½å¤Ÿéšä¸šåŠ¡å¢é•¿è€Œæ‰©å±•çš„ç³»ç»Ÿ
    - **è¿è¥å¯¼å‘**ï¼šè€ƒè™‘ç›‘æ§ã€ç»´æŠ¤å’Œæ•…éšœæ’é™¤
    - **åä½œç²¾ç¥**ï¼šæ„å»ºæ”¯æŒå›¢é˜Ÿåä½œçš„ç³»ç»Ÿå’Œæµç¨‹

    é€šè¿‡æŒæ¡è¿™äº›æ•°æ®å·¥ç¨‹åŸºç¡€ï¼Œä½ å·²ç»ä¸ºæ„å»ºå¥å£®ã€å¯æ‰©å±•å’Œå¯ç»´æŠ¤çš„MLç³»ç»Ÿå¥ å®šäº†åšå®çš„åŸºç¡€ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨è¿™ä¸ªåŸºç¡€ä¸Šæ„å»ºæ›´å¤æ‚å’Œå¼ºå¤§çš„MLOpsèƒ½åŠ›ã€‚

    ---

    ğŸš€ **ç»§ç»­ä½ çš„MLOpså­¦ä¹ ä¹‹æ—…ï¼Œè®°ä½ï¼šä¼˜ç§€çš„æ•°æ®ç®¡é“æ˜¯æˆåŠŸMLç³»ç»Ÿçš„åŸºçŸ³ï¼**
    """
    )
    return


if __name__ == "__main__":
    app.run()
